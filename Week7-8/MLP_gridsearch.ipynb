{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjMing/Pattern-EGCI463/blob/main/Week7-8/MLP_gridsearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SwrLe7uU9ix"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JdwF4sK8U9i2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\",force_remount=True)\n",
        "path=\"/content/drive/My Drive/data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5pjHVSPWqsp",
        "outputId": "b9f59228-a731-49c1-e990-2e216a0fd324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUAH8RbyU9i2"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(path+'Iris.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2Pt8E11uTSfG",
        "outputId": "a6357feb-9c89-4de4-8ea7-2a9124744119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
              "0      1            5.1           3.5            1.4           0.2   \n",
              "1      2            4.9           3.0            1.4           0.2   \n",
              "2      3            4.7           3.2            1.3           0.2   \n",
              "3      4            4.6           3.1            1.5           0.2   \n",
              "4      5            5.0           3.6            1.4           0.2   \n",
              "..   ...            ...           ...            ...           ...   \n",
              "145  146            6.7           3.0            5.2           2.3   \n",
              "146  147            6.3           2.5            5.0           1.9   \n",
              "147  148            6.5           3.0            5.2           2.0   \n",
              "148  149            6.2           3.4            5.4           2.3   \n",
              "149  150            5.9           3.0            5.1           1.8   \n",
              "\n",
              "            Species  \n",
              "0       Iris-setosa  \n",
              "1       Iris-setosa  \n",
              "2       Iris-setosa  \n",
              "3       Iris-setosa  \n",
              "4       Iris-setosa  \n",
              "..              ...  \n",
              "145  Iris-virginica  \n",
              "146  Iris-virginica  \n",
              "147  Iris-virginica  \n",
              "148  Iris-virginica  \n",
              "149  Iris-virginica  \n",
              "\n",
              "[150 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d69c7e8-264b-4bd5-be53-2177e9354838\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>146</td>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>147</td>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>148</td>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>149</td>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>150</td>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d69c7e8-264b-4bd5-be53-2177e9354838')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d69c7e8-264b-4bd5-be53-2177e9354838 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d69c7e8-264b-4bd5-be53-2177e9354838');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fihQdEE4U9i3"
      },
      "outputs": [],
      "source": [
        "data.loc[data['Species']=='Iris-setosa', 'Species_value']=0\n",
        "data.loc[data['Species']=='Iris-versicolor', 'Species_value']=1\n",
        "data.loc[data['Species']=='Iris-virginica', 'Species_value']=2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "6f-Ws5GyfguL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "87296bab-251e-4bb0-a0a5-81766492fb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
              "0      1            5.1           3.5            1.4           0.2   \n",
              "1      2            4.9           3.0            1.4           0.2   \n",
              "2      3            4.7           3.2            1.3           0.2   \n",
              "3      4            4.6           3.1            1.5           0.2   \n",
              "4      5            5.0           3.6            1.4           0.2   \n",
              "..   ...            ...           ...            ...           ...   \n",
              "145  146            6.7           3.0            5.2           2.3   \n",
              "146  147            6.3           2.5            5.0           1.9   \n",
              "147  148            6.5           3.0            5.2           2.0   \n",
              "148  149            6.2           3.4            5.4           2.3   \n",
              "149  150            5.9           3.0            5.1           1.8   \n",
              "\n",
              "            Species  Species_value  \n",
              "0       Iris-setosa            0.0  \n",
              "1       Iris-setosa            0.0  \n",
              "2       Iris-setosa            0.0  \n",
              "3       Iris-setosa            0.0  \n",
              "4       Iris-setosa            0.0  \n",
              "..              ...            ...  \n",
              "145  Iris-virginica            2.0  \n",
              "146  Iris-virginica            2.0  \n",
              "147  Iris-virginica            2.0  \n",
              "148  Iris-virginica            2.0  \n",
              "149  Iris-virginica            2.0  \n",
              "\n",
              "[150 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4133611-1adf-4fcb-994e-263985e0fbe1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "      <th>Species_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>146</td>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>147</td>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>148</td>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>149</td>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>150</td>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4133611-1adf-4fcb-994e-263985e0fbe1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4133611-1adf-4fcb-994e-263985e0fbe1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4133611-1adf-4fcb-994e-263985e0fbe1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train , X_test, Y_train, Y_test=train_test_split(data.iloc[:,1:5],data.iloc[:,-1],test_size=0.2)"
      ],
      "metadata": {
        "id": "wtM89uSvEcyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml=MLPClassifier(verbose=1,hidden_layer_sizes=(5,3),max_iter=10000,tol=0.000000001)\n",
        "ml.fit(X_train,Y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MiiDCpLfqjp",
        "outputId": "acf05c6e-b63b-40e8-fb34-f40cc65edc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 5001, loss = 0.01158317\n",
            "Iteration 5002, loss = 0.01157625\n",
            "Iteration 5003, loss = 0.01156941\n",
            "Iteration 5004, loss = 0.01156248\n",
            "Iteration 5005, loss = 0.01155559\n",
            "Iteration 5006, loss = 0.01154868\n",
            "Iteration 5007, loss = 0.01154171\n",
            "Iteration 5008, loss = 0.01153476\n",
            "Iteration 5009, loss = 0.01152781\n",
            "Iteration 5010, loss = 0.01152081\n",
            "Iteration 5011, loss = 0.01151442\n",
            "Iteration 5012, loss = 0.01150712\n",
            "Iteration 5013, loss = 0.01150030\n",
            "Iteration 5014, loss = 0.01149357\n",
            "Iteration 5015, loss = 0.01148678\n",
            "Iteration 5016, loss = 0.01147999\n",
            "Iteration 5017, loss = 0.01147318\n",
            "Iteration 5018, loss = 0.01146631\n",
            "Iteration 5019, loss = 0.01145946\n",
            "Iteration 5020, loss = 0.01145258\n",
            "Iteration 5021, loss = 0.01144567\n",
            "Iteration 5022, loss = 0.01143877\n",
            "Iteration 5023, loss = 0.01143186\n",
            "Iteration 5024, loss = 0.01142492\n",
            "Iteration 5025, loss = 0.01141799\n",
            "Iteration 5026, loss = 0.01141104\n",
            "Iteration 5027, loss = 0.01140409\n",
            "Iteration 5028, loss = 0.01139786\n",
            "Iteration 5029, loss = 0.01139086\n",
            "Iteration 5030, loss = 0.01138369\n",
            "Iteration 5031, loss = 0.01137698\n",
            "Iteration 5032, loss = 0.01137024\n",
            "Iteration 5033, loss = 0.01136350\n",
            "Iteration 5034, loss = 0.01135672\n",
            "Iteration 5035, loss = 0.01134992\n",
            "Iteration 5036, loss = 0.01134312\n",
            "Iteration 5037, loss = 0.01133629\n",
            "Iteration 5038, loss = 0.01132944\n",
            "Iteration 5039, loss = 0.01132260\n",
            "Iteration 5040, loss = 0.01131573\n",
            "Iteration 5041, loss = 0.01130886\n",
            "Iteration 5042, loss = 0.01130216\n",
            "Iteration 5043, loss = 0.01129528\n",
            "Iteration 5044, loss = 0.01128851\n",
            "Iteration 5045, loss = 0.01128174\n",
            "Iteration 5046, loss = 0.01127497\n",
            "Iteration 5047, loss = 0.01126840\n",
            "Iteration 5048, loss = 0.01126153\n",
            "Iteration 5049, loss = 0.01125486\n",
            "Iteration 5050, loss = 0.01124812\n",
            "Iteration 5051, loss = 0.01124141\n",
            "Iteration 5052, loss = 0.01123468\n",
            "Iteration 5053, loss = 0.01122790\n",
            "Iteration 5054, loss = 0.01122114\n",
            "Iteration 5055, loss = 0.01121436\n",
            "Iteration 5056, loss = 0.01120764\n",
            "Iteration 5057, loss = 0.01120090\n",
            "Iteration 5058, loss = 0.01119423\n",
            "Iteration 5059, loss = 0.01118754\n",
            "Iteration 5060, loss = 0.01118083\n",
            "Iteration 5061, loss = 0.01117410\n",
            "Iteration 5062, loss = 0.01116737\n",
            "Iteration 5063, loss = 0.01116072\n",
            "Iteration 5064, loss = 0.01115401\n",
            "Iteration 5065, loss = 0.01114738\n",
            "Iteration 5066, loss = 0.01114072\n",
            "Iteration 5067, loss = 0.01113405\n",
            "Iteration 5068, loss = 0.01112736\n",
            "Iteration 5069, loss = 0.01112065\n",
            "Iteration 5070, loss = 0.01111394\n",
            "Iteration 5071, loss = 0.01110730\n",
            "Iteration 5072, loss = 0.01110064\n",
            "Iteration 5073, loss = 0.01109402\n",
            "Iteration 5074, loss = 0.01108739\n",
            "Iteration 5075, loss = 0.01108075\n",
            "Iteration 5076, loss = 0.01107408\n",
            "Iteration 5077, loss = 0.01106741\n",
            "Iteration 5078, loss = 0.01106072\n",
            "Iteration 5079, loss = 0.01105441\n",
            "Iteration 5080, loss = 0.01104746\n",
            "Iteration 5081, loss = 0.01104088\n",
            "Iteration 5082, loss = 0.01103428\n",
            "Iteration 5083, loss = 0.01102767\n",
            "Iteration 5084, loss = 0.01102103\n",
            "Iteration 5085, loss = 0.01101439\n",
            "Iteration 5086, loss = 0.01100800\n",
            "Iteration 5087, loss = 0.01100122\n",
            "Iteration 5088, loss = 0.01099467\n",
            "Iteration 5089, loss = 0.01098810\n",
            "Iteration 5090, loss = 0.01098153\n",
            "Iteration 5091, loss = 0.01097492\n",
            "Iteration 5092, loss = 0.01096831\n",
            "Iteration 5093, loss = 0.01096169\n",
            "Iteration 5094, loss = 0.01095524\n",
            "Iteration 5095, loss = 0.01094855\n",
            "Iteration 5096, loss = 0.01094203\n",
            "Iteration 5097, loss = 0.01093549\n",
            "Iteration 5098, loss = 0.01092894\n",
            "Iteration 5099, loss = 0.01092236\n",
            "Iteration 5100, loss = 0.01091578\n",
            "Iteration 5101, loss = 0.01090918\n",
            "Iteration 5102, loss = 0.01090303\n",
            "Iteration 5103, loss = 0.01089610\n",
            "Iteration 5104, loss = 0.01088961\n",
            "Iteration 5105, loss = 0.01088310\n",
            "Iteration 5106, loss = 0.01087657\n",
            "Iteration 5107, loss = 0.01087003\n",
            "Iteration 5108, loss = 0.01086347\n",
            "Iteration 5109, loss = 0.01085724\n",
            "Iteration 5110, loss = 0.01085048\n",
            "Iteration 5111, loss = 0.01084402\n",
            "Iteration 5112, loss = 0.01083754\n",
            "Iteration 5113, loss = 0.01083106\n",
            "Iteration 5114, loss = 0.01082454\n",
            "Iteration 5115, loss = 0.01081801\n",
            "Iteration 5116, loss = 0.01081148\n",
            "Iteration 5117, loss = 0.01080521\n",
            "Iteration 5118, loss = 0.01079852\n",
            "Iteration 5119, loss = 0.01079209\n",
            "Iteration 5120, loss = 0.01078564\n",
            "Iteration 5121, loss = 0.01077917\n",
            "Iteration 5122, loss = 0.01077268\n",
            "Iteration 5123, loss = 0.01076618\n",
            "Iteration 5124, loss = 0.01075974\n",
            "Iteration 5125, loss = 0.01075331\n",
            "Iteration 5126, loss = 0.01074691\n",
            "Iteration 5127, loss = 0.01074049\n",
            "Iteration 5128, loss = 0.01073405\n",
            "Iteration 5129, loss = 0.01072760\n",
            "Iteration 5130, loss = 0.01072113\n",
            "Iteration 5131, loss = 0.01071466\n",
            "Iteration 5132, loss = 0.01070816\n",
            "Iteration 5133, loss = 0.01070223\n",
            "Iteration 5134, loss = 0.01069540\n",
            "Iteration 5135, loss = 0.01068909\n",
            "Iteration 5136, loss = 0.01068281\n",
            "Iteration 5137, loss = 0.01067650\n",
            "Iteration 5138, loss = 0.01067019\n",
            "Iteration 5139, loss = 0.01066384\n",
            "Iteration 5140, loss = 0.01065746\n",
            "Iteration 5141, loss = 0.01065108\n",
            "Iteration 5142, loss = 0.01064467\n",
            "Iteration 5143, loss = 0.01063825\n",
            "Iteration 5144, loss = 0.01063182\n",
            "Iteration 5145, loss = 0.01062537\n",
            "Iteration 5146, loss = 0.01061892\n",
            "Iteration 5147, loss = 0.01061247\n",
            "Iteration 5148, loss = 0.01060600\n",
            "Iteration 5149, loss = 0.01059952\n",
            "Iteration 5150, loss = 0.01059359\n",
            "Iteration 5151, loss = 0.01058707\n",
            "Iteration 5152, loss = 0.01058055\n",
            "Iteration 5153, loss = 0.01057429\n",
            "Iteration 5154, loss = 0.01056801\n",
            "Iteration 5155, loss = 0.01056175\n",
            "Iteration 5156, loss = 0.01055543\n",
            "Iteration 5157, loss = 0.01054909\n",
            "Iteration 5158, loss = 0.01054277\n",
            "Iteration 5159, loss = 0.01053640\n",
            "Iteration 5160, loss = 0.01053003\n",
            "Iteration 5161, loss = 0.01052366\n",
            "Iteration 5162, loss = 0.01051726\n",
            "Iteration 5163, loss = 0.01051086\n",
            "Iteration 5164, loss = 0.01050463\n",
            "Iteration 5165, loss = 0.01049821\n",
            "Iteration 5166, loss = 0.01049191\n",
            "Iteration 5167, loss = 0.01048562\n",
            "Iteration 5168, loss = 0.01047930\n",
            "Iteration 5169, loss = 0.01047311\n",
            "Iteration 5170, loss = 0.01046693\n",
            "Iteration 5171, loss = 0.01046067\n",
            "Iteration 5172, loss = 0.01045442\n",
            "Iteration 5173, loss = 0.01044816\n",
            "Iteration 5174, loss = 0.01044186\n",
            "Iteration 5175, loss = 0.01043556\n",
            "Iteration 5176, loss = 0.01042926\n",
            "Iteration 5177, loss = 0.01042292\n",
            "Iteration 5178, loss = 0.01041659\n",
            "Iteration 5179, loss = 0.01041082\n",
            "Iteration 5180, loss = 0.01040423\n",
            "Iteration 5181, loss = 0.01039803\n",
            "Iteration 5182, loss = 0.01039189\n",
            "Iteration 5183, loss = 0.01038573\n",
            "Iteration 5184, loss = 0.01037960\n",
            "Iteration 5185, loss = 0.01037339\n",
            "Iteration 5186, loss = 0.01036718\n",
            "Iteration 5187, loss = 0.01036098\n",
            "Iteration 5188, loss = 0.01035472\n",
            "Iteration 5189, loss = 0.01034846\n",
            "Iteration 5190, loss = 0.01034222\n",
            "Iteration 5191, loss = 0.01033592\n",
            "Iteration 5192, loss = 0.01032964\n",
            "Iteration 5193, loss = 0.01032337\n",
            "Iteration 5194, loss = 0.01031706\n",
            "Iteration 5195, loss = 0.01031116\n",
            "Iteration 5196, loss = 0.01030478\n",
            "Iteration 5197, loss = 0.01029857\n",
            "Iteration 5198, loss = 0.01029251\n",
            "Iteration 5199, loss = 0.01028639\n",
            "Iteration 5200, loss = 0.01028029\n",
            "Iteration 5201, loss = 0.01027415\n",
            "Iteration 5202, loss = 0.01026798\n",
            "Iteration 5203, loss = 0.01026182\n",
            "Iteration 5204, loss = 0.01025563\n",
            "Iteration 5205, loss = 0.01024942\n",
            "Iteration 5206, loss = 0.01024322\n",
            "Iteration 5207, loss = 0.01023699\n",
            "Iteration 5208, loss = 0.01023076\n",
            "Iteration 5209, loss = 0.01022453\n",
            "Iteration 5210, loss = 0.01021885\n",
            "Iteration 5211, loss = 0.01021252\n",
            "Iteration 5212, loss = 0.01020621\n",
            "Iteration 5213, loss = 0.01020019\n",
            "Iteration 5214, loss = 0.01019416\n",
            "Iteration 5215, loss = 0.01018810\n",
            "Iteration 5216, loss = 0.01018202\n",
            "Iteration 5217, loss = 0.01017592\n",
            "Iteration 5218, loss = 0.01016980\n",
            "Iteration 5219, loss = 0.01016367\n",
            "Iteration 5220, loss = 0.01015753\n",
            "Iteration 5221, loss = 0.01015137\n",
            "Iteration 5222, loss = 0.01014521\n",
            "Iteration 5223, loss = 0.01013904\n",
            "Iteration 5224, loss = 0.01013286\n",
            "Iteration 5225, loss = 0.01012717\n",
            "Iteration 5226, loss = 0.01012088\n",
            "Iteration 5227, loss = 0.01011472\n",
            "Iteration 5228, loss = 0.01010875\n",
            "Iteration 5229, loss = 0.01010277\n",
            "Iteration 5230, loss = 0.01009677\n",
            "Iteration 5231, loss = 0.01009074\n",
            "Iteration 5232, loss = 0.01008469\n",
            "Iteration 5233, loss = 0.01007863\n",
            "Iteration 5234, loss = 0.01007255\n",
            "Iteration 5235, loss = 0.01006646\n",
            "Iteration 5236, loss = 0.01006037\n",
            "Iteration 5237, loss = 0.01005425\n",
            "Iteration 5238, loss = 0.01004814\n",
            "Iteration 5239, loss = 0.01004202\n",
            "Iteration 5240, loss = 0.01003628\n",
            "Iteration 5241, loss = 0.01003005\n",
            "Iteration 5242, loss = 0.01002403\n",
            "Iteration 5243, loss = 0.01001811\n",
            "Iteration 5244, loss = 0.01001218\n",
            "Iteration 5245, loss = 0.01000624\n",
            "Iteration 5246, loss = 0.01000026\n",
            "Iteration 5247, loss = 0.00999427\n",
            "Iteration 5248, loss = 0.00998826\n",
            "Iteration 5249, loss = 0.00998223\n",
            "Iteration 5250, loss = 0.00997620\n",
            "Iteration 5251, loss = 0.00997016\n",
            "Iteration 5252, loss = 0.00996410\n",
            "Iteration 5253, loss = 0.00995804\n",
            "Iteration 5254, loss = 0.00995197\n",
            "Iteration 5255, loss = 0.00994623\n",
            "Iteration 5256, loss = 0.00994006\n",
            "Iteration 5257, loss = 0.00993414\n",
            "Iteration 5258, loss = 0.00992828\n",
            "Iteration 5259, loss = 0.00992240\n",
            "Iteration 5260, loss = 0.00991651\n",
            "Iteration 5261, loss = 0.00991058\n",
            "Iteration 5262, loss = 0.00990464\n",
            "Iteration 5263, loss = 0.00989869\n",
            "Iteration 5264, loss = 0.00989272\n",
            "Iteration 5265, loss = 0.00988674\n",
            "Iteration 5266, loss = 0.00988075\n",
            "Iteration 5267, loss = 0.00987474\n",
            "Iteration 5268, loss = 0.00986873\n",
            "Iteration 5269, loss = 0.00986272\n",
            "Iteration 5270, loss = 0.00985703\n",
            "Iteration 5271, loss = 0.00985092\n",
            "Iteration 5272, loss = 0.00984505\n",
            "Iteration 5273, loss = 0.00983924\n",
            "Iteration 5274, loss = 0.00983342\n",
            "Iteration 5275, loss = 0.00982758\n",
            "Iteration 5276, loss = 0.00982170\n",
            "Iteration 5277, loss = 0.00981582\n",
            "Iteration 5278, loss = 0.00980992\n",
            "Iteration 5279, loss = 0.00980400\n",
            "Iteration 5280, loss = 0.00979807\n",
            "Iteration 5281, loss = 0.00979213\n",
            "Iteration 5282, loss = 0.00978618\n",
            "Iteration 5283, loss = 0.00978023\n",
            "Iteration 5284, loss = 0.00977426\n",
            "Iteration 5285, loss = 0.00976864\n",
            "Iteration 5286, loss = 0.00976258\n",
            "Iteration 5287, loss = 0.00975675\n",
            "Iteration 5288, loss = 0.00975100\n",
            "Iteration 5289, loss = 0.00974522\n",
            "Iteration 5290, loss = 0.00973944\n",
            "Iteration 5291, loss = 0.00973361\n",
            "Iteration 5292, loss = 0.00972778\n",
            "Iteration 5293, loss = 0.00972194\n",
            "Iteration 5294, loss = 0.00971607\n",
            "Iteration 5295, loss = 0.00971019\n",
            "Iteration 5296, loss = 0.00970431\n",
            "Iteration 5297, loss = 0.00969841\n",
            "Iteration 5298, loss = 0.00969251\n",
            "Iteration 5299, loss = 0.00968660\n",
            "Iteration 5300, loss = 0.00968106\n",
            "Iteration 5301, loss = 0.00967505\n",
            "Iteration 5302, loss = 0.00966924\n",
            "Iteration 5303, loss = 0.00966354\n",
            "Iteration 5304, loss = 0.00965782\n",
            "Iteration 5305, loss = 0.00965208\n",
            "Iteration 5306, loss = 0.00964631\n",
            "Iteration 5307, loss = 0.00964053\n",
            "Iteration 5308, loss = 0.00963474\n",
            "Iteration 5309, loss = 0.00962892\n",
            "Iteration 5310, loss = 0.00962310\n",
            "Iteration 5311, loss = 0.00961727\n",
            "Iteration 5312, loss = 0.00961142\n",
            "Iteration 5313, loss = 0.00960557\n",
            "Iteration 5314, loss = 0.00959972\n",
            "Iteration 5315, loss = 0.00959428\n",
            "Iteration 5316, loss = 0.00958833\n",
            "Iteration 5317, loss = 0.00958252\n",
            "Iteration 5318, loss = 0.00957687\n",
            "Iteration 5319, loss = 0.00957119\n",
            "Iteration 5320, loss = 0.00956551\n",
            "Iteration 5321, loss = 0.00955979\n",
            "Iteration 5322, loss = 0.00955406\n",
            "Iteration 5323, loss = 0.00954832\n",
            "Iteration 5324, loss = 0.00954256\n",
            "Iteration 5325, loss = 0.00953679\n",
            "Iteration 5326, loss = 0.00953101\n",
            "Iteration 5327, loss = 0.00952521\n",
            "Iteration 5328, loss = 0.00951942\n",
            "Iteration 5329, loss = 0.00951361\n",
            "Iteration 5330, loss = 0.00950828\n",
            "Iteration 5331, loss = 0.00950239\n",
            "Iteration 5332, loss = 0.00949657\n",
            "Iteration 5333, loss = 0.00949097\n",
            "Iteration 5334, loss = 0.00948535\n",
            "Iteration 5335, loss = 0.00947972\n",
            "Iteration 5336, loss = 0.00947405\n",
            "Iteration 5337, loss = 0.00946837\n",
            "Iteration 5338, loss = 0.00946268\n",
            "Iteration 5339, loss = 0.00945697\n",
            "Iteration 5340, loss = 0.00945125\n",
            "Iteration 5341, loss = 0.00944553\n",
            "Iteration 5342, loss = 0.00943978\n",
            "Iteration 5343, loss = 0.00943404\n",
            "Iteration 5344, loss = 0.00942829\n",
            "Iteration 5345, loss = 0.00942306\n",
            "Iteration 5346, loss = 0.00941722\n",
            "Iteration 5347, loss = 0.00941140\n",
            "Iteration 5348, loss = 0.00940585\n",
            "Iteration 5349, loss = 0.00940028\n",
            "Iteration 5350, loss = 0.00939470\n",
            "Iteration 5351, loss = 0.00938908\n",
            "Iteration 5352, loss = 0.00938345\n",
            "Iteration 5353, loss = 0.00937782\n",
            "Iteration 5354, loss = 0.00937215\n",
            "Iteration 5355, loss = 0.00936649\n",
            "Iteration 5356, loss = 0.00936081\n",
            "Iteration 5357, loss = 0.00935512\n",
            "Iteration 5358, loss = 0.00934943\n",
            "Iteration 5359, loss = 0.00934373\n",
            "Iteration 5360, loss = 0.00933860\n",
            "Iteration 5361, loss = 0.00933281\n",
            "Iteration 5362, loss = 0.00932699\n",
            "Iteration 5363, loss = 0.00932149\n",
            "Iteration 5364, loss = 0.00931597\n",
            "Iteration 5365, loss = 0.00931044\n",
            "Iteration 5366, loss = 0.00930488\n",
            "Iteration 5367, loss = 0.00929930\n",
            "Iteration 5368, loss = 0.00929372\n",
            "Iteration 5369, loss = 0.00928810\n",
            "Iteration 5370, loss = 0.00928249\n",
            "Iteration 5371, loss = 0.00927687\n",
            "Iteration 5372, loss = 0.00927122\n",
            "Iteration 5373, loss = 0.00926559\n",
            "Iteration 5374, loss = 0.00925994\n",
            "Iteration 5375, loss = 0.00925490\n",
            "Iteration 5376, loss = 0.00924915\n",
            "Iteration 5377, loss = 0.00924335\n",
            "Iteration 5378, loss = 0.00923790\n",
            "Iteration 5379, loss = 0.00923243\n",
            "Iteration 5380, loss = 0.00922695\n",
            "Iteration 5381, loss = 0.00922144\n",
            "Iteration 5382, loss = 0.00921591\n",
            "Iteration 5383, loss = 0.00921038\n",
            "Iteration 5384, loss = 0.00920481\n",
            "Iteration 5385, loss = 0.00919925\n",
            "Iteration 5386, loss = 0.00919368\n",
            "Iteration 5387, loss = 0.00918809\n",
            "Iteration 5388, loss = 0.00918250\n",
            "Iteration 5389, loss = 0.00917690\n",
            "Iteration 5390, loss = 0.00917194\n",
            "Iteration 5391, loss = 0.00916625\n",
            "Iteration 5392, loss = 0.00916047\n",
            "Iteration 5393, loss = 0.00915506\n",
            "Iteration 5394, loss = 0.00914964\n",
            "Iteration 5395, loss = 0.00914422\n",
            "Iteration 5396, loss = 0.00913875\n",
            "Iteration 5397, loss = 0.00913327\n",
            "Iteration 5398, loss = 0.00912779\n",
            "Iteration 5399, loss = 0.00912228\n",
            "Iteration 5400, loss = 0.00911676\n",
            "Iteration 5401, loss = 0.00911124\n",
            "Iteration 5402, loss = 0.00910570\n",
            "Iteration 5403, loss = 0.00910016\n",
            "Iteration 5404, loss = 0.00909461\n",
            "Iteration 5405, loss = 0.00908973\n",
            "Iteration 5406, loss = 0.00908409\n",
            "Iteration 5407, loss = 0.00907833\n",
            "Iteration 5408, loss = 0.00907298\n",
            "Iteration 5409, loss = 0.00906761\n",
            "Iteration 5410, loss = 0.00906223\n",
            "Iteration 5411, loss = 0.00905681\n",
            "Iteration 5412, loss = 0.00905138\n",
            "Iteration 5413, loss = 0.00904595\n",
            "Iteration 5414, loss = 0.00904049\n",
            "Iteration 5415, loss = 0.00903502\n",
            "Iteration 5416, loss = 0.00902955\n",
            "Iteration 5417, loss = 0.00902406\n",
            "Iteration 5418, loss = 0.00901857\n",
            "Iteration 5419, loss = 0.00901308\n",
            "Iteration 5420, loss = 0.00900826\n",
            "Iteration 5421, loss = 0.00900267\n",
            "Iteration 5422, loss = 0.00899694\n",
            "Iteration 5423, loss = 0.00899163\n",
            "Iteration 5424, loss = 0.00898631\n",
            "Iteration 5425, loss = 0.00898098\n",
            "Iteration 5426, loss = 0.00897561\n",
            "Iteration 5427, loss = 0.00897024\n",
            "Iteration 5428, loss = 0.00896485\n",
            "Iteration 5429, loss = 0.00895944\n",
            "Iteration 5430, loss = 0.00895403\n",
            "Iteration 5431, loss = 0.00894860\n",
            "Iteration 5432, loss = 0.00894316\n",
            "Iteration 5433, loss = 0.00893772\n",
            "Iteration 5434, loss = 0.00893228\n",
            "Iteration 5435, loss = 0.00892752\n",
            "Iteration 5436, loss = 0.00892198\n",
            "Iteration 5437, loss = 0.00891629\n",
            "Iteration 5438, loss = 0.00891103\n",
            "Iteration 5439, loss = 0.00890576\n",
            "Iteration 5440, loss = 0.00890048\n",
            "Iteration 5441, loss = 0.00889515\n",
            "Iteration 5442, loss = 0.00888983\n",
            "Iteration 5443, loss = 0.00888449\n",
            "Iteration 5444, loss = 0.00887913\n",
            "Iteration 5445, loss = 0.00887376\n",
            "Iteration 5446, loss = 0.00886839\n",
            "Iteration 5447, loss = 0.00886300\n",
            "Iteration 5448, loss = 0.00885761\n",
            "Iteration 5449, loss = 0.00885223\n",
            "Iteration 5450, loss = 0.00884694\n",
            "Iteration 5451, loss = 0.00884164\n",
            "Iteration 5452, loss = 0.00883633\n",
            "Iteration 5453, loss = 0.00883101\n",
            "Iteration 5454, loss = 0.00882580\n",
            "Iteration 5455, loss = 0.00882047\n",
            "Iteration 5456, loss = 0.00881522\n",
            "Iteration 5457, loss = 0.00880995\n",
            "Iteration 5458, loss = 0.00880469\n",
            "Iteration 5459, loss = 0.00879939\n",
            "Iteration 5460, loss = 0.00879409\n",
            "Iteration 5461, loss = 0.00878878\n",
            "Iteration 5462, loss = 0.00878345\n",
            "Iteration 5463, loss = 0.00877835\n",
            "Iteration 5464, loss = 0.00877290\n",
            "Iteration 5465, loss = 0.00876767\n",
            "Iteration 5466, loss = 0.00876242\n",
            "Iteration 5467, loss = 0.00875716\n",
            "Iteration 5468, loss = 0.00875188\n",
            "Iteration 5469, loss = 0.00874662\n",
            "Iteration 5470, loss = 0.00874143\n",
            "Iteration 5471, loss = 0.00873624\n",
            "Iteration 5472, loss = 0.00873101\n",
            "Iteration 5473, loss = 0.00872580\n",
            "Iteration 5474, loss = 0.00872055\n",
            "Iteration 5475, loss = 0.00871529\n",
            "Iteration 5476, loss = 0.00871003\n",
            "Iteration 5477, loss = 0.00870475\n",
            "Iteration 5478, loss = 0.00869967\n",
            "Iteration 5479, loss = 0.00869429\n",
            "Iteration 5480, loss = 0.00868911\n",
            "Iteration 5481, loss = 0.00868391\n",
            "Iteration 5482, loss = 0.00867869\n",
            "Iteration 5483, loss = 0.00867346\n",
            "Iteration 5484, loss = 0.00866826\n",
            "Iteration 5485, loss = 0.00866311\n",
            "Iteration 5486, loss = 0.00865796\n",
            "Iteration 5487, loss = 0.00865278\n",
            "Iteration 5488, loss = 0.00864761\n",
            "Iteration 5489, loss = 0.00864241\n",
            "Iteration 5490, loss = 0.00863720\n",
            "Iteration 5491, loss = 0.00863199\n",
            "Iteration 5492, loss = 0.00862676\n",
            "Iteration 5493, loss = 0.00862177\n",
            "Iteration 5494, loss = 0.00861640\n",
            "Iteration 5495, loss = 0.00861126\n",
            "Iteration 5496, loss = 0.00860611\n",
            "Iteration 5497, loss = 0.00860094\n",
            "Iteration 5498, loss = 0.00859576\n",
            "Iteration 5499, loss = 0.00859065\n",
            "Iteration 5500, loss = 0.00858550\n",
            "Iteration 5501, loss = 0.00858040\n",
            "Iteration 5502, loss = 0.00857527\n",
            "Iteration 5503, loss = 0.00857014\n",
            "Iteration 5504, loss = 0.00856499\n",
            "Iteration 5505, loss = 0.00855983\n",
            "Iteration 5506, loss = 0.00855466\n",
            "Iteration 5507, loss = 0.00854948\n",
            "Iteration 5508, loss = 0.00854458\n",
            "Iteration 5509, loss = 0.00853921\n",
            "Iteration 5510, loss = 0.00853412\n",
            "Iteration 5511, loss = 0.00852901\n",
            "Iteration 5512, loss = 0.00852389\n",
            "Iteration 5513, loss = 0.00851876\n",
            "Iteration 5514, loss = 0.00851374\n",
            "Iteration 5515, loss = 0.00850859\n",
            "Iteration 5516, loss = 0.00850354\n",
            "Iteration 5517, loss = 0.00849846\n",
            "Iteration 5518, loss = 0.00849338\n",
            "Iteration 5519, loss = 0.00848828\n",
            "Iteration 5520, loss = 0.00848316\n",
            "Iteration 5521, loss = 0.00847804\n",
            "Iteration 5522, loss = 0.00847290\n",
            "Iteration 5523, loss = 0.00846810\n",
            "Iteration 5524, loss = 0.00846273\n",
            "Iteration 5525, loss = 0.00845769\n",
            "Iteration 5526, loss = 0.00845263\n",
            "Iteration 5527, loss = 0.00844755\n",
            "Iteration 5528, loss = 0.00844247\n",
            "Iteration 5529, loss = 0.00843754\n",
            "Iteration 5530, loss = 0.00843240\n",
            "Iteration 5531, loss = 0.00842739\n",
            "Iteration 5532, loss = 0.00842235\n",
            "Iteration 5533, loss = 0.00841732\n",
            "Iteration 5534, loss = 0.00841226\n",
            "Iteration 5535, loss = 0.00840719\n",
            "Iteration 5536, loss = 0.00840213\n",
            "Iteration 5537, loss = 0.00839703\n",
            "Iteration 5538, loss = 0.00839231\n",
            "Iteration 5539, loss = 0.00838696\n",
            "Iteration 5540, loss = 0.00838196\n",
            "Iteration 5541, loss = 0.00837694\n",
            "Iteration 5542, loss = 0.00837192\n",
            "Iteration 5543, loss = 0.00836688\n",
            "Iteration 5544, loss = 0.00836203\n",
            "Iteration 5545, loss = 0.00835690\n",
            "Iteration 5546, loss = 0.00835194\n",
            "Iteration 5547, loss = 0.00834695\n",
            "Iteration 5548, loss = 0.00834196\n",
            "Iteration 5549, loss = 0.00833695\n",
            "Iteration 5550, loss = 0.00833193\n",
            "Iteration 5551, loss = 0.00832690\n",
            "Iteration 5552, loss = 0.00832186\n",
            "Iteration 5553, loss = 0.00831721\n",
            "Iteration 5554, loss = 0.00831190\n",
            "Iteration 5555, loss = 0.00830706\n",
            "Iteration 5556, loss = 0.00830219\n",
            "Iteration 5557, loss = 0.00829730\n",
            "Iteration 5558, loss = 0.00829241\n",
            "Iteration 5559, loss = 0.00828747\n",
            "Iteration 5560, loss = 0.00828252\n",
            "Iteration 5561, loss = 0.00827757\n",
            "Iteration 5562, loss = 0.00827259\n",
            "Iteration 5563, loss = 0.00826761\n",
            "Iteration 5564, loss = 0.00826261\n",
            "Iteration 5565, loss = 0.00825760\n",
            "Iteration 5566, loss = 0.00825259\n",
            "Iteration 5567, loss = 0.00824757\n",
            "Iteration 5568, loss = 0.00824254\n",
            "Iteration 5569, loss = 0.00823752\n",
            "Iteration 5570, loss = 0.00823303\n",
            "Iteration 5571, loss = 0.00822798\n",
            "Iteration 5572, loss = 0.00822281\n",
            "Iteration 5573, loss = 0.00821792\n",
            "Iteration 5574, loss = 0.00821308\n",
            "Iteration 5575, loss = 0.00820822\n",
            "Iteration 5576, loss = 0.00820329\n",
            "Iteration 5577, loss = 0.00819840\n",
            "Iteration 5578, loss = 0.00819347\n",
            "Iteration 5579, loss = 0.00818852\n",
            "Iteration 5580, loss = 0.00818360\n",
            "Iteration 5581, loss = 0.00817863\n",
            "Iteration 5582, loss = 0.00817367\n",
            "Iteration 5583, loss = 0.00816871\n",
            "Iteration 5584, loss = 0.00816422\n",
            "Iteration 5585, loss = 0.00815914\n",
            "Iteration 5586, loss = 0.00815411\n",
            "Iteration 5587, loss = 0.00814932\n",
            "Iteration 5588, loss = 0.00814452\n",
            "Iteration 5589, loss = 0.00813969\n",
            "Iteration 5590, loss = 0.00813485\n",
            "Iteration 5591, loss = 0.00812999\n",
            "Iteration 5592, loss = 0.00812511\n",
            "Iteration 5593, loss = 0.00812022\n",
            "Iteration 5594, loss = 0.00811533\n",
            "Iteration 5595, loss = 0.00811042\n",
            "Iteration 5596, loss = 0.00810550\n",
            "Iteration 5597, loss = 0.00810058\n",
            "Iteration 5598, loss = 0.00809565\n",
            "Iteration 5599, loss = 0.00809106\n",
            "Iteration 5600, loss = 0.00808604\n",
            "Iteration 5601, loss = 0.00808121\n",
            "Iteration 5602, loss = 0.00807645\n",
            "Iteration 5603, loss = 0.00807168\n",
            "Iteration 5604, loss = 0.00806691\n",
            "Iteration 5605, loss = 0.00806209\n",
            "Iteration 5606, loss = 0.00805728\n",
            "Iteration 5607, loss = 0.00805245\n",
            "Iteration 5608, loss = 0.00804760\n",
            "Iteration 5609, loss = 0.00804275\n",
            "Iteration 5610, loss = 0.00803788\n",
            "Iteration 5611, loss = 0.00803301\n",
            "Iteration 5612, loss = 0.00802814\n",
            "Iteration 5613, loss = 0.00802325\n",
            "Iteration 5614, loss = 0.00801882\n",
            "Iteration 5615, loss = 0.00801386\n",
            "Iteration 5616, loss = 0.00800893\n",
            "Iteration 5617, loss = 0.00800422\n",
            "Iteration 5618, loss = 0.00799949\n",
            "Iteration 5619, loss = 0.00799476\n",
            "Iteration 5620, loss = 0.00798999\n",
            "Iteration 5621, loss = 0.00798522\n",
            "Iteration 5622, loss = 0.00798044\n",
            "Iteration 5623, loss = 0.00797563\n",
            "Iteration 5624, loss = 0.00797082\n",
            "Iteration 5625, loss = 0.00796600\n",
            "Iteration 5626, loss = 0.00796117\n",
            "Iteration 5627, loss = 0.00795635\n",
            "Iteration 5628, loss = 0.00795150\n",
            "Iteration 5629, loss = 0.00794728\n",
            "Iteration 5630, loss = 0.00794236\n",
            "Iteration 5631, loss = 0.00793731\n",
            "Iteration 5632, loss = 0.00793265\n",
            "Iteration 5633, loss = 0.00792797\n",
            "Iteration 5634, loss = 0.00792328\n",
            "Iteration 5635, loss = 0.00791856\n",
            "Iteration 5636, loss = 0.00791383\n",
            "Iteration 5637, loss = 0.00790909\n",
            "Iteration 5638, loss = 0.00790433\n",
            "Iteration 5639, loss = 0.00789957\n",
            "Iteration 5640, loss = 0.00789479\n",
            "Iteration 5641, loss = 0.00789000\n",
            "Iteration 5642, loss = 0.00788522\n",
            "Iteration 5643, loss = 0.00788053\n",
            "Iteration 5644, loss = 0.00787574\n",
            "Iteration 5645, loss = 0.00787104\n",
            "Iteration 5646, loss = 0.00786633\n",
            "Iteration 5647, loss = 0.00786160\n",
            "Iteration 5648, loss = 0.00785710\n",
            "Iteration 5649, loss = 0.00785225\n",
            "Iteration 5650, loss = 0.00784760\n",
            "Iteration 5651, loss = 0.00784292\n",
            "Iteration 5652, loss = 0.00783825\n",
            "Iteration 5653, loss = 0.00783355\n",
            "Iteration 5654, loss = 0.00782884\n",
            "Iteration 5655, loss = 0.00782413\n",
            "Iteration 5656, loss = 0.00781940\n",
            "Iteration 5657, loss = 0.00781503\n",
            "Iteration 5658, loss = 0.00781004\n",
            "Iteration 5659, loss = 0.00780540\n",
            "Iteration 5660, loss = 0.00780074\n",
            "Iteration 5661, loss = 0.00779607\n",
            "Iteration 5662, loss = 0.00779139\n",
            "Iteration 5663, loss = 0.00778689\n",
            "Iteration 5664, loss = 0.00778212\n",
            "Iteration 5665, loss = 0.00777751\n",
            "Iteration 5666, loss = 0.00777287\n",
            "Iteration 5667, loss = 0.00776825\n",
            "Iteration 5668, loss = 0.00776358\n",
            "Iteration 5669, loss = 0.00775892\n",
            "Iteration 5670, loss = 0.00775425\n",
            "Iteration 5671, loss = 0.00774956\n",
            "Iteration 5672, loss = 0.00774527\n",
            "Iteration 5673, loss = 0.00774032\n",
            "Iteration 5674, loss = 0.00773582\n",
            "Iteration 5675, loss = 0.00773130\n",
            "Iteration 5676, loss = 0.00772675\n",
            "Iteration 5677, loss = 0.00772221\n",
            "Iteration 5678, loss = 0.00771762\n",
            "Iteration 5679, loss = 0.00771303\n",
            "Iteration 5680, loss = 0.00770843\n",
            "Iteration 5681, loss = 0.00770379\n",
            "Iteration 5682, loss = 0.00769917\n",
            "Iteration 5683, loss = 0.00769453\n",
            "Iteration 5684, loss = 0.00768987\n",
            "Iteration 5685, loss = 0.00768522\n",
            "Iteration 5686, loss = 0.00768055\n",
            "Iteration 5687, loss = 0.00767588\n",
            "Iteration 5688, loss = 0.00767121\n",
            "Iteration 5689, loss = 0.00766711\n",
            "Iteration 5690, loss = 0.00766243\n",
            "Iteration 5691, loss = 0.00765754\n",
            "Iteration 5692, loss = 0.00765300\n",
            "Iteration 5693, loss = 0.00764852\n",
            "Iteration 5694, loss = 0.00764399\n",
            "Iteration 5695, loss = 0.00763941\n",
            "Iteration 5696, loss = 0.00763488\n",
            "Iteration 5697, loss = 0.00763029\n",
            "Iteration 5698, loss = 0.00762570\n",
            "Iteration 5699, loss = 0.00762112\n",
            "Iteration 5700, loss = 0.00761650\n",
            "Iteration 5701, loss = 0.00761189\n",
            "Iteration 5702, loss = 0.00760729\n",
            "Iteration 5703, loss = 0.00760324\n",
            "Iteration 5704, loss = 0.00759852\n",
            "Iteration 5705, loss = 0.00759372\n",
            "Iteration 5706, loss = 0.00758928\n",
            "Iteration 5707, loss = 0.00758482\n",
            "Iteration 5708, loss = 0.00758034\n",
            "Iteration 5709, loss = 0.00757584\n",
            "Iteration 5710, loss = 0.00757132\n",
            "Iteration 5711, loss = 0.00756680\n",
            "Iteration 5712, loss = 0.00756225\n",
            "Iteration 5713, loss = 0.00755771\n",
            "Iteration 5714, loss = 0.00755315\n",
            "Iteration 5715, loss = 0.00754858\n",
            "Iteration 5716, loss = 0.00754401\n",
            "Iteration 5717, loss = 0.00753943\n",
            "Iteration 5718, loss = 0.00753531\n",
            "Iteration 5719, loss = 0.00753066\n",
            "Iteration 5720, loss = 0.00752601\n",
            "Iteration 5721, loss = 0.00752159\n",
            "Iteration 5722, loss = 0.00751717\n",
            "Iteration 5723, loss = 0.00751273\n",
            "Iteration 5724, loss = 0.00750826\n",
            "Iteration 5725, loss = 0.00750379\n",
            "Iteration 5726, loss = 0.00749930\n",
            "Iteration 5727, loss = 0.00749480\n",
            "Iteration 5728, loss = 0.00749030\n",
            "Iteration 5729, loss = 0.00748577\n",
            "Iteration 5730, loss = 0.00748125\n",
            "Iteration 5731, loss = 0.00747672\n",
            "Iteration 5732, loss = 0.00747218\n",
            "Iteration 5733, loss = 0.00746826\n",
            "Iteration 5734, loss = 0.00746364\n",
            "Iteration 5735, loss = 0.00745889\n",
            "Iteration 5736, loss = 0.00745451\n",
            "Iteration 5737, loss = 0.00745012\n",
            "Iteration 5738, loss = 0.00744573\n",
            "Iteration 5739, loss = 0.00744130\n",
            "Iteration 5740, loss = 0.00743687\n",
            "Iteration 5741, loss = 0.00743243\n",
            "Iteration 5742, loss = 0.00742796\n",
            "Iteration 5743, loss = 0.00742350\n",
            "Iteration 5744, loss = 0.00741902\n",
            "Iteration 5745, loss = 0.00741453\n",
            "Iteration 5746, loss = 0.00741005\n",
            "Iteration 5747, loss = 0.00740572\n",
            "Iteration 5748, loss = 0.00740116\n",
            "Iteration 5749, loss = 0.00739675\n",
            "Iteration 5750, loss = 0.00739234\n",
            "Iteration 5751, loss = 0.00738798\n",
            "Iteration 5752, loss = 0.00738359\n",
            "Iteration 5753, loss = 0.00737924\n",
            "Iteration 5754, loss = 0.00737486\n",
            "Iteration 5755, loss = 0.00737049\n",
            "Iteration 5756, loss = 0.00736608\n",
            "Iteration 5757, loss = 0.00736167\n",
            "Iteration 5758, loss = 0.00735726\n",
            "Iteration 5759, loss = 0.00735283\n",
            "Iteration 5760, loss = 0.00734839\n",
            "Iteration 5761, loss = 0.00734412\n",
            "Iteration 5762, loss = 0.00733962\n",
            "Iteration 5763, loss = 0.00733527\n",
            "Iteration 5764, loss = 0.00733088\n",
            "Iteration 5765, loss = 0.00732651\n",
            "Iteration 5766, loss = 0.00732212\n",
            "Iteration 5767, loss = 0.00731789\n",
            "Iteration 5768, loss = 0.00731344\n",
            "Iteration 5769, loss = 0.00730910\n",
            "Iteration 5770, loss = 0.00730475\n",
            "Iteration 5771, loss = 0.00730043\n",
            "Iteration 5772, loss = 0.00729604\n",
            "Iteration 5773, loss = 0.00729168\n",
            "Iteration 5774, loss = 0.00728730\n",
            "Iteration 5775, loss = 0.00728298\n",
            "Iteration 5776, loss = 0.00727860\n",
            "Iteration 5777, loss = 0.00727429\n",
            "Iteration 5778, loss = 0.00726997\n",
            "Iteration 5779, loss = 0.00726564\n",
            "Iteration 5780, loss = 0.00726129\n",
            "Iteration 5781, loss = 0.00725693\n",
            "Iteration 5782, loss = 0.00725260\n",
            "Iteration 5783, loss = 0.00724831\n",
            "Iteration 5784, loss = 0.00724402\n",
            "Iteration 5785, loss = 0.00723972\n",
            "Iteration 5786, loss = 0.00723541\n",
            "Iteration 5787, loss = 0.00723108\n",
            "Iteration 5788, loss = 0.00722675\n",
            "Iteration 5789, loss = 0.00722241\n",
            "Iteration 5790, loss = 0.00721818\n",
            "Iteration 5791, loss = 0.00721380\n",
            "Iteration 5792, loss = 0.00720953\n",
            "Iteration 5793, loss = 0.00720524\n",
            "Iteration 5794, loss = 0.00720095\n",
            "Iteration 5795, loss = 0.00719664\n",
            "Iteration 5796, loss = 0.00719232\n",
            "Iteration 5797, loss = 0.00718815\n",
            "Iteration 5798, loss = 0.00718377\n",
            "Iteration 5799, loss = 0.00717953\n",
            "Iteration 5800, loss = 0.00717527\n",
            "Iteration 5801, loss = 0.00717100\n",
            "Iteration 5802, loss = 0.00716671\n",
            "Iteration 5803, loss = 0.00716241\n",
            "Iteration 5804, loss = 0.00715811\n",
            "Iteration 5805, loss = 0.00715409\n",
            "Iteration 5806, loss = 0.00714959\n",
            "Iteration 5807, loss = 0.00714536\n",
            "Iteration 5808, loss = 0.00714111\n",
            "Iteration 5809, loss = 0.00713686\n",
            "Iteration 5810, loss = 0.00713259\n",
            "Iteration 5811, loss = 0.00712832\n",
            "Iteration 5812, loss = 0.00712415\n",
            "Iteration 5813, loss = 0.00711994\n",
            "Iteration 5814, loss = 0.00711572\n",
            "Iteration 5815, loss = 0.00711150\n",
            "Iteration 5816, loss = 0.00710725\n",
            "Iteration 5817, loss = 0.00710301\n",
            "Iteration 5818, loss = 0.00709874\n",
            "Iteration 5819, loss = 0.00709447\n",
            "Iteration 5820, loss = 0.00709034\n",
            "Iteration 5821, loss = 0.00708602\n",
            "Iteration 5822, loss = 0.00708183\n",
            "Iteration 5823, loss = 0.00707761\n",
            "Iteration 5824, loss = 0.00707340\n",
            "Iteration 5825, loss = 0.00706917\n",
            "Iteration 5826, loss = 0.00706494\n",
            "Iteration 5827, loss = 0.00706080\n",
            "Iteration 5828, loss = 0.00705663\n",
            "Iteration 5829, loss = 0.00705244\n",
            "Iteration 5830, loss = 0.00704827\n",
            "Iteration 5831, loss = 0.00704405\n",
            "Iteration 5832, loss = 0.00703984\n",
            "Iteration 5833, loss = 0.00703563\n",
            "Iteration 5834, loss = 0.00703138\n",
            "Iteration 5835, loss = 0.00702742\n",
            "Iteration 5836, loss = 0.00702300\n",
            "Iteration 5837, loss = 0.00701886\n",
            "Iteration 5838, loss = 0.00701468\n",
            "Iteration 5839, loss = 0.00701050\n",
            "Iteration 5840, loss = 0.00700632\n",
            "Iteration 5841, loss = 0.00700228\n",
            "Iteration 5842, loss = 0.00699802\n",
            "Iteration 5843, loss = 0.00699390\n",
            "Iteration 5844, loss = 0.00698974\n",
            "Iteration 5845, loss = 0.00698561\n",
            "Iteration 5846, loss = 0.00698143\n",
            "Iteration 5847, loss = 0.00697726\n",
            "Iteration 5848, loss = 0.00697309\n",
            "Iteration 5849, loss = 0.00696888\n",
            "Iteration 5850, loss = 0.00696511\n",
            "Iteration 5851, loss = 0.00696067\n",
            "Iteration 5852, loss = 0.00695660\n",
            "Iteration 5853, loss = 0.00695255\n",
            "Iteration 5854, loss = 0.00694848\n",
            "Iteration 5855, loss = 0.00694443\n",
            "Iteration 5856, loss = 0.00694031\n",
            "Iteration 5857, loss = 0.00693621\n",
            "Iteration 5858, loss = 0.00693209\n",
            "Iteration 5859, loss = 0.00692794\n",
            "Iteration 5860, loss = 0.00692381\n",
            "Iteration 5861, loss = 0.00691965\n",
            "Iteration 5862, loss = 0.00691548\n",
            "Iteration 5863, loss = 0.00691132\n",
            "Iteration 5864, loss = 0.00690714\n",
            "Iteration 5865, loss = 0.00690296\n",
            "Iteration 5866, loss = 0.00689881\n",
            "Iteration 5867, loss = 0.00689470\n",
            "Iteration 5868, loss = 0.00689062\n",
            "Iteration 5869, loss = 0.00688649\n",
            "Iteration 5870, loss = 0.00688254\n",
            "Iteration 5871, loss = 0.00687835\n",
            "Iteration 5872, loss = 0.00687432\n",
            "Iteration 5873, loss = 0.00687026\n",
            "Iteration 5874, loss = 0.00686618\n",
            "Iteration 5875, loss = 0.00686210\n",
            "Iteration 5876, loss = 0.00685800\n",
            "Iteration 5877, loss = 0.00685390\n",
            "Iteration 5878, loss = 0.00684979\n",
            "Iteration 5879, loss = 0.00684573\n",
            "Iteration 5880, loss = 0.00684164\n",
            "Iteration 5881, loss = 0.00683760\n",
            "Iteration 5882, loss = 0.00683355\n",
            "Iteration 5883, loss = 0.00682949\n",
            "Iteration 5884, loss = 0.00682541\n",
            "Iteration 5885, loss = 0.00682133\n",
            "Iteration 5886, loss = 0.00681744\n",
            "Iteration 5887, loss = 0.00681324\n",
            "Iteration 5888, loss = 0.00680922\n",
            "Iteration 5889, loss = 0.00680520\n",
            "Iteration 5890, loss = 0.00680116\n",
            "Iteration 5891, loss = 0.00679710\n",
            "Iteration 5892, loss = 0.00679304\n",
            "Iteration 5893, loss = 0.00678900\n",
            "Iteration 5894, loss = 0.00678500\n",
            "Iteration 5895, loss = 0.00678100\n",
            "Iteration 5896, loss = 0.00677699\n",
            "Iteration 5897, loss = 0.00677297\n",
            "Iteration 5898, loss = 0.00676894\n",
            "Iteration 5899, loss = 0.00676490\n",
            "Iteration 5900, loss = 0.00676084\n",
            "Iteration 5901, loss = 0.00675687\n",
            "Iteration 5902, loss = 0.00675282\n",
            "Iteration 5903, loss = 0.00674884\n",
            "Iteration 5904, loss = 0.00674485\n",
            "Iteration 5905, loss = 0.00674084\n",
            "Iteration 5906, loss = 0.00673682\n",
            "Iteration 5907, loss = 0.00673280\n",
            "Iteration 5908, loss = 0.00672882\n",
            "Iteration 5909, loss = 0.00672483\n",
            "Iteration 5910, loss = 0.00672087\n",
            "Iteration 5911, loss = 0.00671690\n",
            "Iteration 5912, loss = 0.00671291\n",
            "Iteration 5913, loss = 0.00670891\n",
            "Iteration 5914, loss = 0.00670491\n",
            "Iteration 5915, loss = 0.00670090\n",
            "Iteration 5916, loss = 0.00669705\n",
            "Iteration 5917, loss = 0.00669294\n",
            "Iteration 5918, loss = 0.00668900\n",
            "Iteration 5919, loss = 0.00668504\n",
            "Iteration 5920, loss = 0.00668108\n",
            "Iteration 5921, loss = 0.00667710\n",
            "Iteration 5922, loss = 0.00667311\n",
            "Iteration 5923, loss = 0.00666929\n",
            "Iteration 5924, loss = 0.00666522\n",
            "Iteration 5925, loss = 0.00666129\n",
            "Iteration 5926, loss = 0.00665736\n",
            "Iteration 5927, loss = 0.00665341\n",
            "Iteration 5928, loss = 0.00664945\n",
            "Iteration 5929, loss = 0.00664549\n",
            "Iteration 5930, loss = 0.00664151\n",
            "Iteration 5931, loss = 0.00663782\n",
            "Iteration 5932, loss = 0.00663363\n",
            "Iteration 5933, loss = 0.00662973\n",
            "Iteration 5934, loss = 0.00662581\n",
            "Iteration 5935, loss = 0.00662188\n",
            "Iteration 5936, loss = 0.00661793\n",
            "Iteration 5937, loss = 0.00661399\n",
            "Iteration 5938, loss = 0.00661013\n",
            "Iteration 5939, loss = 0.00660625\n",
            "Iteration 5940, loss = 0.00660235\n",
            "Iteration 5941, loss = 0.00659846\n",
            "Iteration 5942, loss = 0.00659453\n",
            "Iteration 5943, loss = 0.00659061\n",
            "Iteration 5944, loss = 0.00658667\n",
            "Iteration 5945, loss = 0.00658272\n",
            "Iteration 5946, loss = 0.00657889\n",
            "Iteration 5947, loss = 0.00657492\n",
            "Iteration 5948, loss = 0.00657105\n",
            "Iteration 5949, loss = 0.00656716\n",
            "Iteration 5950, loss = 0.00656326\n",
            "Iteration 5951, loss = 0.00655935\n",
            "Iteration 5952, loss = 0.00655543\n",
            "Iteration 5953, loss = 0.00655178\n",
            "Iteration 5954, loss = 0.00654769\n",
            "Iteration 5955, loss = 0.00654383\n",
            "Iteration 5956, loss = 0.00653998\n",
            "Iteration 5957, loss = 0.00653610\n",
            "Iteration 5958, loss = 0.00653221\n",
            "Iteration 5959, loss = 0.00652832\n",
            "Iteration 5960, loss = 0.00652454\n",
            "Iteration 5961, loss = 0.00652060\n",
            "Iteration 5962, loss = 0.00651677\n",
            "Iteration 5963, loss = 0.00651293\n",
            "Iteration 5964, loss = 0.00650907\n",
            "Iteration 5965, loss = 0.00650521\n",
            "Iteration 5966, loss = 0.00650134\n",
            "Iteration 5967, loss = 0.00649745\n",
            "Iteration 5968, loss = 0.00649373\n",
            "Iteration 5969, loss = 0.00648976\n",
            "Iteration 5970, loss = 0.00648595\n",
            "Iteration 5971, loss = 0.00648212\n",
            "Iteration 5972, loss = 0.00647828\n",
            "Iteration 5973, loss = 0.00647443\n",
            "Iteration 5974, loss = 0.00647057\n",
            "Iteration 5975, loss = 0.00646683\n",
            "Iteration 5976, loss = 0.00646294\n",
            "Iteration 5977, loss = 0.00645913\n",
            "Iteration 5978, loss = 0.00645533\n",
            "Iteration 5979, loss = 0.00645151\n",
            "Iteration 5980, loss = 0.00644768\n",
            "Iteration 5981, loss = 0.00644384\n",
            "Iteration 5982, loss = 0.00643999\n",
            "Iteration 5983, loss = 0.00643637\n",
            "Iteration 5984, loss = 0.00643237\n",
            "Iteration 5985, loss = 0.00642859\n",
            "Iteration 5986, loss = 0.00642480\n",
            "Iteration 5987, loss = 0.00642100\n",
            "Iteration 5988, loss = 0.00641718\n",
            "Iteration 5989, loss = 0.00641336\n",
            "Iteration 5990, loss = 0.00640974\n",
            "Iteration 5991, loss = 0.00640580\n",
            "Iteration 5992, loss = 0.00640203\n",
            "Iteration 5993, loss = 0.00639826\n",
            "Iteration 5994, loss = 0.00639448\n",
            "Iteration 5995, loss = 0.00639068\n",
            "Iteration 5996, loss = 0.00638688\n",
            "Iteration 5997, loss = 0.00638307\n",
            "Iteration 5998, loss = 0.00637956\n",
            "Iteration 5999, loss = 0.00637552\n",
            "Iteration 6000, loss = 0.00637178\n",
            "Iteration 6001, loss = 0.00636802\n",
            "Iteration 6002, loss = 0.00636425\n",
            "Iteration 6003, loss = 0.00636047\n",
            "Iteration 6004, loss = 0.00635671\n",
            "Iteration 6005, loss = 0.00635300\n",
            "Iteration 6006, loss = 0.00634928\n",
            "Iteration 6007, loss = 0.00634554\n",
            "Iteration 6008, loss = 0.00634181\n",
            "Iteration 6009, loss = 0.00633804\n",
            "Iteration 6010, loss = 0.00633428\n",
            "Iteration 6011, loss = 0.00633051\n",
            "Iteration 6012, loss = 0.00632672\n",
            "Iteration 6013, loss = 0.00632305\n",
            "Iteration 6014, loss = 0.00631924\n",
            "Iteration 6015, loss = 0.00631554\n",
            "Iteration 6016, loss = 0.00631180\n",
            "Iteration 6017, loss = 0.00630807\n",
            "Iteration 6018, loss = 0.00630433\n",
            "Iteration 6019, loss = 0.00630057\n",
            "Iteration 6020, loss = 0.00629706\n",
            "Iteration 6021, loss = 0.00629315\n",
            "Iteration 6022, loss = 0.00628945\n",
            "Iteration 6023, loss = 0.00628575\n",
            "Iteration 6024, loss = 0.00628204\n",
            "Iteration 6025, loss = 0.00627831\n",
            "Iteration 6026, loss = 0.00627458\n",
            "Iteration 6027, loss = 0.00627094\n",
            "Iteration 6028, loss = 0.00626718\n",
            "Iteration 6029, loss = 0.00626351\n",
            "Iteration 6030, loss = 0.00625983\n",
            "Iteration 6031, loss = 0.00625614\n",
            "Iteration 6032, loss = 0.00625243\n",
            "Iteration 6033, loss = 0.00624872\n",
            "Iteration 6034, loss = 0.00624499\n",
            "Iteration 6035, loss = 0.00624140\n",
            "Iteration 6036, loss = 0.00623763\n",
            "Iteration 6037, loss = 0.00623397\n",
            "Iteration 6038, loss = 0.00623030\n",
            "Iteration 6039, loss = 0.00622662\n",
            "Iteration 6040, loss = 0.00622293\n",
            "Iteration 6041, loss = 0.00621923\n",
            "Iteration 6042, loss = 0.00621562\n",
            "Iteration 6043, loss = 0.00621191\n",
            "Iteration 6044, loss = 0.00620827\n",
            "Iteration 6045, loss = 0.00620463\n",
            "Iteration 6046, loss = 0.00620096\n",
            "Iteration 6047, loss = 0.00619729\n",
            "Iteration 6048, loss = 0.00619361\n",
            "Iteration 6049, loss = 0.00618992\n",
            "Iteration 6050, loss = 0.00618642\n",
            "Iteration 6051, loss = 0.00618262\n",
            "Iteration 6052, loss = 0.00617900\n",
            "Iteration 6053, loss = 0.00617536\n",
            "Iteration 6054, loss = 0.00617172\n",
            "Iteration 6055, loss = 0.00616806\n",
            "Iteration 6056, loss = 0.00616439\n",
            "Iteration 6057, loss = 0.00616089\n",
            "Iteration 6058, loss = 0.00615715\n",
            "Iteration 6059, loss = 0.00615354\n",
            "Iteration 6060, loss = 0.00614993\n",
            "Iteration 6061, loss = 0.00614631\n",
            "Iteration 6062, loss = 0.00614266\n",
            "Iteration 6063, loss = 0.00613902\n",
            "Iteration 6064, loss = 0.00613537\n",
            "Iteration 6065, loss = 0.00613196\n",
            "Iteration 6066, loss = 0.00612813\n",
            "Iteration 6067, loss = 0.00612454\n",
            "Iteration 6068, loss = 0.00612094\n",
            "Iteration 6069, loss = 0.00611733\n",
            "Iteration 6070, loss = 0.00611371\n",
            "Iteration 6071, loss = 0.00611008\n",
            "Iteration 6072, loss = 0.00610668\n",
            "Iteration 6073, loss = 0.00610290\n",
            "Iteration 6074, loss = 0.00609933\n",
            "Iteration 6075, loss = 0.00609575\n",
            "Iteration 6076, loss = 0.00609216\n",
            "Iteration 6077, loss = 0.00608855\n",
            "Iteration 6078, loss = 0.00608495\n",
            "Iteration 6079, loss = 0.00608133\n",
            "Iteration 6080, loss = 0.00607801\n",
            "Iteration 6081, loss = 0.00607416\n",
            "Iteration 6082, loss = 0.00607061\n",
            "Iteration 6083, loss = 0.00606704\n",
            "Iteration 6084, loss = 0.00606346\n",
            "Iteration 6085, loss = 0.00605988\n",
            "Iteration 6086, loss = 0.00605631\n",
            "Iteration 6087, loss = 0.00605294\n",
            "Iteration 6088, loss = 0.00604920\n",
            "Iteration 6089, loss = 0.00604561\n",
            "Iteration 6090, loss = 0.00604229\n",
            "Iteration 6091, loss = 0.00603850\n",
            "Iteration 6092, loss = 0.00603499\n",
            "Iteration 6093, loss = 0.00603144\n",
            "Iteration 6094, loss = 0.00602789\n",
            "Iteration 6095, loss = 0.00602434\n",
            "Iteration 6096, loss = 0.00602076\n",
            "Iteration 6097, loss = 0.00601733\n",
            "Iteration 6098, loss = 0.00601371\n",
            "Iteration 6099, loss = 0.00601018\n",
            "Iteration 6100, loss = 0.00600668\n",
            "Iteration 6101, loss = 0.00600314\n",
            "Iteration 6102, loss = 0.00599959\n",
            "Iteration 6103, loss = 0.00599605\n",
            "Iteration 6104, loss = 0.00599248\n",
            "Iteration 6105, loss = 0.00598906\n",
            "Iteration 6106, loss = 0.00598543\n",
            "Iteration 6107, loss = 0.00598194\n",
            "Iteration 6108, loss = 0.00597843\n",
            "Iteration 6109, loss = 0.00597491\n",
            "Iteration 6110, loss = 0.00597139\n",
            "Iteration 6111, loss = 0.00596785\n",
            "Iteration 6112, loss = 0.00596432\n",
            "Iteration 6113, loss = 0.00596086\n",
            "Iteration 6114, loss = 0.00595737\n",
            "Iteration 6115, loss = 0.00595389\n",
            "Iteration 6116, loss = 0.00595039\n",
            "Iteration 6117, loss = 0.00594687\n",
            "Iteration 6118, loss = 0.00594336\n",
            "Iteration 6119, loss = 0.00593983\n",
            "Iteration 6120, loss = 0.00593630\n",
            "Iteration 6121, loss = 0.00593285\n",
            "Iteration 6122, loss = 0.00592938\n",
            "Iteration 6123, loss = 0.00592591\n",
            "Iteration 6124, loss = 0.00592242\n",
            "Iteration 6125, loss = 0.00591893\n",
            "Iteration 6126, loss = 0.00591542\n",
            "Iteration 6127, loss = 0.00591191\n",
            "Iteration 6128, loss = 0.00590860\n",
            "Iteration 6129, loss = 0.00590497\n",
            "Iteration 6130, loss = 0.00590152\n",
            "Iteration 6131, loss = 0.00589806\n",
            "Iteration 6132, loss = 0.00589459\n",
            "Iteration 6133, loss = 0.00589111\n",
            "Iteration 6134, loss = 0.00588762\n",
            "Iteration 6135, loss = 0.00588430\n",
            "Iteration 6136, loss = 0.00588072\n",
            "Iteration 6137, loss = 0.00587729\n",
            "Iteration 6138, loss = 0.00587385\n",
            "Iteration 6139, loss = 0.00587040\n",
            "Iteration 6140, loss = 0.00586694\n",
            "Iteration 6141, loss = 0.00586348\n",
            "Iteration 6142, loss = 0.00586000\n",
            "Iteration 6143, loss = 0.00585670\n",
            "Iteration 6144, loss = 0.00585312\n",
            "Iteration 6145, loss = 0.00584971\n",
            "Iteration 6146, loss = 0.00584628\n",
            "Iteration 6147, loss = 0.00584285\n",
            "Iteration 6148, loss = 0.00583940\n",
            "Iteration 6149, loss = 0.00583595\n",
            "Iteration 6150, loss = 0.00583256\n",
            "Iteration 6151, loss = 0.00582912\n",
            "Iteration 6152, loss = 0.00582572\n",
            "Iteration 6153, loss = 0.00582232\n",
            "Iteration 6154, loss = 0.00581890\n",
            "Iteration 6155, loss = 0.00581547\n",
            "Iteration 6156, loss = 0.00581204\n",
            "Iteration 6157, loss = 0.00580860\n",
            "Iteration 6158, loss = 0.00580520\n",
            "Iteration 6159, loss = 0.00580178\n",
            "Iteration 6160, loss = 0.00579840\n",
            "Iteration 6161, loss = 0.00579501\n",
            "Iteration 6162, loss = 0.00579160\n",
            "Iteration 6163, loss = 0.00578819\n",
            "Iteration 6164, loss = 0.00578477\n",
            "Iteration 6165, loss = 0.00578135\n",
            "Iteration 6166, loss = 0.00577815\n",
            "Iteration 6167, loss = 0.00577456\n",
            "Iteration 6168, loss = 0.00577119\n",
            "Iteration 6169, loss = 0.00576782\n",
            "Iteration 6170, loss = 0.00576443\n",
            "Iteration 6171, loss = 0.00576103\n",
            "Iteration 6172, loss = 0.00575763\n",
            "Iteration 6173, loss = 0.00575443\n",
            "Iteration 6174, loss = 0.00575089\n",
            "Iteration 6175, loss = 0.00574755\n",
            "Iteration 6176, loss = 0.00574419\n",
            "Iteration 6177, loss = 0.00574082\n",
            "Iteration 6178, loss = 0.00573745\n",
            "Iteration 6179, loss = 0.00573406\n",
            "Iteration 6180, loss = 0.00573067\n",
            "Iteration 6181, loss = 0.00572748\n",
            "Iteration 6182, loss = 0.00572395\n",
            "Iteration 6183, loss = 0.00572062\n",
            "Iteration 6184, loss = 0.00571728\n",
            "Iteration 6185, loss = 0.00571393\n",
            "Iteration 6186, loss = 0.00571056\n",
            "Iteration 6187, loss = 0.00570719\n",
            "Iteration 6188, loss = 0.00570392\n",
            "Iteration 6189, loss = 0.00570053\n",
            "Iteration 6190, loss = 0.00569720\n",
            "Iteration 6191, loss = 0.00569389\n",
            "Iteration 6192, loss = 0.00569055\n",
            "Iteration 6193, loss = 0.00568720\n",
            "Iteration 6194, loss = 0.00568385\n",
            "Iteration 6195, loss = 0.00568049\n",
            "Iteration 6196, loss = 0.00567721\n",
            "Iteration 6197, loss = 0.00567384\n",
            "Iteration 6198, loss = 0.00567054\n",
            "Iteration 6199, loss = 0.00566723\n",
            "Iteration 6200, loss = 0.00566391\n",
            "Iteration 6201, loss = 0.00566057\n",
            "Iteration 6202, loss = 0.00565723\n",
            "Iteration 6203, loss = 0.00565389\n",
            "Iteration 6204, loss = 0.00565081\n",
            "Iteration 6205, loss = 0.00564727\n",
            "Iteration 6206, loss = 0.00564398\n",
            "Iteration 6207, loss = 0.00564069\n",
            "Iteration 6208, loss = 0.00563738\n",
            "Iteration 6209, loss = 0.00563406\n",
            "Iteration 6210, loss = 0.00563074\n",
            "Iteration 6211, loss = 0.00562765\n",
            "Iteration 6212, loss = 0.00562417\n",
            "Iteration 6213, loss = 0.00562090\n",
            "Iteration 6214, loss = 0.00561762\n",
            "Iteration 6215, loss = 0.00561434\n",
            "Iteration 6216, loss = 0.00561104\n",
            "Iteration 6217, loss = 0.00560774\n",
            "Iteration 6218, loss = 0.00560442\n",
            "Iteration 6219, loss = 0.00560135\n",
            "Iteration 6220, loss = 0.00559787\n",
            "Iteration 6221, loss = 0.00559462\n",
            "Iteration 6222, loss = 0.00559135\n",
            "Iteration 6223, loss = 0.00558808\n",
            "Iteration 6224, loss = 0.00558480\n",
            "Iteration 6225, loss = 0.00558151\n",
            "Iteration 6226, loss = 0.00557834\n",
            "Iteration 6227, loss = 0.00557500\n",
            "Iteration 6228, loss = 0.00557176\n",
            "Iteration 6229, loss = 0.00556852\n",
            "Iteration 6230, loss = 0.00556526\n",
            "Iteration 6231, loss = 0.00556199\n",
            "Iteration 6232, loss = 0.00555873\n",
            "Iteration 6233, loss = 0.00555544\n",
            "Iteration 6234, loss = 0.00555227\n",
            "Iteration 6235, loss = 0.00554895\n",
            "Iteration 6236, loss = 0.00554573\n",
            "Iteration 6237, loss = 0.00554249\n",
            "Iteration 6238, loss = 0.00553925\n",
            "Iteration 6239, loss = 0.00553600\n",
            "Iteration 6240, loss = 0.00553274\n",
            "Iteration 6241, loss = 0.00552948\n",
            "Iteration 6242, loss = 0.00552630\n",
            "Iteration 6243, loss = 0.00552309\n",
            "Iteration 6244, loss = 0.00551988\n",
            "Iteration 6245, loss = 0.00551665\n",
            "Iteration 6246, loss = 0.00551341\n",
            "Iteration 6247, loss = 0.00551018\n",
            "Iteration 6248, loss = 0.00550692\n",
            "Iteration 6249, loss = 0.00550367\n",
            "Iteration 6250, loss = 0.00550074\n",
            "Iteration 6251, loss = 0.00549728\n",
            "Iteration 6252, loss = 0.00549415\n",
            "Iteration 6253, loss = 0.00549099\n",
            "Iteration 6254, loss = 0.00548786\n",
            "Iteration 6255, loss = 0.00548470\n",
            "Iteration 6256, loss = 0.00548150\n",
            "Iteration 6257, loss = 0.00547834\n",
            "Iteration 6258, loss = 0.00547512\n",
            "Iteration 6259, loss = 0.00547191\n",
            "Iteration 6260, loss = 0.00546871\n",
            "Iteration 6261, loss = 0.00546547\n",
            "Iteration 6262, loss = 0.00546225\n",
            "Iteration 6263, loss = 0.00545901\n",
            "Iteration 6264, loss = 0.00545576\n",
            "Iteration 6265, loss = 0.00545253\n",
            "Iteration 6266, loss = 0.00544927\n",
            "Iteration 6267, loss = 0.00544640\n",
            "Iteration 6268, loss = 0.00544315\n",
            "Iteration 6269, loss = 0.00543978\n",
            "Iteration 6270, loss = 0.00543665\n",
            "Iteration 6271, loss = 0.00543354\n",
            "Iteration 6272, loss = 0.00543038\n",
            "Iteration 6273, loss = 0.00542723\n",
            "Iteration 6274, loss = 0.00542407\n",
            "Iteration 6275, loss = 0.00542088\n",
            "Iteration 6276, loss = 0.00541771\n",
            "Iteration 6277, loss = 0.00541451\n",
            "Iteration 6278, loss = 0.00541132\n",
            "Iteration 6279, loss = 0.00540812\n",
            "Iteration 6280, loss = 0.00540491\n",
            "Iteration 6281, loss = 0.00540195\n",
            "Iteration 6282, loss = 0.00539863\n",
            "Iteration 6283, loss = 0.00539554\n",
            "Iteration 6284, loss = 0.00539244\n",
            "Iteration 6285, loss = 0.00538934\n",
            "Iteration 6286, loss = 0.00538625\n",
            "Iteration 6287, loss = 0.00538311\n",
            "Iteration 6288, loss = 0.00537999\n",
            "Iteration 6289, loss = 0.00537685\n",
            "Iteration 6290, loss = 0.00537369\n",
            "Iteration 6291, loss = 0.00537054\n",
            "Iteration 6292, loss = 0.00536737\n",
            "Iteration 6293, loss = 0.00536420\n",
            "Iteration 6294, loss = 0.00536103\n",
            "Iteration 6295, loss = 0.00535784\n",
            "Iteration 6296, loss = 0.00535466\n",
            "Iteration 6297, loss = 0.00535195\n",
            "Iteration 6298, loss = 0.00534872\n",
            "Iteration 6299, loss = 0.00534538\n",
            "Iteration 6300, loss = 0.00534228\n",
            "Iteration 6301, loss = 0.00533924\n",
            "Iteration 6302, loss = 0.00533615\n",
            "Iteration 6303, loss = 0.00533304\n",
            "Iteration 6304, loss = 0.00532997\n",
            "Iteration 6305, loss = 0.00532683\n",
            "Iteration 6306, loss = 0.00532372\n",
            "Iteration 6307, loss = 0.00532059\n",
            "Iteration 6308, loss = 0.00531744\n",
            "Iteration 6309, loss = 0.00531432\n",
            "Iteration 6310, loss = 0.00531116\n",
            "Iteration 6311, loss = 0.00530818\n",
            "Iteration 6312, loss = 0.00530494\n",
            "Iteration 6313, loss = 0.00530186\n",
            "Iteration 6314, loss = 0.00529877\n",
            "Iteration 6315, loss = 0.00529570\n",
            "Iteration 6316, loss = 0.00529264\n",
            "Iteration 6317, loss = 0.00528960\n",
            "Iteration 6318, loss = 0.00528655\n",
            "Iteration 6319, loss = 0.00528348\n",
            "Iteration 6320, loss = 0.00528040\n",
            "Iteration 6321, loss = 0.00527732\n",
            "Iteration 6322, loss = 0.00527423\n",
            "Iteration 6323, loss = 0.00527113\n",
            "Iteration 6324, loss = 0.00526803\n",
            "Iteration 6325, loss = 0.00526492\n",
            "Iteration 6326, loss = 0.00526214\n",
            "Iteration 6327, loss = 0.00525890\n",
            "Iteration 6328, loss = 0.00525582\n",
            "Iteration 6329, loss = 0.00525281\n",
            "Iteration 6330, loss = 0.00524982\n",
            "Iteration 6331, loss = 0.00524680\n",
            "Iteration 6332, loss = 0.00524377\n",
            "Iteration 6333, loss = 0.00524074\n",
            "Iteration 6334, loss = 0.00523767\n",
            "Iteration 6335, loss = 0.00523462\n",
            "Iteration 6336, loss = 0.00523155\n",
            "Iteration 6337, loss = 0.00522847\n",
            "Iteration 6338, loss = 0.00522540\n",
            "Iteration 6339, loss = 0.00522231\n",
            "Iteration 6340, loss = 0.00521922\n",
            "Iteration 6341, loss = 0.00521613\n",
            "Iteration 6342, loss = 0.00521331\n",
            "Iteration 6343, loss = 0.00521018\n",
            "Iteration 6344, loss = 0.00520710\n",
            "Iteration 6345, loss = 0.00520410\n",
            "Iteration 6346, loss = 0.00520115\n",
            "Iteration 6347, loss = 0.00519814\n",
            "Iteration 6348, loss = 0.00519513\n",
            "Iteration 6349, loss = 0.00519213\n",
            "Iteration 6350, loss = 0.00518908\n",
            "Iteration 6351, loss = 0.00518606\n",
            "Iteration 6352, loss = 0.00518302\n",
            "Iteration 6353, loss = 0.00517996\n",
            "Iteration 6354, loss = 0.00517692\n",
            "Iteration 6355, loss = 0.00517386\n",
            "Iteration 6356, loss = 0.00517080\n",
            "Iteration 6357, loss = 0.00516815\n",
            "Iteration 6358, loss = 0.00516501\n",
            "Iteration 6359, loss = 0.00516188\n",
            "Iteration 6360, loss = 0.00515890\n",
            "Iteration 6361, loss = 0.00515597\n",
            "Iteration 6362, loss = 0.00515301\n",
            "Iteration 6363, loss = 0.00515001\n",
            "Iteration 6364, loss = 0.00514706\n",
            "Iteration 6365, loss = 0.00514399\n",
            "Iteration 6366, loss = 0.00514095\n",
            "Iteration 6367, loss = 0.00513792\n",
            "Iteration 6368, loss = 0.00513499\n",
            "Iteration 6369, loss = 0.00513196\n",
            "Iteration 6370, loss = 0.00512903\n",
            "Iteration 6371, loss = 0.00512606\n",
            "Iteration 6372, loss = 0.00512318\n",
            "Iteration 6373, loss = 0.00512020\n",
            "Iteration 6374, loss = 0.00511724\n",
            "Iteration 6375, loss = 0.00511429\n",
            "Iteration 6376, loss = 0.00511128\n",
            "Iteration 6377, loss = 0.00510832\n",
            "Iteration 6378, loss = 0.00510532\n",
            "Iteration 6379, loss = 0.00510231\n",
            "Iteration 6380, loss = 0.00509933\n",
            "Iteration 6381, loss = 0.00509631\n",
            "Iteration 6382, loss = 0.00509344\n",
            "Iteration 6383, loss = 0.00509036\n",
            "Iteration 6384, loss = 0.00508742\n",
            "Iteration 6385, loss = 0.00508447\n",
            "Iteration 6386, loss = 0.00508150\n",
            "Iteration 6387, loss = 0.00507861\n",
            "Iteration 6388, loss = 0.00507570\n",
            "Iteration 6389, loss = 0.00507278\n",
            "Iteration 6390, loss = 0.00506985\n",
            "Iteration 6391, loss = 0.00506692\n",
            "Iteration 6392, loss = 0.00506397\n",
            "Iteration 6393, loss = 0.00506101\n",
            "Iteration 6394, loss = 0.00505805\n",
            "Iteration 6395, loss = 0.00505509\n",
            "Iteration 6396, loss = 0.00505212\n",
            "Iteration 6397, loss = 0.00504937\n",
            "Iteration 6398, loss = 0.00504626\n",
            "Iteration 6399, loss = 0.00504342\n",
            "Iteration 6400, loss = 0.00504055\n",
            "Iteration 6401, loss = 0.00503769\n",
            "Iteration 6402, loss = 0.00503481\n",
            "Iteration 6403, loss = 0.00503191\n",
            "Iteration 6404, loss = 0.00502901\n",
            "Iteration 6405, loss = 0.00502608\n",
            "Iteration 6406, loss = 0.00502317\n",
            "Iteration 6407, loss = 0.00502024\n",
            "Iteration 6408, loss = 0.00501729\n",
            "Iteration 6409, loss = 0.00501436\n",
            "Iteration 6410, loss = 0.00501141\n",
            "Iteration 6411, loss = 0.00500846\n",
            "Iteration 6412, loss = 0.00500550\n",
            "Iteration 6413, loss = 0.00500255\n",
            "Iteration 6414, loss = 0.00499968\n",
            "Iteration 6415, loss = 0.00499678\n",
            "Iteration 6416, loss = 0.00499387\n",
            "Iteration 6417, loss = 0.00499102\n",
            "Iteration 6418, loss = 0.00498812\n",
            "Iteration 6419, loss = 0.00498529\n",
            "Iteration 6420, loss = 0.00498240\n",
            "Iteration 6421, loss = 0.00497954\n",
            "Iteration 6422, loss = 0.00497666\n",
            "Iteration 6423, loss = 0.00497375\n",
            "Iteration 6424, loss = 0.00497087\n",
            "Iteration 6425, loss = 0.00496795\n",
            "Iteration 6426, loss = 0.00496504\n",
            "Iteration 6427, loss = 0.00496221\n",
            "Iteration 6428, loss = 0.00495928\n",
            "Iteration 6429, loss = 0.00495644\n",
            "Iteration 6430, loss = 0.00495356\n",
            "Iteration 6431, loss = 0.00495070\n",
            "Iteration 6432, loss = 0.00494782\n",
            "Iteration 6433, loss = 0.00494492\n",
            "Iteration 6434, loss = 0.00494224\n",
            "Iteration 6435, loss = 0.00493922\n",
            "Iteration 6436, loss = 0.00493638\n",
            "Iteration 6437, loss = 0.00493355\n",
            "Iteration 6438, loss = 0.00493068\n",
            "Iteration 6439, loss = 0.00492782\n",
            "Iteration 6440, loss = 0.00492496\n",
            "Iteration 6441, loss = 0.00492208\n",
            "Iteration 6442, loss = 0.00491946\n",
            "Iteration 6443, loss = 0.00491639\n",
            "Iteration 6444, loss = 0.00491357\n",
            "Iteration 6445, loss = 0.00491074\n",
            "Iteration 6446, loss = 0.00490790\n",
            "Iteration 6447, loss = 0.00490505\n",
            "Iteration 6448, loss = 0.00490219\n",
            "Iteration 6449, loss = 0.00489944\n",
            "Iteration 6450, loss = 0.00489656\n",
            "Iteration 6451, loss = 0.00489374\n",
            "Iteration 6452, loss = 0.00489093\n",
            "Iteration 6453, loss = 0.00488811\n",
            "Iteration 6454, loss = 0.00488527\n",
            "Iteration 6455, loss = 0.00488244\n",
            "Iteration 6456, loss = 0.00487958\n",
            "Iteration 6457, loss = 0.00487675\n",
            "Iteration 6458, loss = 0.00487395\n",
            "Iteration 6459, loss = 0.00487116\n",
            "Iteration 6460, loss = 0.00486835\n",
            "Iteration 6461, loss = 0.00486554\n",
            "Iteration 6462, loss = 0.00486272\n",
            "Iteration 6463, loss = 0.00485989\n",
            "Iteration 6464, loss = 0.00485706\n",
            "Iteration 6465, loss = 0.00485431\n",
            "Iteration 6466, loss = 0.00485146\n",
            "Iteration 6467, loss = 0.00484867\n",
            "Iteration 6468, loss = 0.00484588\n",
            "Iteration 6469, loss = 0.00484308\n",
            "Iteration 6470, loss = 0.00484027\n",
            "Iteration 6471, loss = 0.00483746\n",
            "Iteration 6472, loss = 0.00483464\n",
            "Iteration 6473, loss = 0.00483207\n",
            "Iteration 6474, loss = 0.00482906\n",
            "Iteration 6475, loss = 0.00482630\n",
            "Iteration 6476, loss = 0.00482352\n",
            "Iteration 6477, loss = 0.00482073\n",
            "Iteration 6478, loss = 0.00481794\n",
            "Iteration 6479, loss = 0.00481514\n",
            "Iteration 6480, loss = 0.00481249\n",
            "Iteration 6481, loss = 0.00480960\n",
            "Iteration 6482, loss = 0.00480685\n",
            "Iteration 6483, loss = 0.00480409\n",
            "Iteration 6484, loss = 0.00480132\n",
            "Iteration 6485, loss = 0.00479854\n",
            "Iteration 6486, loss = 0.00479576\n",
            "Iteration 6487, loss = 0.00479297\n",
            "Iteration 6488, loss = 0.00479026\n",
            "Iteration 6489, loss = 0.00478744\n",
            "Iteration 6490, loss = 0.00478471\n",
            "Iteration 6491, loss = 0.00478195\n",
            "Iteration 6492, loss = 0.00477920\n",
            "Iteration 6493, loss = 0.00477643\n",
            "Iteration 6494, loss = 0.00477366\n",
            "Iteration 6495, loss = 0.00477088\n",
            "Iteration 6496, loss = 0.00476826\n",
            "Iteration 6497, loss = 0.00476538\n",
            "Iteration 6498, loss = 0.00476266\n",
            "Iteration 6499, loss = 0.00475992\n",
            "Iteration 6500, loss = 0.00475718\n",
            "Iteration 6501, loss = 0.00475442\n",
            "Iteration 6502, loss = 0.00475166\n",
            "Iteration 6503, loss = 0.00474896\n",
            "Iteration 6504, loss = 0.00474620\n",
            "Iteration 6505, loss = 0.00474349\n",
            "Iteration 6506, loss = 0.00474077\n",
            "Iteration 6507, loss = 0.00473804\n",
            "Iteration 6508, loss = 0.00473530\n",
            "Iteration 6509, loss = 0.00473256\n",
            "Iteration 6510, loss = 0.00472981\n",
            "Iteration 6511, loss = 0.00472706\n",
            "Iteration 6512, loss = 0.00472457\n",
            "Iteration 6513, loss = 0.00472163\n",
            "Iteration 6514, loss = 0.00471900\n",
            "Iteration 6515, loss = 0.00471633\n",
            "Iteration 6516, loss = 0.00471368\n",
            "Iteration 6517, loss = 0.00471100\n",
            "Iteration 6518, loss = 0.00470830\n",
            "Iteration 6519, loss = 0.00470562\n",
            "Iteration 6520, loss = 0.00470290\n",
            "Iteration 6521, loss = 0.00470019\n",
            "Iteration 6522, loss = 0.00469747\n",
            "Iteration 6523, loss = 0.00469473\n",
            "Iteration 6524, loss = 0.00469201\n",
            "Iteration 6525, loss = 0.00468926\n",
            "Iteration 6526, loss = 0.00468652\n",
            "Iteration 6527, loss = 0.00468378\n",
            "Iteration 6528, loss = 0.00468102\n",
            "Iteration 6529, loss = 0.00467841\n",
            "Iteration 6530, loss = 0.00467564\n",
            "Iteration 6531, loss = 0.00467298\n",
            "Iteration 6532, loss = 0.00467034\n",
            "Iteration 6533, loss = 0.00466771\n",
            "Iteration 6534, loss = 0.00466503\n",
            "Iteration 6535, loss = 0.00466237\n",
            "Iteration 6536, loss = 0.00465969\n",
            "Iteration 6537, loss = 0.00465700\n",
            "Iteration 6538, loss = 0.00465431\n",
            "Iteration 6539, loss = 0.00465161\n",
            "Iteration 6540, loss = 0.00464890\n",
            "Iteration 6541, loss = 0.00464619\n",
            "Iteration 6542, loss = 0.00464347\n",
            "Iteration 6543, loss = 0.00464076\n",
            "Iteration 6544, loss = 0.00463833\n",
            "Iteration 6545, loss = 0.00463554\n",
            "Iteration 6546, loss = 0.00463284\n",
            "Iteration 6547, loss = 0.00463019\n",
            "Iteration 6548, loss = 0.00462760\n",
            "Iteration 6549, loss = 0.00462496\n",
            "Iteration 6550, loss = 0.00462231\n",
            "Iteration 6551, loss = 0.00461968\n",
            "Iteration 6552, loss = 0.00461700\n",
            "Iteration 6553, loss = 0.00461434\n",
            "Iteration 6554, loss = 0.00461167\n",
            "Iteration 6555, loss = 0.00460898\n",
            "Iteration 6556, loss = 0.00460631\n",
            "Iteration 6557, loss = 0.00460361\n",
            "Iteration 6558, loss = 0.00460092\n",
            "Iteration 6559, loss = 0.00459837\n",
            "Iteration 6560, loss = 0.00459560\n",
            "Iteration 6561, loss = 0.00459298\n",
            "Iteration 6562, loss = 0.00459032\n",
            "Iteration 6563, loss = 0.00458777\n",
            "Iteration 6564, loss = 0.00458508\n",
            "Iteration 6565, loss = 0.00458249\n",
            "Iteration 6566, loss = 0.00457988\n",
            "Iteration 6567, loss = 0.00457726\n",
            "Iteration 6568, loss = 0.00457464\n",
            "Iteration 6569, loss = 0.00457200\n",
            "Iteration 6570, loss = 0.00456936\n",
            "Iteration 6571, loss = 0.00456671\n",
            "Iteration 6572, loss = 0.00456405\n",
            "Iteration 6573, loss = 0.00456143\n",
            "Iteration 6574, loss = 0.00455881\n",
            "Iteration 6575, loss = 0.00455621\n",
            "Iteration 6576, loss = 0.00455359\n",
            "Iteration 6577, loss = 0.00455098\n",
            "Iteration 6578, loss = 0.00454835\n",
            "Iteration 6579, loss = 0.00454572\n",
            "Iteration 6580, loss = 0.00454322\n",
            "Iteration 6581, loss = 0.00454052\n",
            "Iteration 6582, loss = 0.00453793\n",
            "Iteration 6583, loss = 0.00453535\n",
            "Iteration 6584, loss = 0.00453274\n",
            "Iteration 6585, loss = 0.00453013\n",
            "Iteration 6586, loss = 0.00452752\n",
            "Iteration 6587, loss = 0.00452489\n",
            "Iteration 6588, loss = 0.00452244\n",
            "Iteration 6589, loss = 0.00451971\n",
            "Iteration 6590, loss = 0.00451714\n",
            "Iteration 6591, loss = 0.00451456\n",
            "Iteration 6592, loss = 0.00451197\n",
            "Iteration 6593, loss = 0.00450937\n",
            "Iteration 6594, loss = 0.00450677\n",
            "Iteration 6595, loss = 0.00450419\n",
            "Iteration 6596, loss = 0.00450163\n",
            "Iteration 6597, loss = 0.00449906\n",
            "Iteration 6598, loss = 0.00449650\n",
            "Iteration 6599, loss = 0.00449393\n",
            "Iteration 6600, loss = 0.00449134\n",
            "Iteration 6601, loss = 0.00448876\n",
            "Iteration 6602, loss = 0.00448616\n",
            "Iteration 6603, loss = 0.00448356\n",
            "Iteration 6604, loss = 0.00448118\n",
            "Iteration 6605, loss = 0.00447842\n",
            "Iteration 6606, loss = 0.00447588\n",
            "Iteration 6607, loss = 0.00447330\n",
            "Iteration 6608, loss = 0.00447075\n",
            "Iteration 6609, loss = 0.00446817\n",
            "Iteration 6610, loss = 0.00446564\n",
            "Iteration 6611, loss = 0.00446310\n",
            "Iteration 6612, loss = 0.00446055\n",
            "Iteration 6613, loss = 0.00445802\n",
            "Iteration 6614, loss = 0.00445548\n",
            "Iteration 6615, loss = 0.00445290\n",
            "Iteration 6616, loss = 0.00445036\n",
            "Iteration 6617, loss = 0.00444778\n",
            "Iteration 6618, loss = 0.00444521\n",
            "Iteration 6619, loss = 0.00444268\n",
            "Iteration 6620, loss = 0.00444012\n",
            "Iteration 6621, loss = 0.00443761\n",
            "Iteration 6622, loss = 0.00443505\n",
            "Iteration 6623, loss = 0.00443253\n",
            "Iteration 6624, loss = 0.00442998\n",
            "Iteration 6625, loss = 0.00442741\n",
            "Iteration 6626, loss = 0.00442488\n",
            "Iteration 6627, loss = 0.00442237\n",
            "Iteration 6628, loss = 0.00441985\n",
            "Iteration 6629, loss = 0.00441735\n",
            "Iteration 6630, loss = 0.00441481\n",
            "Iteration 6631, loss = 0.00441228\n",
            "Iteration 6632, loss = 0.00440975\n",
            "Iteration 6633, loss = 0.00440719\n",
            "Iteration 6634, loss = 0.00440466\n",
            "Iteration 6635, loss = 0.00440215\n",
            "Iteration 6636, loss = 0.00439967\n",
            "Iteration 6637, loss = 0.00439715\n",
            "Iteration 6638, loss = 0.00439464\n",
            "Iteration 6639, loss = 0.00439212\n",
            "Iteration 6640, loss = 0.00438959\n",
            "Iteration 6641, loss = 0.00438706\n",
            "Iteration 6642, loss = 0.00438461\n",
            "Iteration 6643, loss = 0.00438204\n",
            "Iteration 6644, loss = 0.00437955\n",
            "Iteration 6645, loss = 0.00437706\n",
            "Iteration 6646, loss = 0.00437456\n",
            "Iteration 6647, loss = 0.00437205\n",
            "Iteration 6648, loss = 0.00436953\n",
            "Iteration 6649, loss = 0.00436703\n",
            "Iteration 6650, loss = 0.00436455\n",
            "Iteration 6651, loss = 0.00436208\n",
            "Iteration 6652, loss = 0.00435960\n",
            "Iteration 6653, loss = 0.00435711\n",
            "Iteration 6654, loss = 0.00435462\n",
            "Iteration 6655, loss = 0.00435211\n",
            "Iteration 6656, loss = 0.00434961\n",
            "Iteration 6657, loss = 0.00434709\n",
            "Iteration 6658, loss = 0.00434479\n",
            "Iteration 6659, loss = 0.00434213\n",
            "Iteration 6660, loss = 0.00433966\n",
            "Iteration 6661, loss = 0.00433719\n",
            "Iteration 6662, loss = 0.00433471\n",
            "Iteration 6663, loss = 0.00433222\n",
            "Iteration 6664, loss = 0.00432978\n",
            "Iteration 6665, loss = 0.00432732\n",
            "Iteration 6666, loss = 0.00432486\n",
            "Iteration 6667, loss = 0.00432241\n",
            "Iteration 6668, loss = 0.00431995\n",
            "Iteration 6669, loss = 0.00431747\n",
            "Iteration 6670, loss = 0.00431501\n",
            "Iteration 6671, loss = 0.00431252\n",
            "Iteration 6672, loss = 0.00431003\n",
            "Iteration 6673, loss = 0.00430759\n",
            "Iteration 6674, loss = 0.00430511\n",
            "Iteration 6675, loss = 0.00430268\n",
            "Iteration 6676, loss = 0.00430025\n",
            "Iteration 6677, loss = 0.00429774\n",
            "Iteration 6678, loss = 0.00429551\n",
            "Iteration 6679, loss = 0.00429293\n",
            "Iteration 6680, loss = 0.00429049\n",
            "Iteration 6681, loss = 0.00428805\n",
            "Iteration 6682, loss = 0.00428568\n",
            "Iteration 6683, loss = 0.00428326\n",
            "Iteration 6684, loss = 0.00428082\n",
            "Iteration 6685, loss = 0.00427841\n",
            "Iteration 6686, loss = 0.00427594\n",
            "Iteration 6687, loss = 0.00427351\n",
            "Iteration 6688, loss = 0.00427106\n",
            "Iteration 6689, loss = 0.00426858\n",
            "Iteration 6690, loss = 0.00426614\n",
            "Iteration 6691, loss = 0.00426366\n",
            "Iteration 6692, loss = 0.00426119\n",
            "Iteration 6693, loss = 0.00425872\n",
            "Iteration 6694, loss = 0.00425650\n",
            "Iteration 6695, loss = 0.00425399\n",
            "Iteration 6696, loss = 0.00425151\n",
            "Iteration 6697, loss = 0.00424910\n",
            "Iteration 6698, loss = 0.00424676\n",
            "Iteration 6699, loss = 0.00424433\n",
            "Iteration 6700, loss = 0.00424194\n",
            "Iteration 6701, loss = 0.00423954\n",
            "Iteration 6702, loss = 0.00423709\n",
            "Iteration 6703, loss = 0.00423469\n",
            "Iteration 6704, loss = 0.00423225\n",
            "Iteration 6705, loss = 0.00422981\n",
            "Iteration 6706, loss = 0.00422738\n",
            "Iteration 6707, loss = 0.00422492\n",
            "Iteration 6708, loss = 0.00422249\n",
            "Iteration 6709, loss = 0.00422014\n",
            "Iteration 6710, loss = 0.00421765\n",
            "Iteration 6711, loss = 0.00421527\n",
            "Iteration 6712, loss = 0.00421284\n",
            "Iteration 6713, loss = 0.00421049\n",
            "Iteration 6714, loss = 0.00420808\n",
            "Iteration 6715, loss = 0.00420574\n",
            "Iteration 6716, loss = 0.00420336\n",
            "Iteration 6717, loss = 0.00420097\n",
            "Iteration 6718, loss = 0.00419859\n",
            "Iteration 6719, loss = 0.00419618\n",
            "Iteration 6720, loss = 0.00419379\n",
            "Iteration 6721, loss = 0.00419138\n",
            "Iteration 6722, loss = 0.00418896\n",
            "Iteration 6723, loss = 0.00418656\n",
            "Iteration 6724, loss = 0.00418428\n",
            "Iteration 6725, loss = 0.00418179\n",
            "Iteration 6726, loss = 0.00417942\n",
            "Iteration 6727, loss = 0.00417703\n",
            "Iteration 6728, loss = 0.00417467\n",
            "Iteration 6729, loss = 0.00417226\n",
            "Iteration 6730, loss = 0.00417001\n",
            "Iteration 6731, loss = 0.00416757\n",
            "Iteration 6732, loss = 0.00416519\n",
            "Iteration 6733, loss = 0.00416286\n",
            "Iteration 6734, loss = 0.00416049\n",
            "Iteration 6735, loss = 0.00415811\n",
            "Iteration 6736, loss = 0.00415575\n",
            "Iteration 6737, loss = 0.00415335\n",
            "Iteration 6738, loss = 0.00415097\n",
            "Iteration 6739, loss = 0.00414874\n",
            "Iteration 6740, loss = 0.00414626\n",
            "Iteration 6741, loss = 0.00414393\n",
            "Iteration 6742, loss = 0.00414156\n",
            "Iteration 6743, loss = 0.00413922\n",
            "Iteration 6744, loss = 0.00413685\n",
            "Iteration 6745, loss = 0.00413448\n",
            "Iteration 6746, loss = 0.00413219\n",
            "Iteration 6747, loss = 0.00412980\n",
            "Iteration 6748, loss = 0.00412747\n",
            "Iteration 6749, loss = 0.00412515\n",
            "Iteration 6750, loss = 0.00412280\n",
            "Iteration 6751, loss = 0.00412046\n",
            "Iteration 6752, loss = 0.00411811\n",
            "Iteration 6753, loss = 0.00411574\n",
            "Iteration 6754, loss = 0.00411339\n",
            "Iteration 6755, loss = 0.00411123\n",
            "Iteration 6756, loss = 0.00410872\n",
            "Iteration 6757, loss = 0.00410640\n",
            "Iteration 6758, loss = 0.00410406\n",
            "Iteration 6759, loss = 0.00410175\n",
            "Iteration 6760, loss = 0.00409940\n",
            "Iteration 6761, loss = 0.00409706\n",
            "Iteration 6762, loss = 0.00409480\n",
            "Iteration 6763, loss = 0.00409247\n",
            "Iteration 6764, loss = 0.00409018\n",
            "Iteration 6765, loss = 0.00408786\n",
            "Iteration 6766, loss = 0.00408553\n",
            "Iteration 6767, loss = 0.00408322\n",
            "Iteration 6768, loss = 0.00408087\n",
            "Iteration 6769, loss = 0.00407854\n",
            "Iteration 6770, loss = 0.00407620\n",
            "Iteration 6771, loss = 0.00407400\n",
            "Iteration 6772, loss = 0.00407159\n",
            "Iteration 6773, loss = 0.00406927\n",
            "Iteration 6774, loss = 0.00406696\n",
            "Iteration 6775, loss = 0.00406467\n",
            "Iteration 6776, loss = 0.00406232\n",
            "Iteration 6777, loss = 0.00406008\n",
            "Iteration 6778, loss = 0.00405777\n",
            "Iteration 6779, loss = 0.00405546\n",
            "Iteration 6780, loss = 0.00405321\n",
            "Iteration 6781, loss = 0.00405090\n",
            "Iteration 6782, loss = 0.00404860\n",
            "Iteration 6783, loss = 0.00404630\n",
            "Iteration 6784, loss = 0.00404397\n",
            "Iteration 6785, loss = 0.00404167\n",
            "Iteration 6786, loss = 0.00403938\n",
            "Iteration 6787, loss = 0.00403709\n",
            "Iteration 6788, loss = 0.00403483\n",
            "Iteration 6789, loss = 0.00403253\n",
            "Iteration 6790, loss = 0.00403026\n",
            "Iteration 6791, loss = 0.00402796\n",
            "Iteration 6792, loss = 0.00402566\n",
            "Iteration 6793, loss = 0.00402337\n",
            "Iteration 6794, loss = 0.00402122\n",
            "Iteration 6795, loss = 0.00401879\n",
            "Iteration 6796, loss = 0.00401653\n",
            "Iteration 6797, loss = 0.00401427\n",
            "Iteration 6798, loss = 0.00401199\n",
            "Iteration 6799, loss = 0.00400971\n",
            "Iteration 6800, loss = 0.00400743\n",
            "Iteration 6801, loss = 0.00400522\n",
            "Iteration 6802, loss = 0.00400291\n",
            "Iteration 6803, loss = 0.00400066\n",
            "Iteration 6804, loss = 0.00399841\n",
            "Iteration 6805, loss = 0.00399615\n",
            "Iteration 6806, loss = 0.00399388\n",
            "Iteration 6807, loss = 0.00399161\n",
            "Iteration 6808, loss = 0.00398933\n",
            "Iteration 6809, loss = 0.00398705\n",
            "Iteration 6810, loss = 0.00398495\n",
            "Iteration 6811, loss = 0.00398253\n",
            "Iteration 6812, loss = 0.00398030\n",
            "Iteration 6813, loss = 0.00397805\n",
            "Iteration 6814, loss = 0.00397580\n",
            "Iteration 6815, loss = 0.00397354\n",
            "Iteration 6816, loss = 0.00397127\n",
            "Iteration 6817, loss = 0.00396916\n",
            "Iteration 6818, loss = 0.00396680\n",
            "Iteration 6819, loss = 0.00396457\n",
            "Iteration 6820, loss = 0.00396235\n",
            "Iteration 6821, loss = 0.00396011\n",
            "Iteration 6822, loss = 0.00395786\n",
            "Iteration 6823, loss = 0.00395561\n",
            "Iteration 6824, loss = 0.00395335\n",
            "Iteration 6825, loss = 0.00395117\n",
            "Iteration 6826, loss = 0.00394889\n",
            "Iteration 6827, loss = 0.00394668\n",
            "Iteration 6828, loss = 0.00394446\n",
            "Iteration 6829, loss = 0.00394223\n",
            "Iteration 6830, loss = 0.00394000\n",
            "Iteration 6831, loss = 0.00393775\n",
            "Iteration 6832, loss = 0.00393551\n",
            "Iteration 6833, loss = 0.00393332\n",
            "Iteration 6834, loss = 0.00393107\n",
            "Iteration 6835, loss = 0.00392886\n",
            "Iteration 6836, loss = 0.00392665\n",
            "Iteration 6837, loss = 0.00392443\n",
            "Iteration 6838, loss = 0.00392221\n",
            "Iteration 6839, loss = 0.00391998\n",
            "Iteration 6840, loss = 0.00391774\n",
            "Iteration 6841, loss = 0.00391563\n",
            "Iteration 6842, loss = 0.00391332\n",
            "Iteration 6843, loss = 0.00391113\n",
            "Iteration 6844, loss = 0.00390893\n",
            "Iteration 6845, loss = 0.00390673\n",
            "Iteration 6846, loss = 0.00390451\n",
            "Iteration 6847, loss = 0.00390229\n",
            "Iteration 6848, loss = 0.00390007\n",
            "Iteration 6849, loss = 0.00389803\n",
            "Iteration 6850, loss = 0.00389567\n",
            "Iteration 6851, loss = 0.00389349\n",
            "Iteration 6852, loss = 0.00389130\n",
            "Iteration 6853, loss = 0.00388910\n",
            "Iteration 6854, loss = 0.00388690\n",
            "Iteration 6855, loss = 0.00388469\n",
            "Iteration 6856, loss = 0.00388253\n",
            "Iteration 6857, loss = 0.00388033\n",
            "Iteration 6858, loss = 0.00387815\n",
            "Iteration 6859, loss = 0.00387598\n",
            "Iteration 6860, loss = 0.00387380\n",
            "Iteration 6861, loss = 0.00387161\n",
            "Iteration 6862, loss = 0.00386941\n",
            "Iteration 6863, loss = 0.00386721\n",
            "Iteration 6864, loss = 0.00386501\n",
            "Iteration 6865, loss = 0.00386293\n",
            "Iteration 6866, loss = 0.00386065\n",
            "Iteration 6867, loss = 0.00385849\n",
            "Iteration 6868, loss = 0.00385631\n",
            "Iteration 6869, loss = 0.00385414\n",
            "Iteration 6870, loss = 0.00385196\n",
            "Iteration 6871, loss = 0.00384977\n",
            "Iteration 6872, loss = 0.00384768\n",
            "Iteration 6873, loss = 0.00384544\n",
            "Iteration 6874, loss = 0.00384329\n",
            "Iteration 6875, loss = 0.00384114\n",
            "Iteration 6876, loss = 0.00383898\n",
            "Iteration 6877, loss = 0.00383681\n",
            "Iteration 6878, loss = 0.00383464\n",
            "Iteration 6879, loss = 0.00383245\n",
            "Iteration 6880, loss = 0.00383030\n",
            "Iteration 6881, loss = 0.00382814\n",
            "Iteration 6882, loss = 0.00382601\n",
            "Iteration 6883, loss = 0.00382386\n",
            "Iteration 6884, loss = 0.00382171\n",
            "Iteration 6885, loss = 0.00381955\n",
            "Iteration 6886, loss = 0.00381738\n",
            "Iteration 6887, loss = 0.00381521\n",
            "Iteration 6888, loss = 0.00381306\n",
            "Iteration 6889, loss = 0.00381092\n",
            "Iteration 6890, loss = 0.00380879\n",
            "Iteration 6891, loss = 0.00380665\n",
            "Iteration 6892, loss = 0.00380451\n",
            "Iteration 6893, loss = 0.00380236\n",
            "Iteration 6894, loss = 0.00380021\n",
            "Iteration 6895, loss = 0.00379805\n",
            "Iteration 6896, loss = 0.00379597\n",
            "Iteration 6897, loss = 0.00379378\n",
            "Iteration 6898, loss = 0.00379166\n",
            "Iteration 6899, loss = 0.00378953\n",
            "Iteration 6900, loss = 0.00378740\n",
            "Iteration 6901, loss = 0.00378526\n",
            "Iteration 6902, loss = 0.00378312\n",
            "Iteration 6903, loss = 0.00378097\n",
            "Iteration 6904, loss = 0.00377897\n",
            "Iteration 6905, loss = 0.00377672\n",
            "Iteration 6906, loss = 0.00377461\n",
            "Iteration 6907, loss = 0.00377249\n",
            "Iteration 6908, loss = 0.00377037\n",
            "Iteration 6909, loss = 0.00376824\n",
            "Iteration 6910, loss = 0.00376611\n",
            "Iteration 6911, loss = 0.00376400\n",
            "Iteration 6912, loss = 0.00376189\n",
            "Iteration 6913, loss = 0.00375979\n",
            "Iteration 6914, loss = 0.00375769\n",
            "Iteration 6915, loss = 0.00375559\n",
            "Iteration 6916, loss = 0.00375347\n",
            "Iteration 6917, loss = 0.00375135\n",
            "Iteration 6918, loss = 0.00374922\n",
            "Iteration 6919, loss = 0.00374709\n",
            "Iteration 6920, loss = 0.00374507\n",
            "Iteration 6921, loss = 0.00374288\n",
            "Iteration 6922, loss = 0.00374079\n",
            "Iteration 6923, loss = 0.00373869\n",
            "Iteration 6924, loss = 0.00373660\n",
            "Iteration 6925, loss = 0.00373448\n",
            "Iteration 6926, loss = 0.00373237\n",
            "Iteration 6927, loss = 0.00373033\n",
            "Iteration 6928, loss = 0.00372819\n",
            "Iteration 6929, loss = 0.00372611\n",
            "Iteration 6930, loss = 0.00372403\n",
            "Iteration 6931, loss = 0.00372194\n",
            "Iteration 6932, loss = 0.00371984\n",
            "Iteration 6933, loss = 0.00371774\n",
            "Iteration 6934, loss = 0.00371563\n",
            "Iteration 6935, loss = 0.00371353\n",
            "Iteration 6936, loss = 0.00371147\n",
            "Iteration 6937, loss = 0.00370941\n",
            "Iteration 6938, loss = 0.00370733\n",
            "Iteration 6939, loss = 0.00370525\n",
            "Iteration 6940, loss = 0.00370317\n",
            "Iteration 6941, loss = 0.00370107\n",
            "Iteration 6942, loss = 0.00369898\n",
            "Iteration 6943, loss = 0.00369688\n",
            "Iteration 6944, loss = 0.00369483\n",
            "Iteration 6945, loss = 0.00369277\n",
            "Iteration 6946, loss = 0.00369071\n",
            "Iteration 6947, loss = 0.00368864\n",
            "Iteration 6948, loss = 0.00368656\n",
            "Iteration 6949, loss = 0.00368448\n",
            "Iteration 6950, loss = 0.00368239\n",
            "Iteration 6951, loss = 0.00368037\n",
            "Iteration 6952, loss = 0.00367827\n",
            "Iteration 6953, loss = 0.00367622\n",
            "Iteration 6954, loss = 0.00367416\n",
            "Iteration 6955, loss = 0.00367211\n",
            "Iteration 6956, loss = 0.00367004\n",
            "Iteration 6957, loss = 0.00366796\n",
            "Iteration 6958, loss = 0.00366589\n",
            "Iteration 6959, loss = 0.00366394\n",
            "Iteration 6960, loss = 0.00366178\n",
            "Iteration 6961, loss = 0.00365974\n",
            "Iteration 6962, loss = 0.00365770\n",
            "Iteration 6963, loss = 0.00365565\n",
            "Iteration 6964, loss = 0.00365359\n",
            "Iteration 6965, loss = 0.00365153\n",
            "Iteration 6966, loss = 0.00364947\n",
            "Iteration 6967, loss = 0.00364759\n",
            "Iteration 6968, loss = 0.00364538\n",
            "Iteration 6969, loss = 0.00364335\n",
            "Iteration 6970, loss = 0.00364132\n",
            "Iteration 6971, loss = 0.00363928\n",
            "Iteration 6972, loss = 0.00363723\n",
            "Iteration 6973, loss = 0.00363518\n",
            "Iteration 6974, loss = 0.00363318\n",
            "Iteration 6975, loss = 0.00363113\n",
            "Iteration 6976, loss = 0.00362911\n",
            "Iteration 6977, loss = 0.00362709\n",
            "Iteration 6978, loss = 0.00362506\n",
            "Iteration 6979, loss = 0.00362303\n",
            "Iteration 6980, loss = 0.00362099\n",
            "Iteration 6981, loss = 0.00361894\n",
            "Iteration 6982, loss = 0.00361690\n",
            "Iteration 6983, loss = 0.00361497\n",
            "Iteration 6984, loss = 0.00361285\n",
            "Iteration 6985, loss = 0.00361084\n",
            "Iteration 6986, loss = 0.00360884\n",
            "Iteration 6987, loss = 0.00360679\n",
            "Iteration 6988, loss = 0.00360478\n",
            "Iteration 6989, loss = 0.00360278\n",
            "Iteration 6990, loss = 0.00360076\n",
            "Iteration 6991, loss = 0.00359875\n",
            "Iteration 6992, loss = 0.00359672\n",
            "Iteration 6993, loss = 0.00359469\n",
            "Iteration 6994, loss = 0.00359280\n",
            "Iteration 6995, loss = 0.00359069\n",
            "Iteration 6996, loss = 0.00358869\n",
            "Iteration 6997, loss = 0.00358670\n",
            "Iteration 6998, loss = 0.00358469\n",
            "Iteration 6999, loss = 0.00358268\n",
            "Iteration 7000, loss = 0.00358067\n",
            "Iteration 7001, loss = 0.00357864\n",
            "Iteration 7002, loss = 0.00357674\n",
            "Iteration 7003, loss = 0.00357465\n",
            "Iteration 7004, loss = 0.00357267\n",
            "Iteration 7005, loss = 0.00357068\n",
            "Iteration 7006, loss = 0.00356869\n",
            "Iteration 7007, loss = 0.00356669\n",
            "Iteration 7008, loss = 0.00356468\n",
            "Iteration 7009, loss = 0.00356267\n",
            "Iteration 7010, loss = 0.00356074\n",
            "Iteration 7011, loss = 0.00355869\n",
            "Iteration 7012, loss = 0.00355672\n",
            "Iteration 7013, loss = 0.00355474\n",
            "Iteration 7014, loss = 0.00355276\n",
            "Iteration 7015, loss = 0.00355077\n",
            "Iteration 7016, loss = 0.00354877\n",
            "Iteration 7017, loss = 0.00354677\n",
            "Iteration 7018, loss = 0.00354484\n",
            "Iteration 7019, loss = 0.00354282\n",
            "Iteration 7020, loss = 0.00354086\n",
            "Iteration 7021, loss = 0.00353888\n",
            "Iteration 7022, loss = 0.00353691\n",
            "Iteration 7023, loss = 0.00353493\n",
            "Iteration 7024, loss = 0.00353294\n",
            "Iteration 7025, loss = 0.00353095\n",
            "Iteration 7026, loss = 0.00352901\n",
            "Iteration 7027, loss = 0.00352701\n",
            "Iteration 7028, loss = 0.00352506\n",
            "Iteration 7029, loss = 0.00352310\n",
            "Iteration 7030, loss = 0.00352114\n",
            "Iteration 7031, loss = 0.00351917\n",
            "Iteration 7032, loss = 0.00351719\n",
            "Iteration 7033, loss = 0.00351521\n",
            "Iteration 7034, loss = 0.00351326\n",
            "Iteration 7035, loss = 0.00351129\n",
            "Iteration 7036, loss = 0.00350935\n",
            "Iteration 7037, loss = 0.00350740\n",
            "Iteration 7038, loss = 0.00350545\n",
            "Iteration 7039, loss = 0.00350348\n",
            "Iteration 7040, loss = 0.00350152\n",
            "Iteration 7041, loss = 0.00349955\n",
            "Iteration 7042, loss = 0.00349760\n",
            "Iteration 7043, loss = 0.00349565\n",
            "Iteration 7044, loss = 0.00349371\n",
            "Iteration 7045, loss = 0.00349177\n",
            "Iteration 7046, loss = 0.00348983\n",
            "Iteration 7047, loss = 0.00348788\n",
            "Iteration 7048, loss = 0.00348592\n",
            "Iteration 7049, loss = 0.00348396\n",
            "Iteration 7050, loss = 0.00348201\n",
            "Iteration 7051, loss = 0.00348008\n",
            "Iteration 7052, loss = 0.00347816\n",
            "Iteration 7053, loss = 0.00347623\n",
            "Iteration 7054, loss = 0.00347429\n",
            "Iteration 7055, loss = 0.00347235\n",
            "Iteration 7056, loss = 0.00347040\n",
            "Iteration 7057, loss = 0.00346845\n",
            "Iteration 7058, loss = 0.00346650\n",
            "Iteration 7059, loss = 0.00346459\n",
            "Iteration 7060, loss = 0.00346268\n",
            "Iteration 7061, loss = 0.00346075\n",
            "Iteration 7062, loss = 0.00345883\n",
            "Iteration 7063, loss = 0.00345689\n",
            "Iteration 7064, loss = 0.00345496\n",
            "Iteration 7065, loss = 0.00345301\n",
            "Iteration 7066, loss = 0.00345107\n",
            "Iteration 7067, loss = 0.00344917\n",
            "Iteration 7068, loss = 0.00344727\n",
            "Iteration 7069, loss = 0.00344536\n",
            "Iteration 7070, loss = 0.00344344\n",
            "Iteration 7071, loss = 0.00344152\n",
            "Iteration 7072, loss = 0.00343959\n",
            "Iteration 7073, loss = 0.00343766\n",
            "Iteration 7074, loss = 0.00343572\n",
            "Iteration 7075, loss = 0.00343399\n",
            "Iteration 7076, loss = 0.00343191\n",
            "Iteration 7077, loss = 0.00343007\n",
            "Iteration 7078, loss = 0.00342819\n",
            "Iteration 7079, loss = 0.00342634\n",
            "Iteration 7080, loss = 0.00342446\n",
            "Iteration 7081, loss = 0.00342257\n",
            "Iteration 7082, loss = 0.00342069\n",
            "Iteration 7083, loss = 0.00341877\n",
            "Iteration 7084, loss = 0.00341687\n",
            "Iteration 7085, loss = 0.00341496\n",
            "Iteration 7086, loss = 0.00341304\n",
            "Iteration 7087, loss = 0.00341113\n",
            "Iteration 7088, loss = 0.00340920\n",
            "Iteration 7089, loss = 0.00340727\n",
            "Iteration 7090, loss = 0.00340534\n",
            "Iteration 7091, loss = 0.00340341\n",
            "Iteration 7092, loss = 0.00340148\n",
            "Iteration 7093, loss = 0.00339975\n",
            "Iteration 7094, loss = 0.00339780\n",
            "Iteration 7095, loss = 0.00339583\n",
            "Iteration 7096, loss = 0.00339398\n",
            "Iteration 7097, loss = 0.00339212\n",
            "Iteration 7098, loss = 0.00339026\n",
            "Iteration 7099, loss = 0.00338838\n",
            "Iteration 7100, loss = 0.00338651\n",
            "Iteration 7101, loss = 0.00338462\n",
            "Iteration 7102, loss = 0.00338273\n",
            "Iteration 7103, loss = 0.00338083\n",
            "Iteration 7104, loss = 0.00337893\n",
            "Iteration 7105, loss = 0.00337703\n",
            "Iteration 7106, loss = 0.00337513\n",
            "Iteration 7107, loss = 0.00337322\n",
            "Iteration 7108, loss = 0.00337146\n",
            "Iteration 7109, loss = 0.00336948\n",
            "Iteration 7110, loss = 0.00336766\n",
            "Iteration 7111, loss = 0.00336581\n",
            "Iteration 7112, loss = 0.00336399\n",
            "Iteration 7113, loss = 0.00336214\n",
            "Iteration 7114, loss = 0.00336028\n",
            "Iteration 7115, loss = 0.00335843\n",
            "Iteration 7116, loss = 0.00335655\n",
            "Iteration 7117, loss = 0.00335469\n",
            "Iteration 7118, loss = 0.00335281\n",
            "Iteration 7119, loss = 0.00335093\n",
            "Iteration 7120, loss = 0.00334905\n",
            "Iteration 7121, loss = 0.00334716\n",
            "Iteration 7122, loss = 0.00334527\n",
            "Iteration 7123, loss = 0.00334338\n",
            "Iteration 7124, loss = 0.00334154\n",
            "Iteration 7125, loss = 0.00333966\n",
            "Iteration 7126, loss = 0.00333780\n",
            "Iteration 7127, loss = 0.00333594\n",
            "Iteration 7128, loss = 0.00333413\n",
            "Iteration 7129, loss = 0.00333226\n",
            "Iteration 7130, loss = 0.00333046\n",
            "Iteration 7131, loss = 0.00332861\n",
            "Iteration 7132, loss = 0.00332678\n",
            "Iteration 7133, loss = 0.00332494\n",
            "Iteration 7134, loss = 0.00332308\n",
            "Iteration 7135, loss = 0.00332124\n",
            "Iteration 7136, loss = 0.00331937\n",
            "Iteration 7137, loss = 0.00331751\n",
            "Iteration 7138, loss = 0.00331565\n",
            "Iteration 7139, loss = 0.00331388\n",
            "Iteration 7140, loss = 0.00331198\n",
            "Iteration 7141, loss = 0.00331014\n",
            "Iteration 7142, loss = 0.00330830\n",
            "Iteration 7143, loss = 0.00330648\n",
            "Iteration 7144, loss = 0.00330461\n",
            "Iteration 7145, loss = 0.00330283\n",
            "Iteration 7146, loss = 0.00330099\n",
            "Iteration 7147, loss = 0.00329916\n",
            "Iteration 7148, loss = 0.00329737\n",
            "Iteration 7149, loss = 0.00329553\n",
            "Iteration 7150, loss = 0.00329370\n",
            "Iteration 7151, loss = 0.00329188\n",
            "Iteration 7152, loss = 0.00329002\n",
            "Iteration 7153, loss = 0.00328819\n",
            "Iteration 7154, loss = 0.00328634\n",
            "Iteration 7155, loss = 0.00328468\n",
            "Iteration 7156, loss = 0.00328273\n",
            "Iteration 7157, loss = 0.00328096\n",
            "Iteration 7158, loss = 0.00327917\n",
            "Iteration 7159, loss = 0.00327742\n",
            "Iteration 7160, loss = 0.00327561\n",
            "Iteration 7161, loss = 0.00327382\n",
            "Iteration 7162, loss = 0.00327202\n",
            "Iteration 7163, loss = 0.00327020\n",
            "Iteration 7164, loss = 0.00326840\n",
            "Iteration 7165, loss = 0.00326657\n",
            "Iteration 7166, loss = 0.00326475\n",
            "Iteration 7167, loss = 0.00326292\n",
            "Iteration 7168, loss = 0.00326108\n",
            "Iteration 7169, loss = 0.00325926\n",
            "Iteration 7170, loss = 0.00325741\n",
            "Iteration 7171, loss = 0.00325557\n",
            "Iteration 7172, loss = 0.00325374\n",
            "Iteration 7173, loss = 0.00325218\n",
            "Iteration 7174, loss = 0.00325032\n",
            "Iteration 7175, loss = 0.00324836\n",
            "Iteration 7176, loss = 0.00324660\n",
            "Iteration 7177, loss = 0.00324483\n",
            "Iteration 7178, loss = 0.00324306\n",
            "Iteration 7179, loss = 0.00324127\n",
            "Iteration 7180, loss = 0.00323949\n",
            "Iteration 7181, loss = 0.00323769\n",
            "Iteration 7182, loss = 0.00323589\n",
            "Iteration 7183, loss = 0.00323409\n",
            "Iteration 7184, loss = 0.00323228\n",
            "Iteration 7185, loss = 0.00323047\n",
            "Iteration 7186, loss = 0.00322865\n",
            "Iteration 7187, loss = 0.00322684\n",
            "Iteration 7188, loss = 0.00322523\n",
            "Iteration 7189, loss = 0.00322334\n",
            "Iteration 7190, loss = 0.00322155\n",
            "Iteration 7191, loss = 0.00321979\n",
            "Iteration 7192, loss = 0.00321805\n",
            "Iteration 7193, loss = 0.00321630\n",
            "Iteration 7194, loss = 0.00321452\n",
            "Iteration 7195, loss = 0.00321277\n",
            "Iteration 7196, loss = 0.00321098\n",
            "Iteration 7197, loss = 0.00320920\n",
            "Iteration 7198, loss = 0.00320742\n",
            "Iteration 7199, loss = 0.00320562\n",
            "Iteration 7200, loss = 0.00320384\n",
            "Iteration 7201, loss = 0.00320204\n",
            "Iteration 7202, loss = 0.00320024\n",
            "Iteration 7203, loss = 0.00319844\n",
            "Iteration 7204, loss = 0.00319671\n",
            "Iteration 7205, loss = 0.00319490\n",
            "Iteration 7206, loss = 0.00319313\n",
            "Iteration 7207, loss = 0.00319136\n",
            "Iteration 7208, loss = 0.00318964\n",
            "Iteration 7209, loss = 0.00318786\n",
            "Iteration 7210, loss = 0.00318615\n",
            "Iteration 7211, loss = 0.00318438\n",
            "Iteration 7212, loss = 0.00318264\n",
            "Iteration 7213, loss = 0.00318089\n",
            "Iteration 7214, loss = 0.00317912\n",
            "Iteration 7215, loss = 0.00317737\n",
            "Iteration 7216, loss = 0.00317559\n",
            "Iteration 7217, loss = 0.00317382\n",
            "Iteration 7218, loss = 0.00317204\n",
            "Iteration 7219, loss = 0.00317035\n",
            "Iteration 7220, loss = 0.00316855\n",
            "Iteration 7221, loss = 0.00316680\n",
            "Iteration 7222, loss = 0.00316505\n",
            "Iteration 7223, loss = 0.00316331\n",
            "Iteration 7224, loss = 0.00316154\n",
            "Iteration 7225, loss = 0.00315982\n",
            "Iteration 7226, loss = 0.00315810\n",
            "Iteration 7227, loss = 0.00315635\n",
            "Iteration 7228, loss = 0.00315464\n",
            "Iteration 7229, loss = 0.00315290\n",
            "Iteration 7230, loss = 0.00315115\n",
            "Iteration 7231, loss = 0.00314942\n",
            "Iteration 7232, loss = 0.00314765\n",
            "Iteration 7233, loss = 0.00314591\n",
            "Iteration 7234, loss = 0.00314415\n",
            "Iteration 7235, loss = 0.00314254\n",
            "Iteration 7236, loss = 0.00314070\n",
            "Iteration 7237, loss = 0.00313896\n",
            "Iteration 7238, loss = 0.00313724\n",
            "Iteration 7239, loss = 0.00313552\n",
            "Iteration 7240, loss = 0.00313376\n",
            "Iteration 7241, loss = 0.00313206\n",
            "Iteration 7242, loss = 0.00313035\n",
            "Iteration 7243, loss = 0.00312862\n",
            "Iteration 7244, loss = 0.00312693\n",
            "Iteration 7245, loss = 0.00312520\n",
            "Iteration 7246, loss = 0.00312348\n",
            "Iteration 7247, loss = 0.00312176\n",
            "Iteration 7248, loss = 0.00312001\n",
            "Iteration 7249, loss = 0.00311829\n",
            "Iteration 7250, loss = 0.00311654\n",
            "Iteration 7251, loss = 0.00311490\n",
            "Iteration 7252, loss = 0.00311313\n",
            "Iteration 7253, loss = 0.00311140\n",
            "Iteration 7254, loss = 0.00310971\n",
            "Iteration 7255, loss = 0.00310799\n",
            "Iteration 7256, loss = 0.00310626\n",
            "Iteration 7257, loss = 0.00310455\n",
            "Iteration 7258, loss = 0.00310293\n",
            "Iteration 7259, loss = 0.00310113\n",
            "Iteration 7260, loss = 0.00309944\n",
            "Iteration 7261, loss = 0.00309774\n",
            "Iteration 7262, loss = 0.00309604\n",
            "Iteration 7263, loss = 0.00309434\n",
            "Iteration 7264, loss = 0.00309262\n",
            "Iteration 7265, loss = 0.00309091\n",
            "Iteration 7266, loss = 0.00308919\n",
            "Iteration 7267, loss = 0.00308752\n",
            "Iteration 7268, loss = 0.00308584\n",
            "Iteration 7269, loss = 0.00308414\n",
            "Iteration 7270, loss = 0.00308246\n",
            "Iteration 7271, loss = 0.00308075\n",
            "Iteration 7272, loss = 0.00307905\n",
            "Iteration 7273, loss = 0.00307734\n",
            "Iteration 7274, loss = 0.00307563\n",
            "Iteration 7275, loss = 0.00307404\n",
            "Iteration 7276, loss = 0.00307224\n",
            "Iteration 7277, loss = 0.00307057\n",
            "Iteration 7278, loss = 0.00306888\n",
            "Iteration 7279, loss = 0.00306720\n",
            "Iteration 7280, loss = 0.00306551\n",
            "Iteration 7281, loss = 0.00306381\n",
            "Iteration 7282, loss = 0.00306216\n",
            "Iteration 7283, loss = 0.00306046\n",
            "Iteration 7284, loss = 0.00305879\n",
            "Iteration 7285, loss = 0.00305712\n",
            "Iteration 7286, loss = 0.00305544\n",
            "Iteration 7287, loss = 0.00305376\n",
            "Iteration 7288, loss = 0.00305208\n",
            "Iteration 7289, loss = 0.00305038\n",
            "Iteration 7290, loss = 0.00304869\n",
            "Iteration 7291, loss = 0.00304709\n",
            "Iteration 7292, loss = 0.00304535\n",
            "Iteration 7293, loss = 0.00304369\n",
            "Iteration 7294, loss = 0.00304201\n",
            "Iteration 7295, loss = 0.00304035\n",
            "Iteration 7296, loss = 0.00303867\n",
            "Iteration 7297, loss = 0.00303699\n",
            "Iteration 7298, loss = 0.00303531\n",
            "Iteration 7299, loss = 0.00303378\n",
            "Iteration 7300, loss = 0.00303197\n",
            "Iteration 7301, loss = 0.00303032\n",
            "Iteration 7302, loss = 0.00302867\n",
            "Iteration 7303, loss = 0.00302701\n",
            "Iteration 7304, loss = 0.00302534\n",
            "Iteration 7305, loss = 0.00302367\n",
            "Iteration 7306, loss = 0.00302201\n",
            "Iteration 7307, loss = 0.00302037\n",
            "Iteration 7308, loss = 0.00301873\n",
            "Iteration 7309, loss = 0.00301708\n",
            "Iteration 7310, loss = 0.00301543\n",
            "Iteration 7311, loss = 0.00301377\n",
            "Iteration 7312, loss = 0.00301211\n",
            "Iteration 7313, loss = 0.00301045\n",
            "Iteration 7314, loss = 0.00300878\n",
            "Iteration 7315, loss = 0.00300711\n",
            "Iteration 7316, loss = 0.00300562\n",
            "Iteration 7317, loss = 0.00300384\n",
            "Iteration 7318, loss = 0.00300229\n",
            "Iteration 7319, loss = 0.00300059\n",
            "Iteration 7320, loss = 0.00299898\n",
            "Iteration 7321, loss = 0.00299733\n",
            "Iteration 7322, loss = 0.00299568\n",
            "Iteration 7323, loss = 0.00299405\n",
            "Iteration 7324, loss = 0.00299238\n",
            "Iteration 7325, loss = 0.00299074\n",
            "Iteration 7326, loss = 0.00298907\n",
            "Iteration 7327, loss = 0.00298741\n",
            "Iteration 7328, loss = 0.00298578\n",
            "Iteration 7329, loss = 0.00298413\n",
            "Iteration 7330, loss = 0.00298250\n",
            "Iteration 7331, loss = 0.00298088\n",
            "Iteration 7332, loss = 0.00297923\n",
            "Iteration 7333, loss = 0.00297759\n",
            "Iteration 7334, loss = 0.00297595\n",
            "Iteration 7335, loss = 0.00297445\n",
            "Iteration 7336, loss = 0.00297269\n",
            "Iteration 7337, loss = 0.00297108\n",
            "Iteration 7338, loss = 0.00296946\n",
            "Iteration 7339, loss = 0.00296784\n",
            "Iteration 7340, loss = 0.00296621\n",
            "Iteration 7341, loss = 0.00296458\n",
            "Iteration 7342, loss = 0.00296294\n",
            "Iteration 7343, loss = 0.00296134\n",
            "Iteration 7344, loss = 0.00295971\n",
            "Iteration 7345, loss = 0.00295810\n",
            "Iteration 7346, loss = 0.00295649\n",
            "Iteration 7347, loss = 0.00295488\n",
            "Iteration 7348, loss = 0.00295326\n",
            "Iteration 7349, loss = 0.00295163\n",
            "Iteration 7350, loss = 0.00295000\n",
            "Iteration 7351, loss = 0.00294837\n",
            "Iteration 7352, loss = 0.00294684\n",
            "Iteration 7353, loss = 0.00294515\n",
            "Iteration 7354, loss = 0.00294355\n",
            "Iteration 7355, loss = 0.00294194\n",
            "Iteration 7356, loss = 0.00294034\n",
            "Iteration 7357, loss = 0.00293872\n",
            "Iteration 7358, loss = 0.00293710\n",
            "Iteration 7359, loss = 0.00293548\n",
            "Iteration 7360, loss = 0.00293400\n",
            "Iteration 7361, loss = 0.00293227\n",
            "Iteration 7362, loss = 0.00293068\n",
            "Iteration 7363, loss = 0.00292909\n",
            "Iteration 7364, loss = 0.00292749\n",
            "Iteration 7365, loss = 0.00292588\n",
            "Iteration 7366, loss = 0.00292427\n",
            "Iteration 7367, loss = 0.00292266\n",
            "Iteration 7368, loss = 0.00292114\n",
            "Iteration 7369, loss = 0.00291948\n",
            "Iteration 7370, loss = 0.00291790\n",
            "Iteration 7371, loss = 0.00291631\n",
            "Iteration 7372, loss = 0.00291472\n",
            "Iteration 7373, loss = 0.00291312\n",
            "Iteration 7374, loss = 0.00291152\n",
            "Iteration 7375, loss = 0.00290991\n",
            "Iteration 7376, loss = 0.00290831\n",
            "Iteration 7377, loss = 0.00290674\n",
            "Iteration 7378, loss = 0.00290517\n",
            "Iteration 7379, loss = 0.00290359\n",
            "Iteration 7380, loss = 0.00290201\n",
            "Iteration 7381, loss = 0.00290042\n",
            "Iteration 7382, loss = 0.00289882\n",
            "Iteration 7383, loss = 0.00289723\n",
            "Iteration 7384, loss = 0.00289563\n",
            "Iteration 7385, loss = 0.00289410\n",
            "Iteration 7386, loss = 0.00289247\n",
            "Iteration 7387, loss = 0.00289090\n",
            "Iteration 7388, loss = 0.00288933\n",
            "Iteration 7389, loss = 0.00288775\n",
            "Iteration 7390, loss = 0.00288617\n",
            "Iteration 7391, loss = 0.00288458\n",
            "Iteration 7392, loss = 0.00288299\n",
            "Iteration 7393, loss = 0.00288152\n",
            "Iteration 7394, loss = 0.00287985\n",
            "Iteration 7395, loss = 0.00287829\n",
            "Iteration 7396, loss = 0.00287673\n",
            "Iteration 7397, loss = 0.00287516\n",
            "Iteration 7398, loss = 0.00287359\n",
            "Iteration 7399, loss = 0.00287201\n",
            "Iteration 7400, loss = 0.00287043\n",
            "Iteration 7401, loss = 0.00286892\n",
            "Iteration 7402, loss = 0.00286731\n",
            "Iteration 7403, loss = 0.00286575\n",
            "Iteration 7404, loss = 0.00286420\n",
            "Iteration 7405, loss = 0.00286264\n",
            "Iteration 7406, loss = 0.00286107\n",
            "Iteration 7407, loss = 0.00285951\n",
            "Iteration 7408, loss = 0.00285793\n",
            "Iteration 7409, loss = 0.00285636\n",
            "Iteration 7410, loss = 0.00285494\n",
            "Iteration 7411, loss = 0.00285324\n",
            "Iteration 7412, loss = 0.00285170\n",
            "Iteration 7413, loss = 0.00285015\n",
            "Iteration 7414, loss = 0.00284860\n",
            "Iteration 7415, loss = 0.00284704\n",
            "Iteration 7416, loss = 0.00284548\n",
            "Iteration 7417, loss = 0.00284396\n",
            "Iteration 7418, loss = 0.00284239\n",
            "Iteration 7419, loss = 0.00284085\n",
            "Iteration 7420, loss = 0.00283932\n",
            "Iteration 7421, loss = 0.00283777\n",
            "Iteration 7422, loss = 0.00283623\n",
            "Iteration 7423, loss = 0.00283468\n",
            "Iteration 7424, loss = 0.00283312\n",
            "Iteration 7425, loss = 0.00283156\n",
            "Iteration 7426, loss = 0.00283001\n",
            "Iteration 7427, loss = 0.00282848\n",
            "Iteration 7428, loss = 0.00282695\n",
            "Iteration 7429, loss = 0.00282541\n",
            "Iteration 7430, loss = 0.00282388\n",
            "Iteration 7431, loss = 0.00282233\n",
            "Iteration 7432, loss = 0.00282079\n",
            "Iteration 7433, loss = 0.00281924\n",
            "Iteration 7434, loss = 0.00281771\n",
            "Iteration 7435, loss = 0.00281617\n",
            "Iteration 7436, loss = 0.00281465\n",
            "Iteration 7437, loss = 0.00281313\n",
            "Iteration 7438, loss = 0.00281160\n",
            "Iteration 7439, loss = 0.00281006\n",
            "Iteration 7440, loss = 0.00280852\n",
            "Iteration 7441, loss = 0.00280698\n",
            "Iteration 7442, loss = 0.00280544\n",
            "Iteration 7443, loss = 0.00280403\n",
            "Iteration 7444, loss = 0.00280239\n",
            "Iteration 7445, loss = 0.00280087\n",
            "Iteration 7446, loss = 0.00279935\n",
            "Iteration 7447, loss = 0.00279783\n",
            "Iteration 7448, loss = 0.00279630\n",
            "Iteration 7449, loss = 0.00279477\n",
            "Iteration 7450, loss = 0.00279329\n",
            "Iteration 7451, loss = 0.00279174\n",
            "Iteration 7452, loss = 0.00279024\n",
            "Iteration 7453, loss = 0.00278873\n",
            "Iteration 7454, loss = 0.00278722\n",
            "Iteration 7455, loss = 0.00278570\n",
            "Iteration 7456, loss = 0.00278418\n",
            "Iteration 7457, loss = 0.00278265\n",
            "Iteration 7458, loss = 0.00278113\n",
            "Iteration 7459, loss = 0.00277961\n",
            "Iteration 7460, loss = 0.00277811\n",
            "Iteration 7461, loss = 0.00277661\n",
            "Iteration 7462, loss = 0.00277510\n",
            "Iteration 7463, loss = 0.00277360\n",
            "Iteration 7464, loss = 0.00277208\n",
            "Iteration 7465, loss = 0.00277057\n",
            "Iteration 7466, loss = 0.00276905\n",
            "Iteration 7467, loss = 0.00276756\n",
            "Iteration 7468, loss = 0.00276604\n",
            "Iteration 7469, loss = 0.00276455\n",
            "Iteration 7470, loss = 0.00276306\n",
            "Iteration 7471, loss = 0.00276156\n",
            "Iteration 7472, loss = 0.00276006\n",
            "Iteration 7473, loss = 0.00275855\n",
            "Iteration 7474, loss = 0.00275704\n",
            "Iteration 7475, loss = 0.00275552\n",
            "Iteration 7476, loss = 0.00275415\n",
            "Iteration 7477, loss = 0.00275253\n",
            "Iteration 7478, loss = 0.00275105\n",
            "Iteration 7479, loss = 0.00274956\n",
            "Iteration 7480, loss = 0.00274807\n",
            "Iteration 7481, loss = 0.00274657\n",
            "Iteration 7482, loss = 0.00274507\n",
            "Iteration 7483, loss = 0.00274362\n",
            "Iteration 7484, loss = 0.00274210\n",
            "Iteration 7485, loss = 0.00274062\n",
            "Iteration 7486, loss = 0.00273915\n",
            "Iteration 7487, loss = 0.00273766\n",
            "Iteration 7488, loss = 0.00273618\n",
            "Iteration 7489, loss = 0.00273469\n",
            "Iteration 7490, loss = 0.00273319\n",
            "Iteration 7491, loss = 0.00273169\n",
            "Iteration 7492, loss = 0.00273021\n",
            "Iteration 7493, loss = 0.00272874\n",
            "Iteration 7494, loss = 0.00272727\n",
            "Iteration 7495, loss = 0.00272579\n",
            "Iteration 7496, loss = 0.00272432\n",
            "Iteration 7497, loss = 0.00272283\n",
            "Iteration 7498, loss = 0.00272134\n",
            "Iteration 7499, loss = 0.00271986\n",
            "Iteration 7500, loss = 0.00271839\n",
            "Iteration 7501, loss = 0.00271690\n",
            "Iteration 7502, loss = 0.00271545\n",
            "Iteration 7503, loss = 0.00271398\n",
            "Iteration 7504, loss = 0.00271251\n",
            "Iteration 7505, loss = 0.00271104\n",
            "Iteration 7506, loss = 0.00270956\n",
            "Iteration 7507, loss = 0.00270808\n",
            "Iteration 7508, loss = 0.00270659\n",
            "Iteration 7509, loss = 0.00270525\n",
            "Iteration 7510, loss = 0.00270366\n",
            "Iteration 7511, loss = 0.00270221\n",
            "Iteration 7512, loss = 0.00270075\n",
            "Iteration 7513, loss = 0.00269928\n",
            "Iteration 7514, loss = 0.00269781\n",
            "Iteration 7515, loss = 0.00269634\n",
            "Iteration 7516, loss = 0.00269492\n",
            "Iteration 7517, loss = 0.00269343\n",
            "Iteration 7518, loss = 0.00269199\n",
            "Iteration 7519, loss = 0.00269054\n",
            "Iteration 7520, loss = 0.00268909\n",
            "Iteration 7521, loss = 0.00268763\n",
            "Iteration 7522, loss = 0.00268617\n",
            "Iteration 7523, loss = 0.00268470\n",
            "Iteration 7524, loss = 0.00268323\n",
            "Iteration 7525, loss = 0.00268178\n",
            "Iteration 7526, loss = 0.00268033\n",
            "Iteration 7527, loss = 0.00267889\n",
            "Iteration 7528, loss = 0.00267744\n",
            "Iteration 7529, loss = 0.00267600\n",
            "Iteration 7530, loss = 0.00267454\n",
            "Iteration 7531, loss = 0.00267309\n",
            "Iteration 7532, loss = 0.00267163\n",
            "Iteration 7533, loss = 0.00267019\n",
            "Iteration 7534, loss = 0.00266874\n",
            "Iteration 7535, loss = 0.00266731\n",
            "Iteration 7536, loss = 0.00266587\n",
            "Iteration 7537, loss = 0.00266443\n",
            "Iteration 7538, loss = 0.00266298\n",
            "Iteration 7539, loss = 0.00266153\n",
            "Iteration 7540, loss = 0.00266008\n",
            "Iteration 7541, loss = 0.00265863\n",
            "Iteration 7542, loss = 0.00265730\n",
            "Iteration 7543, loss = 0.00265575\n",
            "Iteration 7544, loss = 0.00265433\n",
            "Iteration 7545, loss = 0.00265290\n",
            "Iteration 7546, loss = 0.00265146\n",
            "Iteration 7547, loss = 0.00265002\n",
            "Iteration 7548, loss = 0.00264858\n",
            "Iteration 7549, loss = 0.00264718\n",
            "Iteration 7550, loss = 0.00264573\n",
            "Iteration 7551, loss = 0.00264431\n",
            "Iteration 7552, loss = 0.00264289\n",
            "Iteration 7553, loss = 0.00264147\n",
            "Iteration 7554, loss = 0.00264004\n",
            "Iteration 7555, loss = 0.00263860\n",
            "Iteration 7556, loss = 0.00263717\n",
            "Iteration 7557, loss = 0.00263573\n",
            "Iteration 7558, loss = 0.00263429\n",
            "Iteration 7559, loss = 0.00263289\n",
            "Iteration 7560, loss = 0.00263147\n",
            "Iteration 7561, loss = 0.00263005\n",
            "Iteration 7562, loss = 0.00262864\n",
            "Iteration 7563, loss = 0.00262721\n",
            "Iteration 7564, loss = 0.00262578\n",
            "Iteration 7565, loss = 0.00262435\n",
            "Iteration 7566, loss = 0.00262293\n",
            "Iteration 7567, loss = 0.00262152\n",
            "Iteration 7568, loss = 0.00262011\n",
            "Iteration 7569, loss = 0.00261871\n",
            "Iteration 7570, loss = 0.00261729\n",
            "Iteration 7571, loss = 0.00261588\n",
            "Iteration 7572, loss = 0.00261446\n",
            "Iteration 7573, loss = 0.00261303\n",
            "Iteration 7574, loss = 0.00261160\n",
            "Iteration 7575, loss = 0.00261029\n",
            "Iteration 7576, loss = 0.00260879\n",
            "Iteration 7577, loss = 0.00260739\n",
            "Iteration 7578, loss = 0.00260599\n",
            "Iteration 7579, loss = 0.00260458\n",
            "Iteration 7580, loss = 0.00260317\n",
            "Iteration 7581, loss = 0.00260176\n",
            "Iteration 7582, loss = 0.00260036\n",
            "Iteration 7583, loss = 0.00259896\n",
            "Iteration 7584, loss = 0.00259757\n",
            "Iteration 7585, loss = 0.00259618\n",
            "Iteration 7586, loss = 0.00259478\n",
            "Iteration 7587, loss = 0.00259338\n",
            "Iteration 7588, loss = 0.00259198\n",
            "Iteration 7589, loss = 0.00259056\n",
            "Iteration 7590, loss = 0.00258915\n",
            "Iteration 7591, loss = 0.00258774\n",
            "Iteration 7592, loss = 0.00258648\n",
            "Iteration 7593, loss = 0.00258496\n",
            "Iteration 7594, loss = 0.00258362\n",
            "Iteration 7595, loss = 0.00258225\n",
            "Iteration 7596, loss = 0.00258090\n",
            "Iteration 7597, loss = 0.00257952\n",
            "Iteration 7598, loss = 0.00257815\n",
            "Iteration 7599, loss = 0.00257677\n",
            "Iteration 7600, loss = 0.00257538\n",
            "Iteration 7601, loss = 0.00257399\n",
            "Iteration 7602, loss = 0.00257259\n",
            "Iteration 7603, loss = 0.00257119\n",
            "Iteration 7604, loss = 0.00256980\n",
            "Iteration 7605, loss = 0.00256839\n",
            "Iteration 7606, loss = 0.00256698\n",
            "Iteration 7607, loss = 0.00256558\n",
            "Iteration 7608, loss = 0.00256416\n",
            "Iteration 7609, loss = 0.00256276\n",
            "Iteration 7610, loss = 0.00256143\n",
            "Iteration 7611, loss = 0.00256000\n",
            "Iteration 7612, loss = 0.00255863\n",
            "Iteration 7613, loss = 0.00255728\n",
            "Iteration 7614, loss = 0.00255593\n",
            "Iteration 7615, loss = 0.00255457\n",
            "Iteration 7616, loss = 0.00255321\n",
            "Iteration 7617, loss = 0.00255184\n",
            "Iteration 7618, loss = 0.00255046\n",
            "Iteration 7619, loss = 0.00254908\n",
            "Iteration 7620, loss = 0.00254770\n",
            "Iteration 7621, loss = 0.00254631\n",
            "Iteration 7622, loss = 0.00254492\n",
            "Iteration 7623, loss = 0.00254353\n",
            "Iteration 7624, loss = 0.00254214\n",
            "Iteration 7625, loss = 0.00254074\n",
            "Iteration 7626, loss = 0.00253948\n",
            "Iteration 7627, loss = 0.00253804\n",
            "Iteration 7628, loss = 0.00253669\n",
            "Iteration 7629, loss = 0.00253534\n",
            "Iteration 7630, loss = 0.00253401\n",
            "Iteration 7631, loss = 0.00253266\n",
            "Iteration 7632, loss = 0.00253131\n",
            "Iteration 7633, loss = 0.00252996\n",
            "Iteration 7634, loss = 0.00252859\n",
            "Iteration 7635, loss = 0.00252723\n",
            "Iteration 7636, loss = 0.00252586\n",
            "Iteration 7637, loss = 0.00252448\n",
            "Iteration 7638, loss = 0.00252311\n",
            "Iteration 7639, loss = 0.00252173\n",
            "Iteration 7640, loss = 0.00252035\n",
            "Iteration 7641, loss = 0.00251897\n",
            "Iteration 7642, loss = 0.00251760\n",
            "Iteration 7643, loss = 0.00251626\n",
            "Iteration 7644, loss = 0.00251490\n",
            "Iteration 7645, loss = 0.00251354\n",
            "Iteration 7646, loss = 0.00251219\n",
            "Iteration 7647, loss = 0.00251085\n",
            "Iteration 7648, loss = 0.00250953\n",
            "Iteration 7649, loss = 0.00250818\n",
            "Iteration 7650, loss = 0.00250683\n",
            "Iteration 7651, loss = 0.00250551\n",
            "Iteration 7652, loss = 0.00250414\n",
            "Iteration 7653, loss = 0.00250280\n",
            "Iteration 7654, loss = 0.00250145\n",
            "Iteration 7655, loss = 0.00250008\n",
            "Iteration 7656, loss = 0.00249873\n",
            "Iteration 7657, loss = 0.00249736\n",
            "Iteration 7658, loss = 0.00249606\n",
            "Iteration 7659, loss = 0.00249472\n",
            "Iteration 7660, loss = 0.00249337\n",
            "Iteration 7661, loss = 0.00249205\n",
            "Iteration 7662, loss = 0.00249069\n",
            "Iteration 7663, loss = 0.00248935\n",
            "Iteration 7664, loss = 0.00248800\n",
            "Iteration 7665, loss = 0.00248672\n",
            "Iteration 7666, loss = 0.00248532\n",
            "Iteration 7667, loss = 0.00248400\n",
            "Iteration 7668, loss = 0.00248267\n",
            "Iteration 7669, loss = 0.00248134\n",
            "Iteration 7670, loss = 0.00248000\n",
            "Iteration 7671, loss = 0.00247866\n",
            "Iteration 7672, loss = 0.00247732\n",
            "Iteration 7673, loss = 0.00247602\n",
            "Iteration 7674, loss = 0.00247468\n",
            "Iteration 7675, loss = 0.00247335\n",
            "Iteration 7676, loss = 0.00247204\n",
            "Iteration 7677, loss = 0.00247071\n",
            "Iteration 7678, loss = 0.00246938\n",
            "Iteration 7679, loss = 0.00246805\n",
            "Iteration 7680, loss = 0.00246671\n",
            "Iteration 7681, loss = 0.00246537\n",
            "Iteration 7682, loss = 0.00246416\n",
            "Iteration 7683, loss = 0.00246273\n",
            "Iteration 7684, loss = 0.00246143\n",
            "Iteration 7685, loss = 0.00246010\n",
            "Iteration 7686, loss = 0.00245879\n",
            "Iteration 7687, loss = 0.00245746\n",
            "Iteration 7688, loss = 0.00245613\n",
            "Iteration 7689, loss = 0.00245482\n",
            "Iteration 7690, loss = 0.00245352\n",
            "Iteration 7691, loss = 0.00245221\n",
            "Iteration 7692, loss = 0.00245091\n",
            "Iteration 7693, loss = 0.00244960\n",
            "Iteration 7694, loss = 0.00244826\n",
            "Iteration 7695, loss = 0.00244693\n",
            "Iteration 7696, loss = 0.00244573\n",
            "Iteration 7697, loss = 0.00244433\n",
            "Iteration 7698, loss = 0.00244303\n",
            "Iteration 7699, loss = 0.00244176\n",
            "Iteration 7700, loss = 0.00244047\n",
            "Iteration 7701, loss = 0.00243919\n",
            "Iteration 7702, loss = 0.00243789\n",
            "Iteration 7703, loss = 0.00243660\n",
            "Iteration 7704, loss = 0.00243529\n",
            "Iteration 7705, loss = 0.00243398\n",
            "Iteration 7706, loss = 0.00243267\n",
            "Iteration 7707, loss = 0.00243136\n",
            "Iteration 7708, loss = 0.00243004\n",
            "Iteration 7709, loss = 0.00242872\n",
            "Iteration 7710, loss = 0.00242740\n",
            "Iteration 7711, loss = 0.00242608\n",
            "Iteration 7712, loss = 0.00242475\n",
            "Iteration 7713, loss = 0.00242346\n",
            "Iteration 7714, loss = 0.00242215\n",
            "Iteration 7715, loss = 0.00242085\n",
            "Iteration 7716, loss = 0.00241955\n",
            "Iteration 7717, loss = 0.00241825\n",
            "Iteration 7718, loss = 0.00241701\n",
            "Iteration 7719, loss = 0.00241569\n",
            "Iteration 7720, loss = 0.00241440\n",
            "Iteration 7721, loss = 0.00241312\n",
            "Iteration 7722, loss = 0.00241184\n",
            "Iteration 7723, loss = 0.00241054\n",
            "Iteration 7724, loss = 0.00240926\n",
            "Iteration 7725, loss = 0.00240795\n",
            "Iteration 7726, loss = 0.00240665\n",
            "Iteration 7727, loss = 0.00240536\n",
            "Iteration 7728, loss = 0.00240405\n",
            "Iteration 7729, loss = 0.00240279\n",
            "Iteration 7730, loss = 0.00240151\n",
            "Iteration 7731, loss = 0.00240022\n",
            "Iteration 7732, loss = 0.00239895\n",
            "Iteration 7733, loss = 0.00239765\n",
            "Iteration 7734, loss = 0.00239637\n",
            "Iteration 7735, loss = 0.00239507\n",
            "Iteration 7736, loss = 0.00239380\n",
            "Iteration 7737, loss = 0.00239251\n",
            "Iteration 7738, loss = 0.00239124\n",
            "Iteration 7739, loss = 0.00238997\n",
            "Iteration 7740, loss = 0.00238869\n",
            "Iteration 7741, loss = 0.00238741\n",
            "Iteration 7742, loss = 0.00238613\n",
            "Iteration 7743, loss = 0.00238484\n",
            "Iteration 7744, loss = 0.00238356\n",
            "Iteration 7745, loss = 0.00238233\n",
            "Iteration 7746, loss = 0.00238101\n",
            "Iteration 7747, loss = 0.00237975\n",
            "Iteration 7748, loss = 0.00237848\n",
            "Iteration 7749, loss = 0.00237721\n",
            "Iteration 7750, loss = 0.00237594\n",
            "Iteration 7751, loss = 0.00237466\n",
            "Iteration 7752, loss = 0.00237338\n",
            "Iteration 7753, loss = 0.00237215\n",
            "Iteration 7754, loss = 0.00237085\n",
            "Iteration 7755, loss = 0.00236960\n",
            "Iteration 7756, loss = 0.00236834\n",
            "Iteration 7757, loss = 0.00236708\n",
            "Iteration 7758, loss = 0.00236581\n",
            "Iteration 7759, loss = 0.00236454\n",
            "Iteration 7760, loss = 0.00236327\n",
            "Iteration 7761, loss = 0.00236199\n",
            "Iteration 7762, loss = 0.00236079\n",
            "Iteration 7763, loss = 0.00235947\n",
            "Iteration 7764, loss = 0.00235823\n",
            "Iteration 7765, loss = 0.00235697\n",
            "Iteration 7766, loss = 0.00235572\n",
            "Iteration 7767, loss = 0.00235445\n",
            "Iteration 7768, loss = 0.00235319\n",
            "Iteration 7769, loss = 0.00235192\n",
            "Iteration 7770, loss = 0.00235070\n",
            "Iteration 7771, loss = 0.00234942\n",
            "Iteration 7772, loss = 0.00234818\n",
            "Iteration 7773, loss = 0.00234693\n",
            "Iteration 7774, loss = 0.00234568\n",
            "Iteration 7775, loss = 0.00234443\n",
            "Iteration 7776, loss = 0.00234317\n",
            "Iteration 7777, loss = 0.00234191\n",
            "Iteration 7778, loss = 0.00234065\n",
            "Iteration 7779, loss = 0.00233945\n",
            "Iteration 7780, loss = 0.00233816\n",
            "Iteration 7781, loss = 0.00233692\n",
            "Iteration 7782, loss = 0.00233568\n",
            "Iteration 7783, loss = 0.00233444\n",
            "Iteration 7784, loss = 0.00233318\n",
            "Iteration 7785, loss = 0.00233193\n",
            "Iteration 7786, loss = 0.00233068\n",
            "Iteration 7787, loss = 0.00232946\n",
            "Iteration 7788, loss = 0.00232820\n",
            "Iteration 7789, loss = 0.00232697\n",
            "Iteration 7790, loss = 0.00232574\n",
            "Iteration 7791, loss = 0.00232450\n",
            "Iteration 7792, loss = 0.00232326\n",
            "Iteration 7793, loss = 0.00232201\n",
            "Iteration 7794, loss = 0.00232077\n",
            "Iteration 7795, loss = 0.00231952\n",
            "Iteration 7796, loss = 0.00231832\n",
            "Iteration 7797, loss = 0.00231705\n",
            "Iteration 7798, loss = 0.00231582\n",
            "Iteration 7799, loss = 0.00231459\n",
            "Iteration 7800, loss = 0.00231337\n",
            "Iteration 7801, loss = 0.00231213\n",
            "Iteration 7802, loss = 0.00231089\n",
            "Iteration 7803, loss = 0.00230965\n",
            "Iteration 7804, loss = 0.00230843\n",
            "Iteration 7805, loss = 0.00230719\n",
            "Iteration 7806, loss = 0.00230597\n",
            "Iteration 7807, loss = 0.00230475\n",
            "Iteration 7808, loss = 0.00230353\n",
            "Iteration 7809, loss = 0.00230230\n",
            "Iteration 7810, loss = 0.00230107\n",
            "Iteration 7811, loss = 0.00229983\n",
            "Iteration 7812, loss = 0.00229860\n",
            "Iteration 7813, loss = 0.00229741\n",
            "Iteration 7814, loss = 0.00229615\n",
            "Iteration 7815, loss = 0.00229494\n",
            "Iteration 7816, loss = 0.00229372\n",
            "Iteration 7817, loss = 0.00229251\n",
            "Iteration 7818, loss = 0.00229128\n",
            "Iteration 7819, loss = 0.00229005\n",
            "Iteration 7820, loss = 0.00228883\n",
            "Iteration 7821, loss = 0.00228762\n",
            "Iteration 7822, loss = 0.00228639\n",
            "Iteration 7823, loss = 0.00228519\n",
            "Iteration 7824, loss = 0.00228398\n",
            "Iteration 7825, loss = 0.00228277\n",
            "Iteration 7826, loss = 0.00228155\n",
            "Iteration 7827, loss = 0.00228033\n",
            "Iteration 7828, loss = 0.00227911\n",
            "Iteration 7829, loss = 0.00227788\n",
            "Iteration 7830, loss = 0.00227671\n",
            "Iteration 7831, loss = 0.00227547\n",
            "Iteration 7832, loss = 0.00227427\n",
            "Iteration 7833, loss = 0.00227306\n",
            "Iteration 7834, loss = 0.00227186\n",
            "Iteration 7835, loss = 0.00227064\n",
            "Iteration 7836, loss = 0.00226943\n",
            "Iteration 7837, loss = 0.00226821\n",
            "Iteration 7838, loss = 0.00226701\n",
            "Iteration 7839, loss = 0.00226580\n",
            "Iteration 7840, loss = 0.00226461\n",
            "Iteration 7841, loss = 0.00226341\n",
            "Iteration 7842, loss = 0.00226221\n",
            "Iteration 7843, loss = 0.00226101\n",
            "Iteration 7844, loss = 0.00225980\n",
            "Iteration 7845, loss = 0.00225859\n",
            "Iteration 7846, loss = 0.00225738\n",
            "Iteration 7847, loss = 0.00225621\n",
            "Iteration 7848, loss = 0.00225498\n",
            "Iteration 7849, loss = 0.00225380\n",
            "Iteration 7850, loss = 0.00225260\n",
            "Iteration 7851, loss = 0.00225141\n",
            "Iteration 7852, loss = 0.00225021\n",
            "Iteration 7853, loss = 0.00224901\n",
            "Iteration 7854, loss = 0.00224780\n",
            "Iteration 7855, loss = 0.00224662\n",
            "Iteration 7856, loss = 0.00224542\n",
            "Iteration 7857, loss = 0.00224424\n",
            "Iteration 7858, loss = 0.00224305\n",
            "Iteration 7859, loss = 0.00224186\n",
            "Iteration 7860, loss = 0.00224067\n",
            "Iteration 7861, loss = 0.00223948\n",
            "Iteration 7862, loss = 0.00223828\n",
            "Iteration 7863, loss = 0.00223708\n",
            "Iteration 7864, loss = 0.00223592\n",
            "Iteration 7865, loss = 0.00223471\n",
            "Iteration 7866, loss = 0.00223353\n",
            "Iteration 7867, loss = 0.00223235\n",
            "Iteration 7868, loss = 0.00223117\n",
            "Iteration 7869, loss = 0.00222998\n",
            "Iteration 7870, loss = 0.00222879\n",
            "Iteration 7871, loss = 0.00222760\n",
            "Iteration 7872, loss = 0.00222642\n",
            "Iteration 7873, loss = 0.00222524\n",
            "Iteration 7874, loss = 0.00222407\n",
            "Iteration 7875, loss = 0.00222289\n",
            "Iteration 7876, loss = 0.00222172\n",
            "Iteration 7877, loss = 0.00222054\n",
            "Iteration 7878, loss = 0.00221935\n",
            "Iteration 7879, loss = 0.00221817\n",
            "Iteration 7880, loss = 0.00221698\n",
            "Iteration 7881, loss = 0.00221583\n",
            "Iteration 7882, loss = 0.00221463\n",
            "Iteration 7883, loss = 0.00221347\n",
            "Iteration 7884, loss = 0.00221230\n",
            "Iteration 7885, loss = 0.00221113\n",
            "Iteration 7886, loss = 0.00220995\n",
            "Iteration 7887, loss = 0.00220877\n",
            "Iteration 7888, loss = 0.00220759\n",
            "Iteration 7889, loss = 0.00220642\n",
            "Iteration 7890, loss = 0.00220526\n",
            "Iteration 7891, loss = 0.00220410\n",
            "Iteration 7892, loss = 0.00220294\n",
            "Iteration 7893, loss = 0.00220177\n",
            "Iteration 7894, loss = 0.00220060\n",
            "Iteration 7895, loss = 0.00219943\n",
            "Iteration 7896, loss = 0.00219826\n",
            "Iteration 7897, loss = 0.00219708\n",
            "Iteration 7898, loss = 0.00219594\n",
            "Iteration 7899, loss = 0.00219476\n",
            "Iteration 7900, loss = 0.00219360\n",
            "Iteration 7901, loss = 0.00219244\n",
            "Iteration 7902, loss = 0.00219129\n",
            "Iteration 7903, loss = 0.00219012\n",
            "Iteration 7904, loss = 0.00218896\n",
            "Iteration 7905, loss = 0.00218779\n",
            "Iteration 7906, loss = 0.00218663\n",
            "Iteration 7907, loss = 0.00218547\n",
            "Iteration 7908, loss = 0.00218433\n",
            "Iteration 7909, loss = 0.00218318\n",
            "Iteration 7910, loss = 0.00218202\n",
            "Iteration 7911, loss = 0.00218087\n",
            "Iteration 7912, loss = 0.00217971\n",
            "Iteration 7913, loss = 0.00217854\n",
            "Iteration 7914, loss = 0.00217738\n",
            "Iteration 7915, loss = 0.00217624\n",
            "Iteration 7916, loss = 0.00217508\n",
            "Iteration 7917, loss = 0.00217394\n",
            "Iteration 7918, loss = 0.00217279\n",
            "Iteration 7919, loss = 0.00217164\n",
            "Iteration 7920, loss = 0.00217049\n",
            "Iteration 7921, loss = 0.00216934\n",
            "Iteration 7922, loss = 0.00216818\n",
            "Iteration 7923, loss = 0.00216702\n",
            "Iteration 7924, loss = 0.00216589\n",
            "Iteration 7925, loss = 0.00216475\n",
            "Iteration 7926, loss = 0.00216362\n",
            "Iteration 7927, loss = 0.00216247\n",
            "Iteration 7928, loss = 0.00216133\n",
            "Iteration 7929, loss = 0.00216018\n",
            "Iteration 7930, loss = 0.00215903\n",
            "Iteration 7931, loss = 0.00215787\n",
            "Iteration 7932, loss = 0.00215674\n",
            "Iteration 7933, loss = 0.00215560\n",
            "Iteration 7934, loss = 0.00215447\n",
            "Iteration 7935, loss = 0.00215333\n",
            "Iteration 7936, loss = 0.00215220\n",
            "Iteration 7937, loss = 0.00215105\n",
            "Iteration 7938, loss = 0.00214991\n",
            "Iteration 7939, loss = 0.00214876\n",
            "Iteration 7940, loss = 0.00214762\n",
            "Iteration 7941, loss = 0.00214650\n",
            "Iteration 7942, loss = 0.00214537\n",
            "Iteration 7943, loss = 0.00214425\n",
            "Iteration 7944, loss = 0.00214312\n",
            "Iteration 7945, loss = 0.00214198\n",
            "Iteration 7946, loss = 0.00214084\n",
            "Iteration 7947, loss = 0.00213970\n",
            "Iteration 7948, loss = 0.00213856\n",
            "Iteration 7949, loss = 0.00213744\n",
            "Iteration 7950, loss = 0.00213631\n",
            "Iteration 7951, loss = 0.00213519\n",
            "Iteration 7952, loss = 0.00213406\n",
            "Iteration 7953, loss = 0.00213294\n",
            "Iteration 7954, loss = 0.00213181\n",
            "Iteration 7955, loss = 0.00213068\n",
            "Iteration 7956, loss = 0.00212954\n",
            "Iteration 7957, loss = 0.00212840\n",
            "Iteration 7958, loss = 0.00212739\n",
            "Iteration 7959, loss = 0.00212616\n",
            "Iteration 7960, loss = 0.00212505\n",
            "Iteration 7961, loss = 0.00212393\n",
            "Iteration 7962, loss = 0.00212281\n",
            "Iteration 7963, loss = 0.00212168\n",
            "Iteration 7964, loss = 0.00212056\n",
            "Iteration 7965, loss = 0.00211943\n",
            "Iteration 7966, loss = 0.00211833\n",
            "Iteration 7967, loss = 0.00211722\n",
            "Iteration 7968, loss = 0.00211612\n",
            "Iteration 7969, loss = 0.00211501\n",
            "Iteration 7970, loss = 0.00211389\n",
            "Iteration 7971, loss = 0.00211277\n",
            "Iteration 7972, loss = 0.00211164\n",
            "Iteration 7973, loss = 0.00211052\n",
            "Iteration 7974, loss = 0.00210940\n",
            "Iteration 7975, loss = 0.00210830\n",
            "Iteration 7976, loss = 0.00210718\n",
            "Iteration 7977, loss = 0.00210607\n",
            "Iteration 7978, loss = 0.00210496\n",
            "Iteration 7979, loss = 0.00210386\n",
            "Iteration 7980, loss = 0.00210273\n",
            "Iteration 7981, loss = 0.00210163\n",
            "Iteration 7982, loss = 0.00210050\n",
            "Iteration 7983, loss = 0.00209946\n",
            "Iteration 7984, loss = 0.00209829\n",
            "Iteration 7985, loss = 0.00209720\n",
            "Iteration 7986, loss = 0.00209610\n",
            "Iteration 7987, loss = 0.00209499\n",
            "Iteration 7988, loss = 0.00209389\n",
            "Iteration 7989, loss = 0.00209278\n",
            "Iteration 7990, loss = 0.00209166\n",
            "Iteration 7991, loss = 0.00209055\n",
            "Iteration 7992, loss = 0.00208954\n",
            "Iteration 7993, loss = 0.00208835\n",
            "Iteration 7994, loss = 0.00208726\n",
            "Iteration 7995, loss = 0.00208616\n",
            "Iteration 7996, loss = 0.00208507\n",
            "Iteration 7997, loss = 0.00208396\n",
            "Iteration 7998, loss = 0.00208286\n",
            "Iteration 7999, loss = 0.00208175\n",
            "Iteration 8000, loss = 0.00208072\n",
            "Iteration 8001, loss = 0.00207957\n",
            "Iteration 8002, loss = 0.00207848\n",
            "Iteration 8003, loss = 0.00207740\n",
            "Iteration 8004, loss = 0.00207630\n",
            "Iteration 8005, loss = 0.00207521\n",
            "Iteration 8006, loss = 0.00207411\n",
            "Iteration 8007, loss = 0.00207301\n",
            "Iteration 8008, loss = 0.00207191\n",
            "Iteration 8009, loss = 0.00207088\n",
            "Iteration 8010, loss = 0.00206973\n",
            "Iteration 8011, loss = 0.00206865\n",
            "Iteration 8012, loss = 0.00206756\n",
            "Iteration 8013, loss = 0.00206648\n",
            "Iteration 8014, loss = 0.00206539\n",
            "Iteration 8015, loss = 0.00206429\n",
            "Iteration 8016, loss = 0.00206320\n",
            "Iteration 8017, loss = 0.00206214\n",
            "Iteration 8018, loss = 0.00206103\n",
            "Iteration 8019, loss = 0.00205996\n",
            "Iteration 8020, loss = 0.00205888\n",
            "Iteration 8021, loss = 0.00205780\n",
            "Iteration 8022, loss = 0.00205672\n",
            "Iteration 8023, loss = 0.00205563\n",
            "Iteration 8024, loss = 0.00205454\n",
            "Iteration 8025, loss = 0.00205345\n",
            "Iteration 8026, loss = 0.00205239\n",
            "Iteration 8027, loss = 0.00205129\n",
            "Iteration 8028, loss = 0.00205022\n",
            "Iteration 8029, loss = 0.00204915\n",
            "Iteration 8030, loss = 0.00204808\n",
            "Iteration 8031, loss = 0.00204699\n",
            "Iteration 8032, loss = 0.00204591\n",
            "Iteration 8033, loss = 0.00204483\n",
            "Iteration 8034, loss = 0.00204374\n",
            "Iteration 8035, loss = 0.00204268\n",
            "Iteration 8036, loss = 0.00204162\n",
            "Iteration 8037, loss = 0.00204055\n",
            "Iteration 8038, loss = 0.00203948\n",
            "Iteration 8039, loss = 0.00203841\n",
            "Iteration 8040, loss = 0.00203733\n",
            "Iteration 8041, loss = 0.00203625\n",
            "Iteration 8042, loss = 0.00203517\n",
            "Iteration 8043, loss = 0.00203410\n",
            "Iteration 8044, loss = 0.00203304\n",
            "Iteration 8045, loss = 0.00203198\n",
            "Iteration 8046, loss = 0.00203091\n",
            "Iteration 8047, loss = 0.00202985\n",
            "Iteration 8048, loss = 0.00202878\n",
            "Iteration 8049, loss = 0.00202771\n",
            "Iteration 8050, loss = 0.00202663\n",
            "Iteration 8051, loss = 0.00202555\n",
            "Iteration 8052, loss = 0.00202459\n",
            "Iteration 8053, loss = 0.00202343\n",
            "Iteration 8054, loss = 0.00202238\n",
            "Iteration 8055, loss = 0.00202132\n",
            "Iteration 8056, loss = 0.00202026\n",
            "Iteration 8057, loss = 0.00201919\n",
            "Iteration 8058, loss = 0.00201813\n",
            "Iteration 8059, loss = 0.00201706\n",
            "Iteration 8060, loss = 0.00201609\n",
            "Iteration 8061, loss = 0.00201494\n",
            "Iteration 8062, loss = 0.00201390\n",
            "Iteration 8063, loss = 0.00201285\n",
            "Iteration 8064, loss = 0.00201179\n",
            "Iteration 8065, loss = 0.00201074\n",
            "Iteration 8066, loss = 0.00200968\n",
            "Iteration 8067, loss = 0.00200861\n",
            "Iteration 8068, loss = 0.00200755\n",
            "Iteration 8069, loss = 0.00200659\n",
            "Iteration 8070, loss = 0.00200545\n",
            "Iteration 8071, loss = 0.00200440\n",
            "Iteration 8072, loss = 0.00200335\n",
            "Iteration 8073, loss = 0.00200231\n",
            "Iteration 8074, loss = 0.00200125\n",
            "Iteration 8075, loss = 0.00200020\n",
            "Iteration 8076, loss = 0.00199914\n",
            "Iteration 8077, loss = 0.00199815\n",
            "Iteration 8078, loss = 0.00199705\n",
            "Iteration 8079, loss = 0.00199601\n",
            "Iteration 8080, loss = 0.00199497\n",
            "Iteration 8081, loss = 0.00199393\n",
            "Iteration 8082, loss = 0.00199288\n",
            "Iteration 8083, loss = 0.00199183\n",
            "Iteration 8084, loss = 0.00199078\n",
            "Iteration 8085, loss = 0.00198972\n",
            "Iteration 8086, loss = 0.00198874\n",
            "Iteration 8087, loss = 0.00198764\n",
            "Iteration 8088, loss = 0.00198661\n",
            "Iteration 8089, loss = 0.00198557\n",
            "Iteration 8090, loss = 0.00198453\n",
            "Iteration 8091, loss = 0.00198349\n",
            "Iteration 8092, loss = 0.00198244\n",
            "Iteration 8093, loss = 0.00198140\n",
            "Iteration 8094, loss = 0.00198038\n",
            "Iteration 8095, loss = 0.00197932\n",
            "Iteration 8096, loss = 0.00197830\n",
            "Iteration 8097, loss = 0.00197727\n",
            "Iteration 8098, loss = 0.00197624\n",
            "Iteration 8099, loss = 0.00197520\n",
            "Iteration 8100, loss = 0.00197416\n",
            "Iteration 8101, loss = 0.00197312\n",
            "Iteration 8102, loss = 0.00197207\n",
            "Iteration 8103, loss = 0.00197106\n",
            "Iteration 8104, loss = 0.00197001\n",
            "Iteration 8105, loss = 0.00196899\n",
            "Iteration 8106, loss = 0.00196796\n",
            "Iteration 8107, loss = 0.00196693\n",
            "Iteration 8108, loss = 0.00196590\n",
            "Iteration 8109, loss = 0.00196486\n",
            "Iteration 8110, loss = 0.00196383\n",
            "Iteration 8111, loss = 0.00196279\n",
            "Iteration 8112, loss = 0.00196187\n",
            "Iteration 8113, loss = 0.00196073\n",
            "Iteration 8114, loss = 0.00195972\n",
            "Iteration 8115, loss = 0.00195869\n",
            "Iteration 8116, loss = 0.00195767\n",
            "Iteration 8117, loss = 0.00195664\n",
            "Iteration 8118, loss = 0.00195561\n",
            "Iteration 8119, loss = 0.00195458\n",
            "Iteration 8120, loss = 0.00195364\n",
            "Iteration 8121, loss = 0.00195254\n",
            "Iteration 8122, loss = 0.00195153\n",
            "Iteration 8123, loss = 0.00195052\n",
            "Iteration 8124, loss = 0.00194950\n",
            "Iteration 8125, loss = 0.00194848\n",
            "Iteration 8126, loss = 0.00194745\n",
            "Iteration 8127, loss = 0.00194643\n",
            "Iteration 8128, loss = 0.00194540\n",
            "Iteration 8129, loss = 0.00194447\n",
            "Iteration 8130, loss = 0.00194337\n",
            "Iteration 8131, loss = 0.00194236\n",
            "Iteration 8132, loss = 0.00194135\n",
            "Iteration 8133, loss = 0.00194034\n",
            "Iteration 8134, loss = 0.00193932\n",
            "Iteration 8135, loss = 0.00193830\n",
            "Iteration 8136, loss = 0.00193728\n",
            "Iteration 8137, loss = 0.00193631\n",
            "Iteration 8138, loss = 0.00193526\n",
            "Iteration 8139, loss = 0.00193427\n",
            "Iteration 8140, loss = 0.00193324\n",
            "Iteration 8141, loss = 0.00193222\n",
            "Iteration 8142, loss = 0.00193125\n",
            "Iteration 8143, loss = 0.00193021\n",
            "Iteration 8144, loss = 0.00192921\n",
            "Iteration 8145, loss = 0.00192821\n",
            "Iteration 8146, loss = 0.00192720\n",
            "Iteration 8147, loss = 0.00192619\n",
            "Iteration 8148, loss = 0.00192518\n",
            "Iteration 8149, loss = 0.00192417\n",
            "Iteration 8150, loss = 0.00192315\n",
            "Iteration 8151, loss = 0.00192221\n",
            "Iteration 8152, loss = 0.00192115\n",
            "Iteration 8153, loss = 0.00192016\n",
            "Iteration 8154, loss = 0.00191916\n",
            "Iteration 8155, loss = 0.00191816\n",
            "Iteration 8156, loss = 0.00191715\n",
            "Iteration 8157, loss = 0.00191615\n",
            "Iteration 8158, loss = 0.00191514\n",
            "Iteration 8159, loss = 0.00191413\n",
            "Iteration 8160, loss = 0.00191314\n",
            "Iteration 8161, loss = 0.00191216\n",
            "Iteration 8162, loss = 0.00191116\n",
            "Iteration 8163, loss = 0.00191017\n",
            "Iteration 8164, loss = 0.00190917\n",
            "Iteration 8165, loss = 0.00190817\n",
            "Iteration 8166, loss = 0.00190717\n",
            "Iteration 8167, loss = 0.00190616\n",
            "Iteration 8168, loss = 0.00190516\n",
            "Iteration 8169, loss = 0.00190423\n",
            "Iteration 8170, loss = 0.00190317\n",
            "Iteration 8171, loss = 0.00190219\n",
            "Iteration 8172, loss = 0.00190120\n",
            "Iteration 8173, loss = 0.00190021\n",
            "Iteration 8174, loss = 0.00189921\n",
            "Iteration 8175, loss = 0.00189822\n",
            "Iteration 8176, loss = 0.00189722\n",
            "Iteration 8177, loss = 0.00189630\n",
            "Iteration 8178, loss = 0.00189524\n",
            "Iteration 8179, loss = 0.00189426\n",
            "Iteration 8180, loss = 0.00189328\n",
            "Iteration 8181, loss = 0.00189230\n",
            "Iteration 8182, loss = 0.00189131\n",
            "Iteration 8183, loss = 0.00189032\n",
            "Iteration 8184, loss = 0.00188933\n",
            "Iteration 8185, loss = 0.00188833\n",
            "Iteration 8186, loss = 0.00188739\n",
            "Iteration 8187, loss = 0.00188637\n",
            "Iteration 8188, loss = 0.00188539\n",
            "Iteration 8189, loss = 0.00188441\n",
            "Iteration 8190, loss = 0.00188344\n",
            "Iteration 8191, loss = 0.00188245\n",
            "Iteration 8192, loss = 0.00188146\n",
            "Iteration 8193, loss = 0.00188048\n",
            "Iteration 8194, loss = 0.00187949\n",
            "Iteration 8195, loss = 0.00187858\n",
            "Iteration 8196, loss = 0.00187753\n",
            "Iteration 8197, loss = 0.00187656\n",
            "Iteration 8198, loss = 0.00187559\n",
            "Iteration 8199, loss = 0.00187462\n",
            "Iteration 8200, loss = 0.00187364\n",
            "Iteration 8201, loss = 0.00187265\n",
            "Iteration 8202, loss = 0.00187167\n",
            "Iteration 8203, loss = 0.00187069\n",
            "Iteration 8204, loss = 0.00186973\n",
            "Iteration 8205, loss = 0.00186876\n",
            "Iteration 8206, loss = 0.00186780\n",
            "Iteration 8207, loss = 0.00186683\n",
            "Iteration 8208, loss = 0.00186585\n",
            "Iteration 8209, loss = 0.00186488\n",
            "Iteration 8210, loss = 0.00186390\n",
            "Iteration 8211, loss = 0.00186292\n",
            "Iteration 8212, loss = 0.00186194\n",
            "Iteration 8213, loss = 0.00186103\n",
            "Iteration 8214, loss = 0.00186001\n",
            "Iteration 8215, loss = 0.00185904\n",
            "Iteration 8216, loss = 0.00185808\n",
            "Iteration 8217, loss = 0.00185712\n",
            "Iteration 8218, loss = 0.00185614\n",
            "Iteration 8219, loss = 0.00185517\n",
            "Iteration 8220, loss = 0.00185420\n",
            "Iteration 8221, loss = 0.00185330\n",
            "Iteration 8222, loss = 0.00185227\n",
            "Iteration 8223, loss = 0.00185132\n",
            "Iteration 8224, loss = 0.00185036\n",
            "Iteration 8225, loss = 0.00184940\n",
            "Iteration 8226, loss = 0.00184844\n",
            "Iteration 8227, loss = 0.00184747\n",
            "Iteration 8228, loss = 0.00184650\n",
            "Iteration 8229, loss = 0.00184553\n",
            "Iteration 8230, loss = 0.00184461\n",
            "Iteration 8231, loss = 0.00184362\n",
            "Iteration 8232, loss = 0.00184267\n",
            "Iteration 8233, loss = 0.00184172\n",
            "Iteration 8234, loss = 0.00184076\n",
            "Iteration 8235, loss = 0.00183980\n",
            "Iteration 8236, loss = 0.00183884\n",
            "Iteration 8237, loss = 0.00183788\n",
            "Iteration 8238, loss = 0.00183691\n",
            "Iteration 8239, loss = 0.00183602\n",
            "Iteration 8240, loss = 0.00183501\n",
            "Iteration 8241, loss = 0.00183406\n",
            "Iteration 8242, loss = 0.00183311\n",
            "Iteration 8243, loss = 0.00183216\n",
            "Iteration 8244, loss = 0.00183121\n",
            "Iteration 8245, loss = 0.00183025\n",
            "Iteration 8246, loss = 0.00182929\n",
            "Iteration 8247, loss = 0.00182833\n",
            "Iteration 8248, loss = 0.00182740\n",
            "Iteration 8249, loss = 0.00182646\n",
            "Iteration 8250, loss = 0.00182551\n",
            "Iteration 8251, loss = 0.00182457\n",
            "Iteration 8252, loss = 0.00182362\n",
            "Iteration 8253, loss = 0.00182267\n",
            "Iteration 8254, loss = 0.00182172\n",
            "Iteration 8255, loss = 0.00182076\n",
            "Iteration 8256, loss = 0.00181980\n",
            "Iteration 8257, loss = 0.00181892\n",
            "Iteration 8258, loss = 0.00181792\n",
            "Iteration 8259, loss = 0.00181698\n",
            "Iteration 8260, loss = 0.00181604\n",
            "Iteration 8261, loss = 0.00181510\n",
            "Iteration 8262, loss = 0.00181415\n",
            "Iteration 8263, loss = 0.00181320\n",
            "Iteration 8264, loss = 0.00181225\n",
            "Iteration 8265, loss = 0.00181138\n",
            "Iteration 8266, loss = 0.00181038\n",
            "Iteration 8267, loss = 0.00180945\n",
            "Iteration 8268, loss = 0.00180851\n",
            "Iteration 8269, loss = 0.00180758\n",
            "Iteration 8270, loss = 0.00180664\n",
            "Iteration 8271, loss = 0.00180570\n",
            "Iteration 8272, loss = 0.00180475\n",
            "Iteration 8273, loss = 0.00180381\n",
            "Iteration 8274, loss = 0.00180291\n",
            "Iteration 8275, loss = 0.00180194\n",
            "Iteration 8276, loss = 0.00180101\n",
            "Iteration 8277, loss = 0.00180008\n",
            "Iteration 8278, loss = 0.00179915\n",
            "Iteration 8279, loss = 0.00179822\n",
            "Iteration 8280, loss = 0.00179728\n",
            "Iteration 8281, loss = 0.00179634\n",
            "Iteration 8282, loss = 0.00179540\n",
            "Iteration 8283, loss = 0.00179453\n",
            "Iteration 8284, loss = 0.00179354\n",
            "Iteration 8285, loss = 0.00179262\n",
            "Iteration 8286, loss = 0.00179169\n",
            "Iteration 8287, loss = 0.00179077\n",
            "Iteration 8288, loss = 0.00178984\n",
            "Iteration 8289, loss = 0.00178890\n",
            "Iteration 8290, loss = 0.00178797\n",
            "Iteration 8291, loss = 0.00178703\n",
            "Iteration 8292, loss = 0.00178612\n",
            "Iteration 8293, loss = 0.00178521\n",
            "Iteration 8294, loss = 0.00178429\n",
            "Iteration 8295, loss = 0.00178336\n",
            "Iteration 8296, loss = 0.00178244\n",
            "Iteration 8297, loss = 0.00178151\n",
            "Iteration 8298, loss = 0.00178058\n",
            "Iteration 8299, loss = 0.00177965\n",
            "Iteration 8300, loss = 0.00177872\n",
            "Iteration 8301, loss = 0.00177785\n",
            "Iteration 8302, loss = 0.00177688\n",
            "Iteration 8303, loss = 0.00177596\n",
            "Iteration 8304, loss = 0.00177505\n",
            "Iteration 8305, loss = 0.00177413\n",
            "Iteration 8306, loss = 0.00177320\n",
            "Iteration 8307, loss = 0.00177228\n",
            "Iteration 8308, loss = 0.00177136\n",
            "Iteration 8309, loss = 0.00177050\n",
            "Iteration 8310, loss = 0.00176952\n",
            "Iteration 8311, loss = 0.00176862\n",
            "Iteration 8312, loss = 0.00176771\n",
            "Iteration 8313, loss = 0.00176680\n",
            "Iteration 8314, loss = 0.00176588\n",
            "Iteration 8315, loss = 0.00176496\n",
            "Iteration 8316, loss = 0.00176404\n",
            "Iteration 8317, loss = 0.00176312\n",
            "Iteration 8318, loss = 0.00176224\n",
            "Iteration 8319, loss = 0.00176130\n",
            "Iteration 8320, loss = 0.00176040\n",
            "Iteration 8321, loss = 0.00175949\n",
            "Iteration 8322, loss = 0.00175858\n",
            "Iteration 8323, loss = 0.00175767\n",
            "Iteration 8324, loss = 0.00175676\n",
            "Iteration 8325, loss = 0.00175584\n",
            "Iteration 8326, loss = 0.00175492\n",
            "Iteration 8327, loss = 0.00175407\n",
            "Iteration 8328, loss = 0.00175311\n",
            "Iteration 8329, loss = 0.00175221\n",
            "Iteration 8330, loss = 0.00175131\n",
            "Iteration 8331, loss = 0.00175041\n",
            "Iteration 8332, loss = 0.00174950\n",
            "Iteration 8333, loss = 0.00174859\n",
            "Iteration 8334, loss = 0.00174768\n",
            "Iteration 8335, loss = 0.00174676\n",
            "Iteration 8336, loss = 0.00174595\n",
            "Iteration 8337, loss = 0.00174496\n",
            "Iteration 8338, loss = 0.00174407\n",
            "Iteration 8339, loss = 0.00174317\n",
            "Iteration 8340, loss = 0.00174227\n",
            "Iteration 8341, loss = 0.00174137\n",
            "Iteration 8342, loss = 0.00174046\n",
            "Iteration 8343, loss = 0.00173955\n",
            "Iteration 8344, loss = 0.00173868\n",
            "Iteration 8345, loss = 0.00173776\n",
            "Iteration 8346, loss = 0.00173687\n",
            "Iteration 8347, loss = 0.00173598\n",
            "Iteration 8348, loss = 0.00173509\n",
            "Iteration 8349, loss = 0.00173419\n",
            "Iteration 8350, loss = 0.00173329\n",
            "Iteration 8351, loss = 0.00173239\n",
            "Iteration 8352, loss = 0.00173148\n",
            "Iteration 8353, loss = 0.00173058\n",
            "Iteration 8354, loss = 0.00172977\n",
            "Iteration 8355, loss = 0.00172880\n",
            "Iteration 8356, loss = 0.00172791\n",
            "Iteration 8357, loss = 0.00172702\n",
            "Iteration 8358, loss = 0.00172613\n",
            "Iteration 8359, loss = 0.00172523\n",
            "Iteration 8360, loss = 0.00172434\n",
            "Iteration 8361, loss = 0.00172344\n",
            "Iteration 8362, loss = 0.00172263\n",
            "Iteration 8363, loss = 0.00172166\n",
            "Iteration 8364, loss = 0.00172078\n",
            "Iteration 8365, loss = 0.00171990\n",
            "Iteration 8366, loss = 0.00171901\n",
            "Iteration 8367, loss = 0.00171813\n",
            "Iteration 8368, loss = 0.00171723\n",
            "Iteration 8369, loss = 0.00171634\n",
            "Iteration 8370, loss = 0.00171545\n",
            "Iteration 8371, loss = 0.00171461\n",
            "Iteration 8372, loss = 0.00171368\n",
            "Iteration 8373, loss = 0.00171281\n",
            "Iteration 8374, loss = 0.00171193\n",
            "Iteration 8375, loss = 0.00171105\n",
            "Iteration 8376, loss = 0.00171016\n",
            "Iteration 8377, loss = 0.00170927\n",
            "Iteration 8378, loss = 0.00170838\n",
            "Iteration 8379, loss = 0.00170749\n",
            "Iteration 8380, loss = 0.00170668\n",
            "Iteration 8381, loss = 0.00170573\n",
            "Iteration 8382, loss = 0.00170487\n",
            "Iteration 8383, loss = 0.00170399\n",
            "Iteration 8384, loss = 0.00170311\n",
            "Iteration 8385, loss = 0.00170223\n",
            "Iteration 8386, loss = 0.00170135\n",
            "Iteration 8387, loss = 0.00170047\n",
            "Iteration 8388, loss = 0.00169958\n",
            "Iteration 8389, loss = 0.00169872\n",
            "Iteration 8390, loss = 0.00169785\n",
            "Iteration 8391, loss = 0.00169698\n",
            "Iteration 8392, loss = 0.00169611\n",
            "Iteration 8393, loss = 0.00169523\n",
            "Iteration 8394, loss = 0.00169436\n",
            "Iteration 8395, loss = 0.00169348\n",
            "Iteration 8396, loss = 0.00169259\n",
            "Iteration 8397, loss = 0.00169171\n",
            "Iteration 8398, loss = 0.00169089\n",
            "Iteration 8399, loss = 0.00168997\n",
            "Iteration 8400, loss = 0.00168911\n",
            "Iteration 8401, loss = 0.00168824\n",
            "Iteration 8402, loss = 0.00168737\n",
            "Iteration 8403, loss = 0.00168650\n",
            "Iteration 8404, loss = 0.00168562\n",
            "Iteration 8405, loss = 0.00168475\n",
            "Iteration 8406, loss = 0.00168393\n",
            "Iteration 8407, loss = 0.00168301\n",
            "Iteration 8408, loss = 0.00168216\n",
            "Iteration 8409, loss = 0.00168130\n",
            "Iteration 8410, loss = 0.00168043\n",
            "Iteration 8411, loss = 0.00167957\n",
            "Iteration 8412, loss = 0.00167870\n",
            "Iteration 8413, loss = 0.00167783\n",
            "Iteration 8414, loss = 0.00167695\n",
            "Iteration 8415, loss = 0.00167610\n",
            "Iteration 8416, loss = 0.00167523\n",
            "Iteration 8417, loss = 0.00167438\n",
            "Iteration 8418, loss = 0.00167352\n",
            "Iteration 8419, loss = 0.00167266\n",
            "Iteration 8420, loss = 0.00167180\n",
            "Iteration 8421, loss = 0.00167093\n",
            "Iteration 8422, loss = 0.00167007\n",
            "Iteration 8423, loss = 0.00166920\n",
            "Iteration 8424, loss = 0.00166837\n",
            "Iteration 8425, loss = 0.00166748\n",
            "Iteration 8426, loss = 0.00166663\n",
            "Iteration 8427, loss = 0.00166578\n",
            "Iteration 8428, loss = 0.00166492\n",
            "Iteration 8429, loss = 0.00166406\n",
            "Iteration 8430, loss = 0.00166320\n",
            "Iteration 8431, loss = 0.00166234\n",
            "Iteration 8432, loss = 0.00166148\n",
            "Iteration 8433, loss = 0.00166069\n",
            "Iteration 8434, loss = 0.00165977\n",
            "Iteration 8435, loss = 0.00165893\n",
            "Iteration 8436, loss = 0.00165807\n",
            "Iteration 8437, loss = 0.00165722\n",
            "Iteration 8438, loss = 0.00165637\n",
            "Iteration 8439, loss = 0.00165551\n",
            "Iteration 8440, loss = 0.00165466\n",
            "Iteration 8441, loss = 0.00165381\n",
            "Iteration 8442, loss = 0.00165296\n",
            "Iteration 8443, loss = 0.00165212\n",
            "Iteration 8444, loss = 0.00165128\n",
            "Iteration 8445, loss = 0.00165043\n",
            "Iteration 8446, loss = 0.00164958\n",
            "Iteration 8447, loss = 0.00164873\n",
            "Iteration 8448, loss = 0.00164787\n",
            "Iteration 8449, loss = 0.00164702\n",
            "Iteration 8450, loss = 0.00164616\n",
            "Iteration 8451, loss = 0.00164537\n",
            "Iteration 8452, loss = 0.00164448\n",
            "Iteration 8453, loss = 0.00164364\n",
            "Iteration 8454, loss = 0.00164279\n",
            "Iteration 8455, loss = 0.00164195\n",
            "Iteration 8456, loss = 0.00164110\n",
            "Iteration 8457, loss = 0.00164026\n",
            "Iteration 8458, loss = 0.00163941\n",
            "Iteration 8459, loss = 0.00163861\n",
            "Iteration 8460, loss = 0.00163772\n",
            "Iteration 8461, loss = 0.00163689\n",
            "Iteration 8462, loss = 0.00163606\n",
            "Iteration 8463, loss = 0.00163522\n",
            "Iteration 8464, loss = 0.00163438\n",
            "Iteration 8465, loss = 0.00163354\n",
            "Iteration 8466, loss = 0.00163269\n",
            "Iteration 8467, loss = 0.00163184\n",
            "Iteration 8468, loss = 0.00163102\n",
            "Iteration 8469, loss = 0.00163017\n",
            "Iteration 8470, loss = 0.00162934\n",
            "Iteration 8471, loss = 0.00162851\n",
            "Iteration 8472, loss = 0.00162768\n",
            "Iteration 8473, loss = 0.00162684\n",
            "Iteration 8474, loss = 0.00162600\n",
            "Iteration 8475, loss = 0.00162516\n",
            "Iteration 8476, loss = 0.00162432\n",
            "Iteration 8477, loss = 0.00162351\n",
            "Iteration 8478, loss = 0.00162265\n",
            "Iteration 8479, loss = 0.00162183\n",
            "Iteration 8480, loss = 0.00162100\n",
            "Iteration 8481, loss = 0.00162017\n",
            "Iteration 8482, loss = 0.00161934\n",
            "Iteration 8483, loss = 0.00161850\n",
            "Iteration 8484, loss = 0.00161767\n",
            "Iteration 8485, loss = 0.00161682\n",
            "Iteration 8486, loss = 0.00161605\n",
            "Iteration 8487, loss = 0.00161517\n",
            "Iteration 8488, loss = 0.00161435\n",
            "Iteration 8489, loss = 0.00161352\n",
            "Iteration 8490, loss = 0.00161270\n",
            "Iteration 8491, loss = 0.00161187\n",
            "Iteration 8492, loss = 0.00161104\n",
            "Iteration 8493, loss = 0.00161021\n",
            "Iteration 8494, loss = 0.00160937\n",
            "Iteration 8495, loss = 0.00160856\n",
            "Iteration 8496, loss = 0.00160774\n",
            "Iteration 8497, loss = 0.00160693\n",
            "Iteration 8498, loss = 0.00160610\n",
            "Iteration 8499, loss = 0.00160528\n",
            "Iteration 8500, loss = 0.00160445\n",
            "Iteration 8501, loss = 0.00160363\n",
            "Iteration 8502, loss = 0.00160280\n",
            "Iteration 8503, loss = 0.00160196\n",
            "Iteration 8504, loss = 0.00160119\n",
            "Iteration 8505, loss = 0.00160033\n",
            "Iteration 8506, loss = 0.00159951\n",
            "Iteration 8507, loss = 0.00159869\n",
            "Iteration 8508, loss = 0.00159788\n",
            "Iteration 8509, loss = 0.00159705\n",
            "Iteration 8510, loss = 0.00159623\n",
            "Iteration 8511, loss = 0.00159541\n",
            "Iteration 8512, loss = 0.00159463\n",
            "Iteration 8513, loss = 0.00159377\n",
            "Iteration 8514, loss = 0.00159297\n",
            "Iteration 8515, loss = 0.00159216\n",
            "Iteration 8516, loss = 0.00159134\n",
            "Iteration 8517, loss = 0.00159053\n",
            "Iteration 8518, loss = 0.00158971\n",
            "Iteration 8519, loss = 0.00158889\n",
            "Iteration 8520, loss = 0.00158807\n",
            "Iteration 8521, loss = 0.00158726\n",
            "Iteration 8522, loss = 0.00158645\n",
            "Iteration 8523, loss = 0.00158564\n",
            "Iteration 8524, loss = 0.00158483\n",
            "Iteration 8525, loss = 0.00158403\n",
            "Iteration 8526, loss = 0.00158321\n",
            "Iteration 8527, loss = 0.00158240\n",
            "Iteration 8528, loss = 0.00158158\n",
            "Iteration 8529, loss = 0.00158076\n",
            "Iteration 8530, loss = 0.00157998\n",
            "Iteration 8531, loss = 0.00157915\n",
            "Iteration 8532, loss = 0.00157835\n",
            "Iteration 8533, loss = 0.00157754\n",
            "Iteration 8534, loss = 0.00157674\n",
            "Iteration 8535, loss = 0.00157593\n",
            "Iteration 8536, loss = 0.00157512\n",
            "Iteration 8537, loss = 0.00157431\n",
            "Iteration 8538, loss = 0.00157349\n",
            "Iteration 8539, loss = 0.00157274\n",
            "Iteration 8540, loss = 0.00157189\n",
            "Iteration 8541, loss = 0.00157109\n",
            "Iteration 8542, loss = 0.00157029\n",
            "Iteration 8543, loss = 0.00156949\n",
            "Iteration 8544, loss = 0.00156868\n",
            "Iteration 8545, loss = 0.00156788\n",
            "Iteration 8546, loss = 0.00156707\n",
            "Iteration 8547, loss = 0.00156626\n",
            "Iteration 8548, loss = 0.00156554\n",
            "Iteration 8549, loss = 0.00156466\n",
            "Iteration 8550, loss = 0.00156387\n",
            "Iteration 8551, loss = 0.00156307\n",
            "Iteration 8552, loss = 0.00156228\n",
            "Iteration 8553, loss = 0.00156147\n",
            "Iteration 8554, loss = 0.00156067\n",
            "Iteration 8555, loss = 0.00155987\n",
            "Iteration 8556, loss = 0.00155908\n",
            "Iteration 8557, loss = 0.00155828\n",
            "Iteration 8558, loss = 0.00155749\n",
            "Iteration 8559, loss = 0.00155670\n",
            "Iteration 8560, loss = 0.00155591\n",
            "Iteration 8561, loss = 0.00155511\n",
            "Iteration 8562, loss = 0.00155432\n",
            "Iteration 8563, loss = 0.00155352\n",
            "Iteration 8564, loss = 0.00155271\n",
            "Iteration 8565, loss = 0.00155191\n",
            "Iteration 8566, loss = 0.00155117\n",
            "Iteration 8567, loss = 0.00155033\n",
            "Iteration 8568, loss = 0.00154954\n",
            "Iteration 8569, loss = 0.00154875\n",
            "Iteration 8570, loss = 0.00154797\n",
            "Iteration 8571, loss = 0.00154717\n",
            "Iteration 8572, loss = 0.00154638\n",
            "Iteration 8573, loss = 0.00154558\n",
            "Iteration 8574, loss = 0.00154483\n",
            "Iteration 8575, loss = 0.00154400\n",
            "Iteration 8576, loss = 0.00154323\n",
            "Iteration 8577, loss = 0.00154244\n",
            "Iteration 8578, loss = 0.00154166\n",
            "Iteration 8579, loss = 0.00154087\n",
            "Iteration 8580, loss = 0.00154008\n",
            "Iteration 8581, loss = 0.00153929\n",
            "Iteration 8582, loss = 0.00153849\n",
            "Iteration 8583, loss = 0.00153770\n",
            "Iteration 8584, loss = 0.00153693\n",
            "Iteration 8585, loss = 0.00153615\n",
            "Iteration 8586, loss = 0.00153537\n",
            "Iteration 8587, loss = 0.00153459\n",
            "Iteration 8588, loss = 0.00153381\n",
            "Iteration 8589, loss = 0.00153302\n",
            "Iteration 8590, loss = 0.00153223\n",
            "Iteration 8591, loss = 0.00153144\n",
            "Iteration 8592, loss = 0.00153066\n",
            "Iteration 8593, loss = 0.00152988\n",
            "Iteration 8594, loss = 0.00152911\n",
            "Iteration 8595, loss = 0.00152833\n",
            "Iteration 8596, loss = 0.00152756\n",
            "Iteration 8597, loss = 0.00152678\n",
            "Iteration 8598, loss = 0.00152599\n",
            "Iteration 8599, loss = 0.00152521\n",
            "Iteration 8600, loss = 0.00152442\n",
            "Iteration 8601, loss = 0.00152367\n",
            "Iteration 8602, loss = 0.00152287\n",
            "Iteration 8603, loss = 0.00152210\n",
            "Iteration 8604, loss = 0.00152133\n",
            "Iteration 8605, loss = 0.00152056\n",
            "Iteration 8606, loss = 0.00151978\n",
            "Iteration 8607, loss = 0.00151900\n",
            "Iteration 8608, loss = 0.00151822\n",
            "Iteration 8609, loss = 0.00151744\n",
            "Iteration 8610, loss = 0.00151671\n",
            "Iteration 8611, loss = 0.00151589\n",
            "Iteration 8612, loss = 0.00151513\n",
            "Iteration 8613, loss = 0.00151436\n",
            "Iteration 8614, loss = 0.00151359\n",
            "Iteration 8615, loss = 0.00151282\n",
            "Iteration 8616, loss = 0.00151204\n",
            "Iteration 8617, loss = 0.00151127\n",
            "Iteration 8618, loss = 0.00151049\n",
            "Iteration 8619, loss = 0.00150979\n",
            "Iteration 8620, loss = 0.00150895\n",
            "Iteration 8621, loss = 0.00150819\n",
            "Iteration 8622, loss = 0.00150743\n",
            "Iteration 8623, loss = 0.00150666\n",
            "Iteration 8624, loss = 0.00150589\n",
            "Iteration 8625, loss = 0.00150512\n",
            "Iteration 8626, loss = 0.00150435\n",
            "Iteration 8627, loss = 0.00150358\n",
            "Iteration 8628, loss = 0.00150282\n",
            "Iteration 8629, loss = 0.00150210\n",
            "Iteration 8630, loss = 0.00150130\n",
            "Iteration 8631, loss = 0.00150053\n",
            "Iteration 8632, loss = 0.00149975\n",
            "Iteration 8633, loss = 0.00149902\n",
            "Iteration 8634, loss = 0.00149824\n",
            "Iteration 8635, loss = 0.00149748\n",
            "Iteration 8636, loss = 0.00149672\n",
            "Iteration 8637, loss = 0.00149596\n",
            "Iteration 8638, loss = 0.00149520\n",
            "Iteration 8639, loss = 0.00149443\n",
            "Iteration 8640, loss = 0.00149367\n",
            "Iteration 8641, loss = 0.00149290\n",
            "Iteration 8642, loss = 0.00149219\n",
            "Iteration 8643, loss = 0.00149138\n",
            "Iteration 8644, loss = 0.00149064\n",
            "Iteration 8645, loss = 0.00148988\n",
            "Iteration 8646, loss = 0.00148912\n",
            "Iteration 8647, loss = 0.00148836\n",
            "Iteration 8648, loss = 0.00148760\n",
            "Iteration 8649, loss = 0.00148684\n",
            "Iteration 8650, loss = 0.00148608\n",
            "Iteration 8651, loss = 0.00148536\n",
            "Iteration 8652, loss = 0.00148457\n",
            "Iteration 8653, loss = 0.00148382\n",
            "Iteration 8654, loss = 0.00148307\n",
            "Iteration 8655, loss = 0.00148232\n",
            "Iteration 8656, loss = 0.00148156\n",
            "Iteration 8657, loss = 0.00148081\n",
            "Iteration 8658, loss = 0.00148005\n",
            "Iteration 8659, loss = 0.00147929\n",
            "Iteration 8660, loss = 0.00147856\n",
            "Iteration 8661, loss = 0.00147779\n",
            "Iteration 8662, loss = 0.00147704\n",
            "Iteration 8663, loss = 0.00147630\n",
            "Iteration 8664, loss = 0.00147555\n",
            "Iteration 8665, loss = 0.00147480\n",
            "Iteration 8666, loss = 0.00147405\n",
            "Iteration 8667, loss = 0.00147329\n",
            "Iteration 8668, loss = 0.00147253\n",
            "Iteration 8669, loss = 0.00147179\n",
            "Iteration 8670, loss = 0.00147104\n",
            "Iteration 8671, loss = 0.00147030\n",
            "Iteration 8672, loss = 0.00146956\n",
            "Iteration 8673, loss = 0.00146882\n",
            "Iteration 8674, loss = 0.00146807\n",
            "Iteration 8675, loss = 0.00146732\n",
            "Iteration 8676, loss = 0.00146657\n",
            "Iteration 8677, loss = 0.00146582\n",
            "Iteration 8678, loss = 0.00146506\n",
            "Iteration 8679, loss = 0.00146439\n",
            "Iteration 8680, loss = 0.00146358\n",
            "Iteration 8681, loss = 0.00146284\n",
            "Iteration 8682, loss = 0.00146210\n",
            "Iteration 8683, loss = 0.00146136\n",
            "Iteration 8684, loss = 0.00146061\n",
            "Iteration 8685, loss = 0.00145987\n",
            "Iteration 8686, loss = 0.00145912\n",
            "Iteration 8687, loss = 0.00145842\n",
            "Iteration 8688, loss = 0.00145764\n",
            "Iteration 8689, loss = 0.00145691\n",
            "Iteration 8690, loss = 0.00145618\n",
            "Iteration 8691, loss = 0.00145544\n",
            "Iteration 8692, loss = 0.00145470\n",
            "Iteration 8693, loss = 0.00145396\n",
            "Iteration 8694, loss = 0.00145322\n",
            "Iteration 8695, loss = 0.00145248\n",
            "Iteration 8696, loss = 0.00145173\n",
            "Iteration 8697, loss = 0.00145104\n",
            "Iteration 8698, loss = 0.00145026\n",
            "Iteration 8699, loss = 0.00144953\n",
            "Iteration 8700, loss = 0.00144880\n",
            "Iteration 8701, loss = 0.00144807\n",
            "Iteration 8702, loss = 0.00144733\n",
            "Iteration 8703, loss = 0.00144659\n",
            "Iteration 8704, loss = 0.00144585\n",
            "Iteration 8705, loss = 0.00144512\n",
            "Iteration 8706, loss = 0.00144439\n",
            "Iteration 8707, loss = 0.00144367\n",
            "Iteration 8708, loss = 0.00144294\n",
            "Iteration 8709, loss = 0.00144221\n",
            "Iteration 8710, loss = 0.00144148\n",
            "Iteration 8711, loss = 0.00144074\n",
            "Iteration 8712, loss = 0.00144001\n",
            "Iteration 8713, loss = 0.00143927\n",
            "Iteration 8714, loss = 0.00143853\n",
            "Iteration 8715, loss = 0.00143781\n",
            "Iteration 8716, loss = 0.00143708\n",
            "Iteration 8717, loss = 0.00143636\n",
            "Iteration 8718, loss = 0.00143563\n",
            "Iteration 8719, loss = 0.00143491\n",
            "Iteration 8720, loss = 0.00143418\n",
            "Iteration 8721, loss = 0.00143345\n",
            "Iteration 8722, loss = 0.00143272\n",
            "Iteration 8723, loss = 0.00143198\n",
            "Iteration 8724, loss = 0.00143130\n",
            "Iteration 8725, loss = 0.00143053\n",
            "Iteration 8726, loss = 0.00142982\n",
            "Iteration 8727, loss = 0.00142909\n",
            "Iteration 8728, loss = 0.00142837\n",
            "Iteration 8729, loss = 0.00142765\n",
            "Iteration 8730, loss = 0.00142692\n",
            "Iteration 8731, loss = 0.00142619\n",
            "Iteration 8732, loss = 0.00142546\n",
            "Iteration 8733, loss = 0.00142478\n",
            "Iteration 8734, loss = 0.00142402\n",
            "Iteration 8735, loss = 0.00142331\n",
            "Iteration 8736, loss = 0.00142259\n",
            "Iteration 8737, loss = 0.00142187\n",
            "Iteration 8738, loss = 0.00142115\n",
            "Iteration 8739, loss = 0.00142043\n",
            "Iteration 8740, loss = 0.00141970\n",
            "Iteration 8741, loss = 0.00141898\n",
            "Iteration 8742, loss = 0.00141829\n",
            "Iteration 8743, loss = 0.00141755\n",
            "Iteration 8744, loss = 0.00141684\n",
            "Iteration 8745, loss = 0.00141612\n",
            "Iteration 8746, loss = 0.00141541\n",
            "Iteration 8747, loss = 0.00141469\n",
            "Iteration 8748, loss = 0.00141397\n",
            "Iteration 8749, loss = 0.00141325\n",
            "Iteration 8750, loss = 0.00141253\n",
            "Iteration 8751, loss = 0.00141182\n",
            "Iteration 8752, loss = 0.00141110\n",
            "Iteration 8753, loss = 0.00141040\n",
            "Iteration 8754, loss = 0.00140969\n",
            "Iteration 8755, loss = 0.00140898\n",
            "Iteration 8756, loss = 0.00140826\n",
            "Iteration 8757, loss = 0.00140755\n",
            "Iteration 8758, loss = 0.00140683\n",
            "Iteration 8759, loss = 0.00140611\n",
            "Iteration 8760, loss = 0.00140539\n",
            "Iteration 8761, loss = 0.00140476\n",
            "Iteration 8762, loss = 0.00140397\n",
            "Iteration 8763, loss = 0.00140327\n",
            "Iteration 8764, loss = 0.00140256\n",
            "Iteration 8765, loss = 0.00140185\n",
            "Iteration 8766, loss = 0.00140114\n",
            "Iteration 8767, loss = 0.00140043\n",
            "Iteration 8768, loss = 0.00139972\n",
            "Iteration 8769, loss = 0.00139905\n",
            "Iteration 8770, loss = 0.00139830\n",
            "Iteration 8771, loss = 0.00139760\n",
            "Iteration 8772, loss = 0.00139690\n",
            "Iteration 8773, loss = 0.00139620\n",
            "Iteration 8774, loss = 0.00139549\n",
            "Iteration 8775, loss = 0.00139479\n",
            "Iteration 8776, loss = 0.00139408\n",
            "Iteration 8777, loss = 0.00139337\n",
            "Iteration 8778, loss = 0.00139265\n",
            "Iteration 8779, loss = 0.00139200\n",
            "Iteration 8780, loss = 0.00139125\n",
            "Iteration 8781, loss = 0.00139055\n",
            "Iteration 8782, loss = 0.00138985\n",
            "Iteration 8783, loss = 0.00138915\n",
            "Iteration 8784, loss = 0.00138845\n",
            "Iteration 8785, loss = 0.00138774\n",
            "Iteration 8786, loss = 0.00138704\n",
            "Iteration 8787, loss = 0.00138634\n",
            "Iteration 8788, loss = 0.00138564\n",
            "Iteration 8789, loss = 0.00138495\n",
            "Iteration 8790, loss = 0.00138426\n",
            "Iteration 8791, loss = 0.00138356\n",
            "Iteration 8792, loss = 0.00138286\n",
            "Iteration 8793, loss = 0.00138216\n",
            "Iteration 8794, loss = 0.00138146\n",
            "Iteration 8795, loss = 0.00138075\n",
            "Iteration 8796, loss = 0.00138005\n",
            "Iteration 8797, loss = 0.00137937\n",
            "Iteration 8798, loss = 0.00137866\n",
            "Iteration 8799, loss = 0.00137797\n",
            "Iteration 8800, loss = 0.00137727\n",
            "Iteration 8801, loss = 0.00137658\n",
            "Iteration 8802, loss = 0.00137588\n",
            "Iteration 8803, loss = 0.00137519\n",
            "Iteration 8804, loss = 0.00137449\n",
            "Iteration 8805, loss = 0.00137379\n",
            "Iteration 8806, loss = 0.00137315\n",
            "Iteration 8807, loss = 0.00137240\n",
            "Iteration 8808, loss = 0.00137172\n",
            "Iteration 8809, loss = 0.00137103\n",
            "Iteration 8810, loss = 0.00137034\n",
            "Iteration 8811, loss = 0.00136965\n",
            "Iteration 8812, loss = 0.00136895\n",
            "Iteration 8813, loss = 0.00136826\n",
            "Iteration 8814, loss = 0.00136756\n",
            "Iteration 8815, loss = 0.00136692\n",
            "Iteration 8816, loss = 0.00136618\n",
            "Iteration 8817, loss = 0.00136550\n",
            "Iteration 8818, loss = 0.00136482\n",
            "Iteration 8819, loss = 0.00136413\n",
            "Iteration 8820, loss = 0.00136344\n",
            "Iteration 8821, loss = 0.00136275\n",
            "Iteration 8822, loss = 0.00136206\n",
            "Iteration 8823, loss = 0.00136136\n",
            "Iteration 8824, loss = 0.00136071\n",
            "Iteration 8825, loss = 0.00136000\n",
            "Iteration 8826, loss = 0.00135932\n",
            "Iteration 8827, loss = 0.00135864\n",
            "Iteration 8828, loss = 0.00135795\n",
            "Iteration 8829, loss = 0.00135727\n",
            "Iteration 8830, loss = 0.00135658\n",
            "Iteration 8831, loss = 0.00135589\n",
            "Iteration 8832, loss = 0.00135520\n",
            "Iteration 8833, loss = 0.00135453\n",
            "Iteration 8834, loss = 0.00135384\n",
            "Iteration 8835, loss = 0.00135317\n",
            "Iteration 8836, loss = 0.00135249\n",
            "Iteration 8837, loss = 0.00135181\n",
            "Iteration 8838, loss = 0.00135113\n",
            "Iteration 8839, loss = 0.00135044\n",
            "Iteration 8840, loss = 0.00134976\n",
            "Iteration 8841, loss = 0.00134907\n",
            "Iteration 8842, loss = 0.00134838\n",
            "Iteration 8843, loss = 0.00134771\n",
            "Iteration 8844, loss = 0.00134704\n",
            "Iteration 8845, loss = 0.00134637\n",
            "Iteration 8846, loss = 0.00134569\n",
            "Iteration 8847, loss = 0.00134501\n",
            "Iteration 8848, loss = 0.00134433\n",
            "Iteration 8849, loss = 0.00134365\n",
            "Iteration 8850, loss = 0.00134297\n",
            "Iteration 8851, loss = 0.00134228\n",
            "Iteration 8852, loss = 0.00134167\n",
            "Iteration 8853, loss = 0.00134094\n",
            "Iteration 8854, loss = 0.00134027\n",
            "Iteration 8855, loss = 0.00133959\n",
            "Iteration 8856, loss = 0.00133892\n",
            "Iteration 8857, loss = 0.00133824\n",
            "Iteration 8858, loss = 0.00133757\n",
            "Iteration 8859, loss = 0.00133689\n",
            "Iteration 8860, loss = 0.00133625\n",
            "Iteration 8861, loss = 0.00133555\n",
            "Iteration 8862, loss = 0.00133488\n",
            "Iteration 8863, loss = 0.00133422\n",
            "Iteration 8864, loss = 0.00133355\n",
            "Iteration 8865, loss = 0.00133288\n",
            "Iteration 8866, loss = 0.00133220\n",
            "Iteration 8867, loss = 0.00133153\n",
            "Iteration 8868, loss = 0.00133085\n",
            "Iteration 8869, loss = 0.00133018\n",
            "Iteration 8870, loss = 0.00132955\n",
            "Iteration 8871, loss = 0.00132884\n",
            "Iteration 8872, loss = 0.00132818\n",
            "Iteration 8873, loss = 0.00132751\n",
            "Iteration 8874, loss = 0.00132685\n",
            "Iteration 8875, loss = 0.00132618\n",
            "Iteration 8876, loss = 0.00132551\n",
            "Iteration 8877, loss = 0.00132484\n",
            "Iteration 8878, loss = 0.00132417\n",
            "Iteration 8879, loss = 0.00132351\n",
            "Iteration 8880, loss = 0.00132285\n",
            "Iteration 8881, loss = 0.00132219\n",
            "Iteration 8882, loss = 0.00132153\n",
            "Iteration 8883, loss = 0.00132086\n",
            "Iteration 8884, loss = 0.00132020\n",
            "Iteration 8885, loss = 0.00131953\n",
            "Iteration 8886, loss = 0.00131886\n",
            "Iteration 8887, loss = 0.00131819\n",
            "Iteration 8888, loss = 0.00131753\n",
            "Iteration 8889, loss = 0.00131687\n",
            "Iteration 8890, loss = 0.00131621\n",
            "Iteration 8891, loss = 0.00131555\n",
            "Iteration 8892, loss = 0.00131490\n",
            "Iteration 8893, loss = 0.00131423\n",
            "Iteration 8894, loss = 0.00131357\n",
            "Iteration 8895, loss = 0.00131290\n",
            "Iteration 8896, loss = 0.00131224\n",
            "Iteration 8897, loss = 0.00131162\n",
            "Iteration 8898, loss = 0.00131092\n",
            "Iteration 8899, loss = 0.00131027\n",
            "Iteration 8900, loss = 0.00130961\n",
            "Iteration 8901, loss = 0.00130896\n",
            "Iteration 8902, loss = 0.00130830\n",
            "Iteration 8903, loss = 0.00130764\n",
            "Iteration 8904, loss = 0.00130698\n",
            "Iteration 8905, loss = 0.00130632\n",
            "Iteration 8906, loss = 0.00130569\n",
            "Iteration 8907, loss = 0.00130501\n",
            "Iteration 8908, loss = 0.00130436\n",
            "Iteration 8909, loss = 0.00130371\n",
            "Iteration 8910, loss = 0.00130306\n",
            "Iteration 8911, loss = 0.00130240\n",
            "Iteration 8912, loss = 0.00130174\n",
            "Iteration 8913, loss = 0.00130109\n",
            "Iteration 8914, loss = 0.00130043\n",
            "Iteration 8915, loss = 0.00129979\n",
            "Iteration 8916, loss = 0.00129912\n",
            "Iteration 8917, loss = 0.00129848\n",
            "Iteration 8918, loss = 0.00129783\n",
            "Iteration 8919, loss = 0.00129718\n",
            "Iteration 8920, loss = 0.00129653\n",
            "Iteration 8921, loss = 0.00129588\n",
            "Iteration 8922, loss = 0.00129522\n",
            "Iteration 8923, loss = 0.00129457\n",
            "Iteration 8924, loss = 0.00129391\n",
            "Iteration 8925, loss = 0.00129327\n",
            "Iteration 8926, loss = 0.00129263\n",
            "Iteration 8927, loss = 0.00129198\n",
            "Iteration 8928, loss = 0.00129134\n",
            "Iteration 8929, loss = 0.00129069\n",
            "Iteration 8930, loss = 0.00129004\n",
            "Iteration 8931, loss = 0.00128939\n",
            "Iteration 8932, loss = 0.00128873\n",
            "Iteration 8933, loss = 0.00128808\n",
            "Iteration 8934, loss = 0.00128750\n",
            "Iteration 8935, loss = 0.00128679\n",
            "Iteration 8936, loss = 0.00128615\n",
            "Iteration 8937, loss = 0.00128551\n",
            "Iteration 8938, loss = 0.00128487\n",
            "Iteration 8939, loss = 0.00128422\n",
            "Iteration 8940, loss = 0.00128357\n",
            "Iteration 8941, loss = 0.00128293\n",
            "Iteration 8942, loss = 0.00128231\n",
            "Iteration 8943, loss = 0.00128164\n",
            "Iteration 8944, loss = 0.00128101\n",
            "Iteration 8945, loss = 0.00128037\n",
            "Iteration 8946, loss = 0.00127973\n",
            "Iteration 8947, loss = 0.00127909\n",
            "Iteration 8948, loss = 0.00127845\n",
            "Iteration 8949, loss = 0.00127780\n",
            "Iteration 8950, loss = 0.00127716\n",
            "Iteration 8951, loss = 0.00127651\n",
            "Iteration 8952, loss = 0.00127590\n",
            "Iteration 8953, loss = 0.00127524\n",
            "Iteration 8954, loss = 0.00127460\n",
            "Iteration 8955, loss = 0.00127397\n",
            "Iteration 8956, loss = 0.00127333\n",
            "Iteration 8957, loss = 0.00127269\n",
            "Iteration 8958, loss = 0.00127205\n",
            "Iteration 8959, loss = 0.00127141\n",
            "Iteration 8960, loss = 0.00127076\n",
            "Iteration 8961, loss = 0.00127019\n",
            "Iteration 8962, loss = 0.00126949\n",
            "Iteration 8963, loss = 0.00126887\n",
            "Iteration 8964, loss = 0.00126823\n",
            "Iteration 8965, loss = 0.00126760\n",
            "Iteration 8966, loss = 0.00126697\n",
            "Iteration 8967, loss = 0.00126633\n",
            "Iteration 8968, loss = 0.00126569\n",
            "Iteration 8969, loss = 0.00126505\n",
            "Iteration 8970, loss = 0.00126447\n",
            "Iteration 8971, loss = 0.00126379\n",
            "Iteration 8972, loss = 0.00126316\n",
            "Iteration 8973, loss = 0.00126253\n",
            "Iteration 8974, loss = 0.00126190\n",
            "Iteration 8975, loss = 0.00126127\n",
            "Iteration 8976, loss = 0.00126064\n",
            "Iteration 8977, loss = 0.00126000\n",
            "Iteration 8978, loss = 0.00125936\n",
            "Iteration 8979, loss = 0.00125877\n",
            "Iteration 8980, loss = 0.00125811\n",
            "Iteration 8981, loss = 0.00125749\n",
            "Iteration 8982, loss = 0.00125686\n",
            "Iteration 8983, loss = 0.00125623\n",
            "Iteration 8984, loss = 0.00125560\n",
            "Iteration 8985, loss = 0.00125497\n",
            "Iteration 8986, loss = 0.00125434\n",
            "Iteration 8987, loss = 0.00125371\n",
            "Iteration 8988, loss = 0.00125309\n",
            "Iteration 8989, loss = 0.00125246\n",
            "Iteration 8990, loss = 0.00125184\n",
            "Iteration 8991, loss = 0.00125122\n",
            "Iteration 8992, loss = 0.00125059\n",
            "Iteration 8993, loss = 0.00124997\n",
            "Iteration 8994, loss = 0.00124934\n",
            "Iteration 8995, loss = 0.00124871\n",
            "Iteration 8996, loss = 0.00124808\n",
            "Iteration 8997, loss = 0.00124745\n",
            "Iteration 8998, loss = 0.00124689\n",
            "Iteration 8999, loss = 0.00124621\n",
            "Iteration 9000, loss = 0.00124559\n",
            "Iteration 9001, loss = 0.00124497\n",
            "Iteration 9002, loss = 0.00124435\n",
            "Iteration 9003, loss = 0.00124372\n",
            "Iteration 9004, loss = 0.00124310\n",
            "Iteration 9005, loss = 0.00124247\n",
            "Iteration 9006, loss = 0.00124188\n",
            "Iteration 9007, loss = 0.00124123\n",
            "Iteration 9008, loss = 0.00124062\n",
            "Iteration 9009, loss = 0.00124001\n",
            "Iteration 9010, loss = 0.00123939\n",
            "Iteration 9011, loss = 0.00123877\n",
            "Iteration 9012, loss = 0.00123815\n",
            "Iteration 9013, loss = 0.00123753\n",
            "Iteration 9014, loss = 0.00123690\n",
            "Iteration 9015, loss = 0.00123628\n",
            "Iteration 9016, loss = 0.00123570\n",
            "Iteration 9017, loss = 0.00123505\n",
            "Iteration 9018, loss = 0.00123444\n",
            "Iteration 9019, loss = 0.00123382\n",
            "Iteration 9020, loss = 0.00123321\n",
            "Iteration 9021, loss = 0.00123259\n",
            "Iteration 9022, loss = 0.00123197\n",
            "Iteration 9023, loss = 0.00123135\n",
            "Iteration 9024, loss = 0.00123073\n",
            "Iteration 9025, loss = 0.00123018\n",
            "Iteration 9026, loss = 0.00122951\n",
            "Iteration 9027, loss = 0.00122890\n",
            "Iteration 9028, loss = 0.00122829\n",
            "Iteration 9029, loss = 0.00122768\n",
            "Iteration 9030, loss = 0.00122707\n",
            "Iteration 9031, loss = 0.00122645\n",
            "Iteration 9032, loss = 0.00122583\n",
            "Iteration 9033, loss = 0.00122522\n",
            "Iteration 9034, loss = 0.00122465\n",
            "Iteration 9035, loss = 0.00122400\n",
            "Iteration 9036, loss = 0.00122339\n",
            "Iteration 9037, loss = 0.00122279\n",
            "Iteration 9038, loss = 0.00122218\n",
            "Iteration 9039, loss = 0.00122157\n",
            "Iteration 9040, loss = 0.00122095\n",
            "Iteration 9041, loss = 0.00122034\n",
            "Iteration 9042, loss = 0.00121973\n",
            "Iteration 9043, loss = 0.00121914\n",
            "Iteration 9044, loss = 0.00121851\n",
            "Iteration 9045, loss = 0.00121791\n",
            "Iteration 9046, loss = 0.00121731\n",
            "Iteration 9047, loss = 0.00121671\n",
            "Iteration 9048, loss = 0.00121610\n",
            "Iteration 9049, loss = 0.00121549\n",
            "Iteration 9050, loss = 0.00121488\n",
            "Iteration 9051, loss = 0.00121427\n",
            "Iteration 9052, loss = 0.00121366\n",
            "Iteration 9053, loss = 0.00121306\n",
            "Iteration 9054, loss = 0.00121246\n",
            "Iteration 9055, loss = 0.00121186\n",
            "Iteration 9056, loss = 0.00121126\n",
            "Iteration 9057, loss = 0.00121066\n",
            "Iteration 9058, loss = 0.00121005\n",
            "Iteration 9059, loss = 0.00120944\n",
            "Iteration 9060, loss = 0.00120883\n",
            "Iteration 9061, loss = 0.00120822\n",
            "Iteration 9062, loss = 0.00120767\n",
            "Iteration 9063, loss = 0.00120702\n",
            "Iteration 9064, loss = 0.00120643\n",
            "Iteration 9065, loss = 0.00120583\n",
            "Iteration 9066, loss = 0.00120523\n",
            "Iteration 9067, loss = 0.00120463\n",
            "Iteration 9068, loss = 0.00120402\n",
            "Iteration 9069, loss = 0.00120342\n",
            "Iteration 9070, loss = 0.00120283\n",
            "Iteration 9071, loss = 0.00120222\n",
            "Iteration 9072, loss = 0.00120163\n",
            "Iteration 9073, loss = 0.00120104\n",
            "Iteration 9074, loss = 0.00120044\n",
            "Iteration 9075, loss = 0.00119985\n",
            "Iteration 9076, loss = 0.00119925\n",
            "Iteration 9077, loss = 0.00119865\n",
            "Iteration 9078, loss = 0.00119804\n",
            "Iteration 9079, loss = 0.00119744\n",
            "Iteration 9080, loss = 0.00119686\n",
            "Iteration 9081, loss = 0.00119625\n",
            "Iteration 9082, loss = 0.00119566\n",
            "Iteration 9083, loss = 0.00119507\n",
            "Iteration 9084, loss = 0.00119448\n",
            "Iteration 9085, loss = 0.00119388\n",
            "Iteration 9086, loss = 0.00119328\n",
            "Iteration 9087, loss = 0.00119269\n",
            "Iteration 9088, loss = 0.00119208\n",
            "Iteration 9089, loss = 0.00119154\n",
            "Iteration 9090, loss = 0.00119090\n",
            "Iteration 9091, loss = 0.00119032\n",
            "Iteration 9092, loss = 0.00118973\n",
            "Iteration 9093, loss = 0.00118914\n",
            "Iteration 9094, loss = 0.00118855\n",
            "Iteration 9095, loss = 0.00118795\n",
            "Iteration 9096, loss = 0.00118736\n",
            "Iteration 9097, loss = 0.00118676\n",
            "Iteration 9098, loss = 0.00118620\n",
            "Iteration 9099, loss = 0.00118558\n",
            "Iteration 9100, loss = 0.00118500\n",
            "Iteration 9101, loss = 0.00118441\n",
            "Iteration 9102, loss = 0.00118383\n",
            "Iteration 9103, loss = 0.00118324\n",
            "Iteration 9104, loss = 0.00118265\n",
            "Iteration 9105, loss = 0.00118205\n",
            "Iteration 9106, loss = 0.00118146\n",
            "Iteration 9107, loss = 0.00118088\n",
            "Iteration 9108, loss = 0.00118029\n",
            "Iteration 9109, loss = 0.00117971\n",
            "Iteration 9110, loss = 0.00117913\n",
            "Iteration 9111, loss = 0.00117854\n",
            "Iteration 9112, loss = 0.00117796\n",
            "Iteration 9113, loss = 0.00117737\n",
            "Iteration 9114, loss = 0.00117678\n",
            "Iteration 9115, loss = 0.00117619\n",
            "Iteration 9116, loss = 0.00117560\n",
            "Iteration 9117, loss = 0.00117506\n",
            "Iteration 9118, loss = 0.00117443\n",
            "Iteration 9119, loss = 0.00117385\n",
            "Iteration 9120, loss = 0.00117327\n",
            "Iteration 9121, loss = 0.00117269\n",
            "Iteration 9122, loss = 0.00117211\n",
            "Iteration 9123, loss = 0.00117152\n",
            "Iteration 9124, loss = 0.00117094\n",
            "Iteration 9125, loss = 0.00117037\n",
            "Iteration 9126, loss = 0.00116977\n",
            "Iteration 9127, loss = 0.00116920\n",
            "Iteration 9128, loss = 0.00116863\n",
            "Iteration 9129, loss = 0.00116805\n",
            "Iteration 9130, loss = 0.00116747\n",
            "Iteration 9131, loss = 0.00116689\n",
            "Iteration 9132, loss = 0.00116630\n",
            "Iteration 9133, loss = 0.00116572\n",
            "Iteration 9134, loss = 0.00116513\n",
            "Iteration 9135, loss = 0.00116457\n",
            "Iteration 9136, loss = 0.00116398\n",
            "Iteration 9137, loss = 0.00116341\n",
            "Iteration 9138, loss = 0.00116283\n",
            "Iteration 9139, loss = 0.00116226\n",
            "Iteration 9140, loss = 0.00116168\n",
            "Iteration 9141, loss = 0.00116110\n",
            "Iteration 9142, loss = 0.00116052\n",
            "Iteration 9143, loss = 0.00115994\n",
            "Iteration 9144, loss = 0.00115939\n",
            "Iteration 9145, loss = 0.00115879\n",
            "Iteration 9146, loss = 0.00115822\n",
            "Iteration 9147, loss = 0.00115765\n",
            "Iteration 9148, loss = 0.00115708\n",
            "Iteration 9149, loss = 0.00115650\n",
            "Iteration 9150, loss = 0.00115592\n",
            "Iteration 9151, loss = 0.00115535\n",
            "Iteration 9152, loss = 0.00115477\n",
            "Iteration 9153, loss = 0.00115421\n",
            "Iteration 9154, loss = 0.00115363\n",
            "Iteration 9155, loss = 0.00115306\n",
            "Iteration 9156, loss = 0.00115249\n",
            "Iteration 9157, loss = 0.00115192\n",
            "Iteration 9158, loss = 0.00115135\n",
            "Iteration 9159, loss = 0.00115078\n",
            "Iteration 9160, loss = 0.00115020\n",
            "Iteration 9161, loss = 0.00114963\n",
            "Iteration 9162, loss = 0.00114905\n",
            "Iteration 9163, loss = 0.00114854\n",
            "Iteration 9164, loss = 0.00114791\n",
            "Iteration 9165, loss = 0.00114735\n",
            "Iteration 9166, loss = 0.00114678\n",
            "Iteration 9167, loss = 0.00114622\n",
            "Iteration 9168, loss = 0.00114565\n",
            "Iteration 9169, loss = 0.00114508\n",
            "Iteration 9170, loss = 0.00114451\n",
            "Iteration 9171, loss = 0.00114396\n",
            "Iteration 9172, loss = 0.00114337\n",
            "Iteration 9173, loss = 0.00114282\n",
            "Iteration 9174, loss = 0.00114225\n",
            "Iteration 9175, loss = 0.00114168\n",
            "Iteration 9176, loss = 0.00114111\n",
            "Iteration 9177, loss = 0.00114056\n",
            "Iteration 9178, loss = 0.00113998\n",
            "Iteration 9179, loss = 0.00113942\n",
            "Iteration 9180, loss = 0.00113886\n",
            "Iteration 9181, loss = 0.00113830\n",
            "Iteration 9182, loss = 0.00113774\n",
            "Iteration 9183, loss = 0.00113717\n",
            "Iteration 9184, loss = 0.00113660\n",
            "Iteration 9185, loss = 0.00113604\n",
            "Iteration 9186, loss = 0.00113547\n",
            "Iteration 9187, loss = 0.00113495\n",
            "Iteration 9188, loss = 0.00113435\n",
            "Iteration 9189, loss = 0.00113379\n",
            "Iteration 9190, loss = 0.00113323\n",
            "Iteration 9191, loss = 0.00113268\n",
            "Iteration 9192, loss = 0.00113211\n",
            "Iteration 9193, loss = 0.00113155\n",
            "Iteration 9194, loss = 0.00113099\n",
            "Iteration 9195, loss = 0.00113042\n",
            "Iteration 9196, loss = 0.00112991\n",
            "Iteration 9197, loss = 0.00112930\n",
            "Iteration 9198, loss = 0.00112875\n",
            "Iteration 9199, loss = 0.00112819\n",
            "Iteration 9200, loss = 0.00112764\n",
            "Iteration 9201, loss = 0.00112708\n",
            "Iteration 9202, loss = 0.00112652\n",
            "Iteration 9203, loss = 0.00112596\n",
            "Iteration 9204, loss = 0.00112540\n",
            "Iteration 9205, loss = 0.00112485\n",
            "Iteration 9206, loss = 0.00112429\n",
            "Iteration 9207, loss = 0.00112374\n",
            "Iteration 9208, loss = 0.00112318\n",
            "Iteration 9209, loss = 0.00112263\n",
            "Iteration 9210, loss = 0.00112208\n",
            "Iteration 9211, loss = 0.00112152\n",
            "Iteration 9212, loss = 0.00112096\n",
            "Iteration 9213, loss = 0.00112040\n",
            "Iteration 9214, loss = 0.00111984\n",
            "Iteration 9215, loss = 0.00111931\n",
            "Iteration 9216, loss = 0.00111874\n",
            "Iteration 9217, loss = 0.00111819\n",
            "Iteration 9218, loss = 0.00111764\n",
            "Iteration 9219, loss = 0.00111709\n",
            "Iteration 9220, loss = 0.00111653\n",
            "Iteration 9221, loss = 0.00111598\n",
            "Iteration 9222, loss = 0.00111543\n",
            "Iteration 9223, loss = 0.00111487\n",
            "Iteration 9224, loss = 0.00111434\n",
            "Iteration 9225, loss = 0.00111377\n",
            "Iteration 9226, loss = 0.00111323\n",
            "Iteration 9227, loss = 0.00111268\n",
            "Iteration 9228, loss = 0.00111213\n",
            "Iteration 9229, loss = 0.00111158\n",
            "Iteration 9230, loss = 0.00111103\n",
            "Iteration 9231, loss = 0.00111048\n",
            "Iteration 9232, loss = 0.00110992\n",
            "Iteration 9233, loss = 0.00110937\n",
            "Iteration 9234, loss = 0.00110887\n",
            "Iteration 9235, loss = 0.00110827\n",
            "Iteration 9236, loss = 0.00110773\n",
            "Iteration 9237, loss = 0.00110719\n",
            "Iteration 9238, loss = 0.00110664\n",
            "Iteration 9239, loss = 0.00110610\n",
            "Iteration 9240, loss = 0.00110555\n",
            "Iteration 9241, loss = 0.00110500\n",
            "Iteration 9242, loss = 0.00110444\n",
            "Iteration 9243, loss = 0.00110396\n",
            "Iteration 9244, loss = 0.00110336\n",
            "Iteration 9245, loss = 0.00110282\n",
            "Iteration 9246, loss = 0.00110228\n",
            "Iteration 9247, loss = 0.00110174\n",
            "Iteration 9248, loss = 0.00110119\n",
            "Iteration 9249, loss = 0.00110064\n",
            "Iteration 9250, loss = 0.00110010\n",
            "Iteration 9251, loss = 0.00109955\n",
            "Iteration 9252, loss = 0.00109902\n",
            "Iteration 9253, loss = 0.00109847\n",
            "Iteration 9254, loss = 0.00109793\n",
            "Iteration 9255, loss = 0.00109739\n",
            "Iteration 9256, loss = 0.00109686\n",
            "Iteration 9257, loss = 0.00109631\n",
            "Iteration 9258, loss = 0.00109577\n",
            "Iteration 9259, loss = 0.00109523\n",
            "Iteration 9260, loss = 0.00109468\n",
            "Iteration 9261, loss = 0.00109414\n",
            "Iteration 9262, loss = 0.00109362\n",
            "Iteration 9263, loss = 0.00109306\n",
            "Iteration 9264, loss = 0.00109253\n",
            "Iteration 9265, loss = 0.00109199\n",
            "Iteration 9266, loss = 0.00109145\n",
            "Iteration 9267, loss = 0.00109091\n",
            "Iteration 9268, loss = 0.00109037\n",
            "Iteration 9269, loss = 0.00108983\n",
            "Iteration 9270, loss = 0.00108929\n",
            "Iteration 9271, loss = 0.00108878\n",
            "Iteration 9272, loss = 0.00108822\n",
            "Iteration 9273, loss = 0.00108769\n",
            "Iteration 9274, loss = 0.00108715\n",
            "Iteration 9275, loss = 0.00108662\n",
            "Iteration 9276, loss = 0.00108608\n",
            "Iteration 9277, loss = 0.00108554\n",
            "Iteration 9278, loss = 0.00108501\n",
            "Iteration 9279, loss = 0.00108447\n",
            "Iteration 9280, loss = 0.00108393\n",
            "Iteration 9281, loss = 0.00108345\n",
            "Iteration 9282, loss = 0.00108286\n",
            "Iteration 9283, loss = 0.00108233\n",
            "Iteration 9284, loss = 0.00108180\n",
            "Iteration 9285, loss = 0.00108127\n",
            "Iteration 9286, loss = 0.00108074\n",
            "Iteration 9287, loss = 0.00108020\n",
            "Iteration 9288, loss = 0.00107967\n",
            "Iteration 9289, loss = 0.00107913\n",
            "Iteration 9290, loss = 0.00107860\n",
            "Iteration 9291, loss = 0.00107808\n",
            "Iteration 9292, loss = 0.00107755\n",
            "Iteration 9293, loss = 0.00107703\n",
            "Iteration 9294, loss = 0.00107650\n",
            "Iteration 9295, loss = 0.00107596\n",
            "Iteration 9296, loss = 0.00107543\n",
            "Iteration 9297, loss = 0.00107490\n",
            "Iteration 9298, loss = 0.00107436\n",
            "Iteration 9299, loss = 0.00107383\n",
            "Iteration 9300, loss = 0.00107334\n",
            "Iteration 9301, loss = 0.00107277\n",
            "Iteration 9302, loss = 0.00107225\n",
            "Iteration 9303, loss = 0.00107172\n",
            "Iteration 9304, loss = 0.00107120\n",
            "Iteration 9305, loss = 0.00107066\n",
            "Iteration 9306, loss = 0.00107014\n",
            "Iteration 9307, loss = 0.00106960\n",
            "Iteration 9308, loss = 0.00106910\n",
            "Iteration 9309, loss = 0.00106856\n",
            "Iteration 9310, loss = 0.00106804\n",
            "Iteration 9311, loss = 0.00106751\n",
            "Iteration 9312, loss = 0.00106699\n",
            "Iteration 9313, loss = 0.00106647\n",
            "Iteration 9314, loss = 0.00106594\n",
            "Iteration 9315, loss = 0.00106541\n",
            "Iteration 9316, loss = 0.00106488\n",
            "Iteration 9317, loss = 0.00106435\n",
            "Iteration 9318, loss = 0.00106383\n",
            "Iteration 9319, loss = 0.00106331\n",
            "Iteration 9320, loss = 0.00106279\n",
            "Iteration 9321, loss = 0.00106227\n",
            "Iteration 9322, loss = 0.00106175\n",
            "Iteration 9323, loss = 0.00106123\n",
            "Iteration 9324, loss = 0.00106070\n",
            "Iteration 9325, loss = 0.00106018\n",
            "Iteration 9326, loss = 0.00105965\n",
            "Iteration 9327, loss = 0.00105913\n",
            "Iteration 9328, loss = 0.00105861\n",
            "Iteration 9329, loss = 0.00105810\n",
            "Iteration 9330, loss = 0.00105758\n",
            "Iteration 9331, loss = 0.00105706\n",
            "Iteration 9332, loss = 0.00105654\n",
            "Iteration 9333, loss = 0.00105602\n",
            "Iteration 9334, loss = 0.00105550\n",
            "Iteration 9335, loss = 0.00105498\n",
            "Iteration 9336, loss = 0.00105445\n",
            "Iteration 9337, loss = 0.00105397\n",
            "Iteration 9338, loss = 0.00105342\n",
            "Iteration 9339, loss = 0.00105291\n",
            "Iteration 9340, loss = 0.00105239\n",
            "Iteration 9341, loss = 0.00105188\n",
            "Iteration 9342, loss = 0.00105136\n",
            "Iteration 9343, loss = 0.00105084\n",
            "Iteration 9344, loss = 0.00105032\n",
            "Iteration 9345, loss = 0.00104979\n",
            "Iteration 9346, loss = 0.00104932\n",
            "Iteration 9347, loss = 0.00104877\n",
            "Iteration 9348, loss = 0.00104826\n",
            "Iteration 9349, loss = 0.00104775\n",
            "Iteration 9350, loss = 0.00104723\n",
            "Iteration 9351, loss = 0.00104672\n",
            "Iteration 9352, loss = 0.00104620\n",
            "Iteration 9353, loss = 0.00104568\n",
            "Iteration 9354, loss = 0.00104517\n",
            "Iteration 9355, loss = 0.00104466\n",
            "Iteration 9356, loss = 0.00104414\n",
            "Iteration 9357, loss = 0.00104364\n",
            "Iteration 9358, loss = 0.00104313\n",
            "Iteration 9359, loss = 0.00104262\n",
            "Iteration 9360, loss = 0.00104210\n",
            "Iteration 9361, loss = 0.00104159\n",
            "Iteration 9362, loss = 0.00104108\n",
            "Iteration 9363, loss = 0.00104056\n",
            "Iteration 9364, loss = 0.00104004\n",
            "Iteration 9365, loss = 0.00103956\n",
            "Iteration 9366, loss = 0.00103903\n",
            "Iteration 9367, loss = 0.00103852\n",
            "Iteration 9368, loss = 0.00103801\n",
            "Iteration 9369, loss = 0.00103751\n",
            "Iteration 9370, loss = 0.00103700\n",
            "Iteration 9371, loss = 0.00103648\n",
            "Iteration 9372, loss = 0.00103597\n",
            "Iteration 9373, loss = 0.00103546\n",
            "Iteration 9374, loss = 0.00103498\n",
            "Iteration 9375, loss = 0.00103444\n",
            "Iteration 9376, loss = 0.00103395\n",
            "Iteration 9377, loss = 0.00103344\n",
            "Iteration 9378, loss = 0.00103294\n",
            "Iteration 9379, loss = 0.00103243\n",
            "Iteration 9380, loss = 0.00103192\n",
            "Iteration 9381, loss = 0.00103141\n",
            "Iteration 9382, loss = 0.00103090\n",
            "Iteration 9383, loss = 0.00103039\n",
            "Iteration 9384, loss = 0.00102989\n",
            "Iteration 9385, loss = 0.00102939\n",
            "Iteration 9386, loss = 0.00102889\n",
            "Iteration 9387, loss = 0.00102839\n",
            "Iteration 9388, loss = 0.00102788\n",
            "Iteration 9389, loss = 0.00102738\n",
            "Iteration 9390, loss = 0.00102687\n",
            "Iteration 9391, loss = 0.00102636\n",
            "Iteration 9392, loss = 0.00102585\n",
            "Iteration 9393, loss = 0.00102537\n",
            "Iteration 9394, loss = 0.00102485\n",
            "Iteration 9395, loss = 0.00102436\n",
            "Iteration 9396, loss = 0.00102385\n",
            "Iteration 9397, loss = 0.00102336\n",
            "Iteration 9398, loss = 0.00102285\n",
            "Iteration 9399, loss = 0.00102235\n",
            "Iteration 9400, loss = 0.00102184\n",
            "Iteration 9401, loss = 0.00102134\n",
            "Iteration 9402, loss = 0.00102086\n",
            "Iteration 9403, loss = 0.00102034\n",
            "Iteration 9404, loss = 0.00101985\n",
            "Iteration 9405, loss = 0.00101935\n",
            "Iteration 9406, loss = 0.00101885\n",
            "Iteration 9407, loss = 0.00101835\n",
            "Iteration 9408, loss = 0.00101785\n",
            "Iteration 9409, loss = 0.00101735\n",
            "Iteration 9410, loss = 0.00101685\n",
            "Iteration 9411, loss = 0.00101634\n",
            "Iteration 9412, loss = 0.00101590\n",
            "Iteration 9413, loss = 0.00101535\n",
            "Iteration 9414, loss = 0.00101486\n",
            "Iteration 9415, loss = 0.00101436\n",
            "Iteration 9416, loss = 0.00101387\n",
            "Iteration 9417, loss = 0.00101337\n",
            "Iteration 9418, loss = 0.00101287\n",
            "Iteration 9419, loss = 0.00101237\n",
            "Iteration 9420, loss = 0.00101188\n",
            "Iteration 9421, loss = 0.00101139\n",
            "Iteration 9422, loss = 0.00101090\n",
            "Iteration 9423, loss = 0.00101041\n",
            "Iteration 9424, loss = 0.00100991\n",
            "Iteration 9425, loss = 0.00100942\n",
            "Iteration 9426, loss = 0.00100893\n",
            "Iteration 9427, loss = 0.00100843\n",
            "Iteration 9428, loss = 0.00100793\n",
            "Iteration 9429, loss = 0.00100743\n",
            "Iteration 9430, loss = 0.00100693\n",
            "Iteration 9431, loss = 0.00100648\n",
            "Iteration 9432, loss = 0.00100595\n",
            "Iteration 9433, loss = 0.00100546\n",
            "Iteration 9434, loss = 0.00100497\n",
            "Iteration 9435, loss = 0.00100448\n",
            "Iteration 9436, loss = 0.00100399\n",
            "Iteration 9437, loss = 0.00100349\n",
            "Iteration 9438, loss = 0.00100300\n",
            "Iteration 9439, loss = 0.00100253\n",
            "Iteration 9440, loss = 0.00100202\n",
            "Iteration 9441, loss = 0.00100154\n",
            "Iteration 9442, loss = 0.00100105\n",
            "Iteration 9443, loss = 0.00100057\n",
            "Iteration 9444, loss = 0.00100008\n",
            "Iteration 9445, loss = 0.00099959\n",
            "Iteration 9446, loss = 0.00099909\n",
            "Iteration 9447, loss = 0.00099860\n",
            "Iteration 9448, loss = 0.00099811\n",
            "Iteration 9449, loss = 0.00099763\n",
            "Iteration 9450, loss = 0.00099714\n",
            "Iteration 9451, loss = 0.00099665\n",
            "Iteration 9452, loss = 0.00099617\n",
            "Iteration 9453, loss = 0.00099568\n",
            "Iteration 9454, loss = 0.00099520\n",
            "Iteration 9455, loss = 0.00099471\n",
            "Iteration 9456, loss = 0.00099422\n",
            "Iteration 9457, loss = 0.00099373\n",
            "Iteration 9458, loss = 0.00099325\n",
            "Iteration 9459, loss = 0.00099276\n",
            "Iteration 9460, loss = 0.00099228\n",
            "Iteration 9461, loss = 0.00099180\n",
            "Iteration 9462, loss = 0.00099132\n",
            "Iteration 9463, loss = 0.00099083\n",
            "Iteration 9464, loss = 0.00099034\n",
            "Iteration 9465, loss = 0.00098986\n",
            "Iteration 9466, loss = 0.00098937\n",
            "Iteration 9467, loss = 0.00098888\n",
            "Iteration 9468, loss = 0.00098843\n",
            "Iteration 9469, loss = 0.00098792\n",
            "Iteration 9470, loss = 0.00098744\n",
            "Iteration 9471, loss = 0.00098696\n",
            "Iteration 9472, loss = 0.00098648\n",
            "Iteration 9473, loss = 0.00098600\n",
            "Iteration 9474, loss = 0.00098551\n",
            "Iteration 9475, loss = 0.00098503\n",
            "Iteration 9476, loss = 0.00098454\n",
            "Iteration 9477, loss = 0.00098410\n",
            "Iteration 9478, loss = 0.00098358\n",
            "Iteration 9479, loss = 0.00098311\n",
            "Iteration 9480, loss = 0.00098263\n",
            "Iteration 9481, loss = 0.00098215\n",
            "Iteration 9482, loss = 0.00098168\n",
            "Iteration 9483, loss = 0.00098119\n",
            "Iteration 9484, loss = 0.00098071\n",
            "Iteration 9485, loss = 0.00098023\n",
            "Iteration 9486, loss = 0.00097976\n",
            "Iteration 9487, loss = 0.00097928\n",
            "Iteration 9488, loss = 0.00097880\n",
            "Iteration 9489, loss = 0.00097833\n",
            "Iteration 9490, loss = 0.00097785\n",
            "Iteration 9491, loss = 0.00097738\n",
            "Iteration 9492, loss = 0.00097690\n",
            "Iteration 9493, loss = 0.00097642\n",
            "Iteration 9494, loss = 0.00097594\n",
            "Iteration 9495, loss = 0.00097546\n",
            "Iteration 9496, loss = 0.00097501\n",
            "Iteration 9497, loss = 0.00097451\n",
            "Iteration 9498, loss = 0.00097404\n",
            "Iteration 9499, loss = 0.00097356\n",
            "Iteration 9500, loss = 0.00097309\n",
            "Iteration 9501, loss = 0.00097262\n",
            "Iteration 9502, loss = 0.00097214\n",
            "Iteration 9503, loss = 0.00097166\n",
            "Iteration 9504, loss = 0.00097118\n",
            "Iteration 9505, loss = 0.00097074\n",
            "Iteration 9506, loss = 0.00097024\n",
            "Iteration 9507, loss = 0.00096977\n",
            "Iteration 9508, loss = 0.00096930\n",
            "Iteration 9509, loss = 0.00096883\n",
            "Iteration 9510, loss = 0.00096836\n",
            "Iteration 9511, loss = 0.00096789\n",
            "Iteration 9512, loss = 0.00096741\n",
            "Iteration 9513, loss = 0.00096694\n",
            "Iteration 9514, loss = 0.00096646\n",
            "Iteration 9515, loss = 0.00096600\n",
            "Iteration 9516, loss = 0.00096553\n",
            "Iteration 9517, loss = 0.00096506\n",
            "Iteration 9518, loss = 0.00096460\n",
            "Iteration 9519, loss = 0.00096413\n",
            "Iteration 9520, loss = 0.00096365\n",
            "Iteration 9521, loss = 0.00096318\n",
            "Iteration 9522, loss = 0.00096271\n",
            "Iteration 9523, loss = 0.00096223\n",
            "Iteration 9524, loss = 0.00096178\n",
            "Iteration 9525, loss = 0.00096130\n",
            "Iteration 9526, loss = 0.00096084\n",
            "Iteration 9527, loss = 0.00096037\n",
            "Iteration 9528, loss = 0.00095991\n",
            "Iteration 9529, loss = 0.00095944\n",
            "Iteration 9530, loss = 0.00095897\n",
            "Iteration 9531, loss = 0.00095850\n",
            "Iteration 9532, loss = 0.00095802\n",
            "Iteration 9533, loss = 0.00095758\n",
            "Iteration 9534, loss = 0.00095709\n",
            "Iteration 9535, loss = 0.00095663\n",
            "Iteration 9536, loss = 0.00095617\n",
            "Iteration 9537, loss = 0.00095571\n",
            "Iteration 9538, loss = 0.00095524\n",
            "Iteration 9539, loss = 0.00095477\n",
            "Iteration 9540, loss = 0.00095431\n",
            "Iteration 9541, loss = 0.00095384\n",
            "Iteration 9542, loss = 0.00095337\n",
            "Iteration 9543, loss = 0.00095296\n",
            "Iteration 9544, loss = 0.00095244\n",
            "Iteration 9545, loss = 0.00095199\n",
            "Iteration 9546, loss = 0.00095152\n",
            "Iteration 9547, loss = 0.00095106\n",
            "Iteration 9548, loss = 0.00095060\n",
            "Iteration 9549, loss = 0.00095013\n",
            "Iteration 9550, loss = 0.00094967\n",
            "Iteration 9551, loss = 0.00094921\n",
            "Iteration 9552, loss = 0.00094875\n",
            "Iteration 9553, loss = 0.00094829\n",
            "Iteration 9554, loss = 0.00094784\n",
            "Iteration 9555, loss = 0.00094738\n",
            "Iteration 9556, loss = 0.00094692\n",
            "Iteration 9557, loss = 0.00094646\n",
            "Iteration 9558, loss = 0.00094599\n",
            "Iteration 9559, loss = 0.00094553\n",
            "Iteration 9560, loss = 0.00094506\n",
            "Iteration 9561, loss = 0.00094460\n",
            "Iteration 9562, loss = 0.00094418\n",
            "Iteration 9563, loss = 0.00094368\n",
            "Iteration 9564, loss = 0.00094323\n",
            "Iteration 9565, loss = 0.00094277\n",
            "Iteration 9566, loss = 0.00094231\n",
            "Iteration 9567, loss = 0.00094185\n",
            "Iteration 9568, loss = 0.00094140\n",
            "Iteration 9569, loss = 0.00094093\n",
            "Iteration 9570, loss = 0.00094050\n",
            "Iteration 9571, loss = 0.00094002\n",
            "Iteration 9572, loss = 0.00093957\n",
            "Iteration 9573, loss = 0.00093912\n",
            "Iteration 9574, loss = 0.00093866\n",
            "Iteration 9575, loss = 0.00093821\n",
            "Iteration 9576, loss = 0.00093775\n",
            "Iteration 9577, loss = 0.00093729\n",
            "Iteration 9578, loss = 0.00093683\n",
            "Iteration 9579, loss = 0.00093637\n",
            "Iteration 9580, loss = 0.00093593\n",
            "Iteration 9581, loss = 0.00093547\n",
            "Iteration 9582, loss = 0.00093502\n",
            "Iteration 9583, loss = 0.00093457\n",
            "Iteration 9584, loss = 0.00093412\n",
            "Iteration 9585, loss = 0.00093366\n",
            "Iteration 9586, loss = 0.00093320\n",
            "Iteration 9587, loss = 0.00093275\n",
            "Iteration 9588, loss = 0.00093229\n",
            "Iteration 9589, loss = 0.00093184\n",
            "Iteration 9590, loss = 0.00093139\n",
            "Iteration 9591, loss = 0.00093094\n",
            "Iteration 9592, loss = 0.00093049\n",
            "Iteration 9593, loss = 0.00093004\n",
            "Iteration 9594, loss = 0.00092959\n",
            "Iteration 9595, loss = 0.00092914\n",
            "Iteration 9596, loss = 0.00092869\n",
            "Iteration 9597, loss = 0.00092823\n",
            "Iteration 9598, loss = 0.00092777\n",
            "Iteration 9599, loss = 0.00092736\n",
            "Iteration 9600, loss = 0.00092688\n",
            "Iteration 9601, loss = 0.00092643\n",
            "Iteration 9602, loss = 0.00092598\n",
            "Iteration 9603, loss = 0.00092554\n",
            "Iteration 9604, loss = 0.00092509\n",
            "Iteration 9605, loss = 0.00092463\n",
            "Iteration 9606, loss = 0.00092418\n",
            "Iteration 9607, loss = 0.00092373\n",
            "Iteration 9608, loss = 0.00092332\n",
            "Iteration 9609, loss = 0.00092284\n",
            "Iteration 9610, loss = 0.00092239\n",
            "Iteration 9611, loss = 0.00092195\n",
            "Iteration 9612, loss = 0.00092150\n",
            "Iteration 9613, loss = 0.00092106\n",
            "Iteration 9614, loss = 0.00092061\n",
            "Iteration 9615, loss = 0.00092016\n",
            "Iteration 9616, loss = 0.00091971\n",
            "Iteration 9617, loss = 0.00091927\n",
            "Iteration 9618, loss = 0.00091882\n",
            "Iteration 9619, loss = 0.00091838\n",
            "Iteration 9620, loss = 0.00091794\n",
            "Iteration 9621, loss = 0.00091750\n",
            "Iteration 9622, loss = 0.00091705\n",
            "Iteration 9623, loss = 0.00091660\n",
            "Iteration 9624, loss = 0.00091616\n",
            "Iteration 9625, loss = 0.00091571\n",
            "Iteration 9626, loss = 0.00091526\n",
            "Iteration 9627, loss = 0.00091484\n",
            "Iteration 9628, loss = 0.00091438\n",
            "Iteration 9629, loss = 0.00091394\n",
            "Iteration 9630, loss = 0.00091350\n",
            "Iteration 9631, loss = 0.00091306\n",
            "Iteration 9632, loss = 0.00091261\n",
            "Iteration 9633, loss = 0.00091217\n",
            "Iteration 9634, loss = 0.00091172\n",
            "Iteration 9635, loss = 0.00091128\n",
            "Iteration 9636, loss = 0.00091086\n",
            "Iteration 9637, loss = 0.00091040\n",
            "Iteration 9638, loss = 0.00090996\n",
            "Iteration 9639, loss = 0.00090952\n",
            "Iteration 9640, loss = 0.00090909\n",
            "Iteration 9641, loss = 0.00090865\n",
            "Iteration 9642, loss = 0.00090820\n",
            "Iteration 9643, loss = 0.00090776\n",
            "Iteration 9644, loss = 0.00090732\n",
            "Iteration 9645, loss = 0.00090687\n",
            "Iteration 9646, loss = 0.00090648\n",
            "Iteration 9647, loss = 0.00090600\n",
            "Iteration 9648, loss = 0.00090556\n",
            "Iteration 9649, loss = 0.00090513\n",
            "Iteration 9650, loss = 0.00090469\n",
            "Iteration 9651, loss = 0.00090425\n",
            "Iteration 9652, loss = 0.00090381\n",
            "Iteration 9653, loss = 0.00090337\n",
            "Iteration 9654, loss = 0.00090293\n",
            "Iteration 9655, loss = 0.00090250\n",
            "Iteration 9656, loss = 0.00090207\n",
            "Iteration 9657, loss = 0.00090164\n",
            "Iteration 9658, loss = 0.00090120\n",
            "Iteration 9659, loss = 0.00090077\n",
            "Iteration 9660, loss = 0.00090033\n",
            "Iteration 9661, loss = 0.00089989\n",
            "Iteration 9662, loss = 0.00089945\n",
            "Iteration 9663, loss = 0.00089901\n",
            "Iteration 9664, loss = 0.00089857\n",
            "Iteration 9665, loss = 0.00089817\n",
            "Iteration 9666, loss = 0.00089771\n",
            "Iteration 9667, loss = 0.00089727\n",
            "Iteration 9668, loss = 0.00089684\n",
            "Iteration 9669, loss = 0.00089641\n",
            "Iteration 9670, loss = 0.00089597\n",
            "Iteration 9671, loss = 0.00089554\n",
            "Iteration 9672, loss = 0.00089510\n",
            "Iteration 9673, loss = 0.00089469\n",
            "Iteration 9674, loss = 0.00089424\n",
            "Iteration 9675, loss = 0.00089381\n",
            "Iteration 9676, loss = 0.00089339\n",
            "Iteration 9677, loss = 0.00089296\n",
            "Iteration 9678, loss = 0.00089253\n",
            "Iteration 9679, loss = 0.00089209\n",
            "Iteration 9680, loss = 0.00089166\n",
            "Iteration 9681, loss = 0.00089122\n",
            "Iteration 9682, loss = 0.00089079\n",
            "Iteration 9683, loss = 0.00089036\n",
            "Iteration 9684, loss = 0.00088993\n",
            "Iteration 9685, loss = 0.00088951\n",
            "Iteration 9686, loss = 0.00088908\n",
            "Iteration 9687, loss = 0.00088865\n",
            "Iteration 9688, loss = 0.00088822\n",
            "Iteration 9689, loss = 0.00088779\n",
            "Iteration 9690, loss = 0.00088736\n",
            "Iteration 9691, loss = 0.00088692\n",
            "Iteration 9692, loss = 0.00088649\n",
            "Iteration 9693, loss = 0.00088607\n",
            "Iteration 9694, loss = 0.00088565\n",
            "Iteration 9695, loss = 0.00088522\n",
            "Iteration 9696, loss = 0.00088480\n",
            "Iteration 9697, loss = 0.00088437\n",
            "Iteration 9698, loss = 0.00088394\n",
            "Iteration 9699, loss = 0.00088351\n",
            "Iteration 9700, loss = 0.00088308\n",
            "Iteration 9701, loss = 0.00088265\n",
            "Iteration 9702, loss = 0.00088224\n",
            "Iteration 9703, loss = 0.00088180\n",
            "Iteration 9704, loss = 0.00088138\n",
            "Iteration 9705, loss = 0.00088095\n",
            "Iteration 9706, loss = 0.00088053\n",
            "Iteration 9707, loss = 0.00088010\n",
            "Iteration 9708, loss = 0.00087968\n",
            "Iteration 9709, loss = 0.00087925\n",
            "Iteration 9710, loss = 0.00087882\n",
            "Iteration 9711, loss = 0.00087842\n",
            "Iteration 9712, loss = 0.00087797\n",
            "Iteration 9713, loss = 0.00087756\n",
            "Iteration 9714, loss = 0.00087714\n",
            "Iteration 9715, loss = 0.00087671\n",
            "Iteration 9716, loss = 0.00087629\n",
            "Iteration 9717, loss = 0.00087587\n",
            "Iteration 9718, loss = 0.00087544\n",
            "Iteration 9719, loss = 0.00087502\n",
            "Iteration 9720, loss = 0.00087459\n",
            "Iteration 9721, loss = 0.00087422\n",
            "Iteration 9722, loss = 0.00087375\n",
            "Iteration 9723, loss = 0.00087333\n",
            "Iteration 9724, loss = 0.00087291\n",
            "Iteration 9725, loss = 0.00087249\n",
            "Iteration 9726, loss = 0.00087207\n",
            "Iteration 9727, loss = 0.00087165\n",
            "Iteration 9728, loss = 0.00087123\n",
            "Iteration 9729, loss = 0.00087080\n",
            "Iteration 9730, loss = 0.00087039\n",
            "Iteration 9731, loss = 0.00086997\n",
            "Iteration 9732, loss = 0.00086956\n",
            "Iteration 9733, loss = 0.00086914\n",
            "Iteration 9734, loss = 0.00086872\n",
            "Iteration 9735, loss = 0.00086830\n",
            "Iteration 9736, loss = 0.00086788\n",
            "Iteration 9737, loss = 0.00086746\n",
            "Iteration 9738, loss = 0.00086704\n",
            "Iteration 9739, loss = 0.00086661\n",
            "Iteration 9740, loss = 0.00086623\n",
            "Iteration 9741, loss = 0.00086578\n",
            "Iteration 9742, loss = 0.00086537\n",
            "Iteration 9743, loss = 0.00086495\n",
            "Iteration 9744, loss = 0.00086454\n",
            "Iteration 9745, loss = 0.00086412\n",
            "Iteration 9746, loss = 0.00086370\n",
            "Iteration 9747, loss = 0.00086328\n",
            "Iteration 9748, loss = 0.00086288\n",
            "Iteration 9749, loss = 0.00086245\n",
            "Iteration 9750, loss = 0.00086204\n",
            "Iteration 9751, loss = 0.00086163\n",
            "Iteration 9752, loss = 0.00086122\n",
            "Iteration 9753, loss = 0.00086080\n",
            "Iteration 9754, loss = 0.00086039\n",
            "Iteration 9755, loss = 0.00085997\n",
            "Iteration 9756, loss = 0.00085955\n",
            "Iteration 9757, loss = 0.00085914\n",
            "Iteration 9758, loss = 0.00085872\n",
            "Iteration 9759, loss = 0.00085831\n",
            "Iteration 9760, loss = 0.00085790\n",
            "Iteration 9761, loss = 0.00085749\n",
            "Iteration 9762, loss = 0.00085708\n",
            "Iteration 9763, loss = 0.00085667\n",
            "Iteration 9764, loss = 0.00085625\n",
            "Iteration 9765, loss = 0.00085584\n",
            "Iteration 9766, loss = 0.00085542\n",
            "Iteration 9767, loss = 0.00085501\n",
            "Iteration 9768, loss = 0.00085464\n",
            "Iteration 9769, loss = 0.00085419\n",
            "Iteration 9770, loss = 0.00085378\n",
            "Iteration 9771, loss = 0.00085337\n",
            "Iteration 9772, loss = 0.00085296\n",
            "Iteration 9773, loss = 0.00085255\n",
            "Iteration 9774, loss = 0.00085214\n",
            "Iteration 9775, loss = 0.00085173\n",
            "Iteration 9776, loss = 0.00085132\n",
            "Iteration 9777, loss = 0.00085091\n",
            "Iteration 9778, loss = 0.00085050\n",
            "Iteration 9779, loss = 0.00085010\n",
            "Iteration 9780, loss = 0.00084969\n",
            "Iteration 9781, loss = 0.00084928\n",
            "Iteration 9782, loss = 0.00084887\n",
            "Iteration 9783, loss = 0.00084846\n",
            "Iteration 9784, loss = 0.00084805\n",
            "Iteration 9785, loss = 0.00084764\n",
            "Iteration 9786, loss = 0.00084723\n",
            "Iteration 9787, loss = 0.00084686\n",
            "Iteration 9788, loss = 0.00084641\n",
            "Iteration 9789, loss = 0.00084601\n",
            "Iteration 9790, loss = 0.00084561\n",
            "Iteration 9791, loss = 0.00084520\n",
            "Iteration 9792, loss = 0.00084479\n",
            "Iteration 9793, loss = 0.00084439\n",
            "Iteration 9794, loss = 0.00084398\n",
            "Iteration 9795, loss = 0.00084359\n",
            "Iteration 9796, loss = 0.00084317\n",
            "Iteration 9797, loss = 0.00084277\n",
            "Iteration 9798, loss = 0.00084237\n",
            "Iteration 9799, loss = 0.00084196\n",
            "Iteration 9800, loss = 0.00084156\n",
            "Iteration 9801, loss = 0.00084115\n",
            "Iteration 9802, loss = 0.00084075\n",
            "Iteration 9803, loss = 0.00084034\n",
            "Iteration 9804, loss = 0.00083993\n",
            "Iteration 9805, loss = 0.00083953\n",
            "Iteration 9806, loss = 0.00083913\n",
            "Iteration 9807, loss = 0.00083873\n",
            "Iteration 9808, loss = 0.00083833\n",
            "Iteration 9809, loss = 0.00083793\n",
            "Iteration 9810, loss = 0.00083753\n",
            "Iteration 9811, loss = 0.00083712\n",
            "Iteration 9812, loss = 0.00083672\n",
            "Iteration 9813, loss = 0.00083631\n",
            "Iteration 9814, loss = 0.00083591\n",
            "Iteration 9815, loss = 0.00083555\n",
            "Iteration 9816, loss = 0.00083511\n",
            "Iteration 9817, loss = 0.00083471\n",
            "Iteration 9818, loss = 0.00083431\n",
            "Iteration 9819, loss = 0.00083391\n",
            "Iteration 9820, loss = 0.00083351\n",
            "Iteration 9821, loss = 0.00083311\n",
            "Iteration 9822, loss = 0.00083271\n",
            "Iteration 9823, loss = 0.00083231\n",
            "Iteration 9824, loss = 0.00083191\n",
            "Iteration 9825, loss = 0.00083151\n",
            "Iteration 9826, loss = 0.00083111\n",
            "Iteration 9827, loss = 0.00083071\n",
            "Iteration 9828, loss = 0.00083031\n",
            "Iteration 9829, loss = 0.00082991\n",
            "Iteration 9830, loss = 0.00082954\n",
            "Iteration 9831, loss = 0.00082912\n",
            "Iteration 9832, loss = 0.00082872\n",
            "Iteration 9833, loss = 0.00082833\n",
            "Iteration 9834, loss = 0.00082793\n",
            "Iteration 9835, loss = 0.00082753\n",
            "Iteration 9836, loss = 0.00082714\n",
            "Iteration 9837, loss = 0.00082674\n",
            "Iteration 9838, loss = 0.00082634\n",
            "Iteration 9839, loss = 0.00082595\n",
            "Iteration 9840, loss = 0.00082555\n",
            "Iteration 9841, loss = 0.00082516\n",
            "Iteration 9842, loss = 0.00082476\n",
            "Iteration 9843, loss = 0.00082437\n",
            "Iteration 9844, loss = 0.00082398\n",
            "Iteration 9845, loss = 0.00082358\n",
            "Iteration 9846, loss = 0.00082318\n",
            "Iteration 9847, loss = 0.00082278\n",
            "Iteration 9848, loss = 0.00082239\n",
            "Iteration 9849, loss = 0.00082200\n",
            "Iteration 9850, loss = 0.00082160\n",
            "Iteration 9851, loss = 0.00082121\n",
            "Iteration 9852, loss = 0.00082082\n",
            "Iteration 9853, loss = 0.00082043\n",
            "Iteration 9854, loss = 0.00082003\n",
            "Iteration 9855, loss = 0.00081964\n",
            "Iteration 9856, loss = 0.00081925\n",
            "Iteration 9857, loss = 0.00081885\n",
            "Iteration 9858, loss = 0.00081845\n",
            "Iteration 9859, loss = 0.00081811\n",
            "Iteration 9860, loss = 0.00081767\n",
            "Iteration 9861, loss = 0.00081729\n",
            "Iteration 9862, loss = 0.00081689\n",
            "Iteration 9863, loss = 0.00081651\n",
            "Iteration 9864, loss = 0.00081611\n",
            "Iteration 9865, loss = 0.00081572\n",
            "Iteration 9866, loss = 0.00081533\n",
            "Iteration 9867, loss = 0.00081493\n",
            "Iteration 9868, loss = 0.00081458\n",
            "Iteration 9869, loss = 0.00081415\n",
            "Iteration 9870, loss = 0.00081377\n",
            "Iteration 9871, loss = 0.00081338\n",
            "Iteration 9872, loss = 0.00081299\n",
            "Iteration 9873, loss = 0.00081261\n",
            "Iteration 9874, loss = 0.00081221\n",
            "Iteration 9875, loss = 0.00081182\n",
            "Iteration 9876, loss = 0.00081143\n",
            "Iteration 9877, loss = 0.00081104\n",
            "Iteration 9878, loss = 0.00081069\n",
            "Iteration 9879, loss = 0.00081027\n",
            "Iteration 9880, loss = 0.00080988\n",
            "Iteration 9881, loss = 0.00080950\n",
            "Iteration 9882, loss = 0.00080911\n",
            "Iteration 9883, loss = 0.00080872\n",
            "Iteration 9884, loss = 0.00080833\n",
            "Iteration 9885, loss = 0.00080795\n",
            "Iteration 9886, loss = 0.00080756\n",
            "Iteration 9887, loss = 0.00080718\n",
            "Iteration 9888, loss = 0.00080679\n",
            "Iteration 9889, loss = 0.00080641\n",
            "Iteration 9890, loss = 0.00080602\n",
            "Iteration 9891, loss = 0.00080564\n",
            "Iteration 9892, loss = 0.00080525\n",
            "Iteration 9893, loss = 0.00080487\n",
            "Iteration 9894, loss = 0.00080448\n",
            "Iteration 9895, loss = 0.00080409\n",
            "Iteration 9896, loss = 0.00080370\n",
            "Iteration 9897, loss = 0.00080333\n",
            "Iteration 9898, loss = 0.00080294\n",
            "Iteration 9899, loss = 0.00080256\n",
            "Iteration 9900, loss = 0.00080218\n",
            "Iteration 9901, loss = 0.00080180\n",
            "Iteration 9902, loss = 0.00080141\n",
            "Iteration 9903, loss = 0.00080103\n",
            "Iteration 9904, loss = 0.00080064\n",
            "Iteration 9905, loss = 0.00080025\n",
            "Iteration 9906, loss = 0.00079987\n",
            "Iteration 9907, loss = 0.00079949\n",
            "Iteration 9908, loss = 0.00079911\n",
            "Iteration 9909, loss = 0.00079874\n",
            "Iteration 9910, loss = 0.00079836\n",
            "Iteration 9911, loss = 0.00079797\n",
            "Iteration 9912, loss = 0.00079759\n",
            "Iteration 9913, loss = 0.00079721\n",
            "Iteration 9914, loss = 0.00079682\n",
            "Iteration 9915, loss = 0.00079644\n",
            "Iteration 9916, loss = 0.00079606\n",
            "Iteration 9917, loss = 0.00079568\n",
            "Iteration 9918, loss = 0.00079531\n",
            "Iteration 9919, loss = 0.00079493\n",
            "Iteration 9920, loss = 0.00079455\n",
            "Iteration 9921, loss = 0.00079417\n",
            "Iteration 9922, loss = 0.00079379\n",
            "Iteration 9923, loss = 0.00079341\n",
            "Iteration 9924, loss = 0.00079302\n",
            "Iteration 9925, loss = 0.00079264\n",
            "Iteration 9926, loss = 0.00079231\n",
            "Iteration 9927, loss = 0.00079189\n",
            "Iteration 9928, loss = 0.00079151\n",
            "Iteration 9929, loss = 0.00079114\n",
            "Iteration 9930, loss = 0.00079076\n",
            "Iteration 9931, loss = 0.00079038\n",
            "Iteration 9932, loss = 0.00079000\n",
            "Iteration 9933, loss = 0.00078963\n",
            "Iteration 9934, loss = 0.00078924\n",
            "Iteration 9935, loss = 0.00078891\n",
            "Iteration 9936, loss = 0.00078849\n",
            "Iteration 9937, loss = 0.00078812\n",
            "Iteration 9938, loss = 0.00078775\n",
            "Iteration 9939, loss = 0.00078737\n",
            "Iteration 9940, loss = 0.00078700\n",
            "Iteration 9941, loss = 0.00078662\n",
            "Iteration 9942, loss = 0.00078624\n",
            "Iteration 9943, loss = 0.00078586\n",
            "Iteration 9944, loss = 0.00078549\n",
            "Iteration 9945, loss = 0.00078515\n",
            "Iteration 9946, loss = 0.00078474\n",
            "Iteration 9947, loss = 0.00078437\n",
            "Iteration 9948, loss = 0.00078400\n",
            "Iteration 9949, loss = 0.00078363\n",
            "Iteration 9950, loss = 0.00078325\n",
            "Iteration 9951, loss = 0.00078287\n",
            "Iteration 9952, loss = 0.00078250\n",
            "Iteration 9953, loss = 0.00078212\n",
            "Iteration 9954, loss = 0.00078178\n",
            "Iteration 9955, loss = 0.00078138\n",
            "Iteration 9956, loss = 0.00078101\n",
            "Iteration 9957, loss = 0.00078064\n",
            "Iteration 9958, loss = 0.00078027\n",
            "Iteration 9959, loss = 0.00077990\n",
            "Iteration 9960, loss = 0.00077953\n",
            "Iteration 9961, loss = 0.00077915\n",
            "Iteration 9962, loss = 0.00077878\n",
            "Iteration 9963, loss = 0.00077840\n",
            "Iteration 9964, loss = 0.00077806\n",
            "Iteration 9965, loss = 0.00077766\n",
            "Iteration 9966, loss = 0.00077730\n",
            "Iteration 9967, loss = 0.00077693\n",
            "Iteration 9968, loss = 0.00077656\n",
            "Iteration 9969, loss = 0.00077619\n",
            "Iteration 9970, loss = 0.00077582\n",
            "Iteration 9971, loss = 0.00077545\n",
            "Iteration 9972, loss = 0.00077507\n",
            "Iteration 9973, loss = 0.00077472\n",
            "Iteration 9974, loss = 0.00077434\n",
            "Iteration 9975, loss = 0.00077398\n",
            "Iteration 9976, loss = 0.00077361\n",
            "Iteration 9977, loss = 0.00077324\n",
            "Iteration 9978, loss = 0.00077287\n",
            "Iteration 9979, loss = 0.00077250\n",
            "Iteration 9980, loss = 0.00077213\n",
            "Iteration 9981, loss = 0.00077176\n",
            "Iteration 9982, loss = 0.00077139\n",
            "Iteration 9983, loss = 0.00077105\n",
            "Iteration 9984, loss = 0.00077066\n",
            "Iteration 9985, loss = 0.00077030\n",
            "Iteration 9986, loss = 0.00076993\n",
            "Iteration 9987, loss = 0.00076957\n",
            "Iteration 9988, loss = 0.00076920\n",
            "Iteration 9989, loss = 0.00076883\n",
            "Iteration 9990, loss = 0.00076847\n",
            "Iteration 9991, loss = 0.00076810\n",
            "Iteration 9992, loss = 0.00076774\n",
            "Iteration 9993, loss = 0.00076737\n",
            "Iteration 9994, loss = 0.00076701\n",
            "Iteration 9995, loss = 0.00076664\n",
            "Iteration 9996, loss = 0.00076628\n",
            "Iteration 9997, loss = 0.00076592\n",
            "Iteration 9998, loss = 0.00076555\n",
            "Iteration 9999, loss = 0.00076519\n",
            "Iteration 10000, loss = 0.00076482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(5, 3), max_iter=10000, tol=1e-09, verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ml.score(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65IqXxD7EuF8",
        "outputId": "f0965aac-dff4-4d90-81e6-75dcadcffc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBuMabC-U9i3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0061c661-d424-4277-f9ca-ba0780e19a3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "ml.score(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tzbmjb5U9i4",
        "outputId": "ded21153-01b5-4165-af04-a5f0e0d18a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.80237048\n",
            "Iteration 2, loss = 1.66786596\n",
            "Iteration 3, loss = 1.55666274\n",
            "Iteration 4, loss = 1.46604989\n",
            "Iteration 5, loss = 1.39131958\n",
            "Iteration 6, loss = 1.32715496\n",
            "Iteration 7, loss = 1.27002055\n",
            "Iteration 8, loss = 1.21871823\n",
            "Iteration 9, loss = 1.17342234\n",
            "Iteration 10, loss = 1.13464586\n",
            "Iteration 11, loss = 1.10260390\n",
            "Iteration 12, loss = 1.07695133\n",
            "Iteration 13, loss = 1.05680268\n",
            "Iteration 14, loss = 1.04091415\n",
            "Iteration 15, loss = 1.02784551\n",
            "Iteration 16, loss = 1.01625291\n",
            "Iteration 17, loss = 1.00504564\n",
            "Iteration 18, loss = 0.99339621\n",
            "Iteration 19, loss = 0.97992995\n",
            "Iteration 20, loss = 0.96349104\n",
            "Iteration 21, loss = 0.94353854\n",
            "Iteration 22, loss = 0.92065293\n",
            "Iteration 23, loss = 0.89580031\n",
            "Iteration 24, loss = 0.86980921\n",
            "Iteration 25, loss = 0.84556599\n",
            "Iteration 26, loss = 0.82785568\n",
            "Iteration 27, loss = 0.81107518\n",
            "Iteration 28, loss = 0.79366546\n",
            "Iteration 29, loss = 0.77550609\n",
            "Iteration 30, loss = 0.75703523\n",
            "Iteration 31, loss = 0.73857563\n",
            "Iteration 32, loss = 0.72083855\n",
            "Iteration 33, loss = 0.70474284\n",
            "Iteration 34, loss = 0.68962150\n",
            "Iteration 35, loss = 0.67555078\n",
            "Iteration 36, loss = 0.66227940\n",
            "Iteration 37, loss = 0.64897464\n",
            "Iteration 38, loss = 0.63516811\n",
            "Iteration 39, loss = 0.62091968\n",
            "Iteration 40, loss = 0.60643802\n",
            "Iteration 41, loss = 0.59219223\n",
            "Iteration 42, loss = 0.57865751\n",
            "Iteration 43, loss = 0.56613749\n",
            "Iteration 44, loss = 0.55481365\n",
            "Iteration 45, loss = 0.54405525\n",
            "Iteration 46, loss = 0.53360616\n",
            "Iteration 47, loss = 0.52254745\n",
            "Iteration 48, loss = 0.51082727\n",
            "Iteration 49, loss = 0.49929753\n",
            "Iteration 50, loss = 0.48809207\n",
            "Iteration 51, loss = 0.47765038\n",
            "Iteration 52, loss = 0.46783622\n",
            "Iteration 53, loss = 0.45367294\n",
            "Iteration 54, loss = 0.44334385\n",
            "Iteration 55, loss = 0.43866368\n",
            "Iteration 56, loss = 0.43374665\n",
            "Iteration 57, loss = 0.42771546\n",
            "Iteration 58, loss = 0.42055187\n",
            "Iteration 59, loss = 0.41271536\n",
            "Iteration 60, loss = 0.40497549\n",
            "Iteration 61, loss = 0.39788019\n",
            "Iteration 62, loss = 0.39174334\n",
            "Iteration 63, loss = 0.38625728\n",
            "Iteration 64, loss = 0.38083021\n",
            "Iteration 65, loss = 0.37496853\n",
            "Iteration 66, loss = 0.36862005\n",
            "Iteration 67, loss = 0.36205855\n",
            "Iteration 68, loss = 0.35577640\n",
            "Iteration 69, loss = 0.35003248\n",
            "Iteration 70, loss = 0.34463734\n",
            "Iteration 71, loss = 0.33910125\n",
            "Iteration 72, loss = 0.33310896\n",
            "Iteration 73, loss = 0.32684491\n",
            "Iteration 74, loss = 0.32074634\n",
            "Iteration 75, loss = 0.31507763\n",
            "Iteration 76, loss = 0.30963201\n",
            "Iteration 77, loss = 0.30407721\n",
            "Iteration 78, loss = 0.29805425\n",
            "Iteration 79, loss = 0.29211846\n",
            "Iteration 80, loss = 0.28653860\n",
            "Iteration 81, loss = 0.28116494\n",
            "Iteration 82, loss = 0.27563705\n",
            "Iteration 83, loss = 0.26989209\n",
            "Iteration 84, loss = 0.26424205\n",
            "Iteration 85, loss = 0.25891018\n",
            "Iteration 86, loss = 0.25364178\n",
            "Iteration 87, loss = 0.24820899\n",
            "Iteration 88, loss = 0.24284981\n",
            "Iteration 89, loss = 0.23774337\n",
            "Iteration 90, loss = 0.23272974\n",
            "Iteration 91, loss = 0.22766398\n",
            "Iteration 92, loss = 0.22263732\n",
            "Iteration 93, loss = 0.21780849\n",
            "Iteration 94, loss = 0.21313406\n",
            "Iteration 95, loss = 0.20843825\n",
            "Iteration 96, loss = 0.20382497\n",
            "Iteration 97, loss = 0.19941325\n",
            "Iteration 98, loss = 0.19509442\n",
            "Iteration 99, loss = 0.19080807\n",
            "Iteration 100, loss = 0.18664011\n",
            "Iteration 101, loss = 0.18263915\n",
            "Iteration 102, loss = 0.17871499\n",
            "Iteration 103, loss = 0.17485847\n",
            "Iteration 104, loss = 0.17114937\n",
            "Iteration 105, loss = 0.16756572\n",
            "Iteration 106, loss = 0.16404664\n",
            "Iteration 107, loss = 0.16062153\n",
            "Iteration 108, loss = 0.15733210\n",
            "Iteration 109, loss = 0.15413773\n",
            "Iteration 110, loss = 0.15101877\n",
            "Iteration 111, loss = 0.14800254\n",
            "Iteration 112, loss = 0.14509685\n",
            "Iteration 113, loss = 0.14227312\n",
            "Iteration 114, loss = 0.13952561\n",
            "Iteration 115, loss = 0.13687446\n",
            "Iteration 116, loss = 0.13431618\n",
            "Iteration 117, loss = 0.13183008\n",
            "Iteration 118, loss = 0.12941894\n",
            "Iteration 119, loss = 0.12709356\n",
            "Iteration 120, loss = 0.12484340\n",
            "Iteration 121, loss = 0.12265728\n",
            "Iteration 122, loss = 0.12054627\n",
            "Iteration 123, loss = 0.11850645\n",
            "Iteration 124, loss = 0.11652710\n",
            "Iteration 125, loss = 0.11460863\n",
            "Iteration 126, loss = 0.11275464\n",
            "Iteration 127, loss = 0.11095900\n",
            "Iteration 128, loss = 0.10921633\n",
            "Iteration 129, loss = 0.10753239\n",
            "Iteration 130, loss = 0.10590270\n",
            "Iteration 131, loss = 0.10432004\n",
            "Iteration 132, loss = 0.10278487\n",
            "Iteration 133, loss = 0.10129786\n",
            "Iteration 134, loss = 0.09985438\n",
            "Iteration 135, loss = 0.09845193\n",
            "Iteration 136, loss = 0.09709124\n",
            "Iteration 137, loss = 0.09577066\n",
            "Iteration 138, loss = 0.09448686\n",
            "Iteration 139, loss = 0.09324053\n",
            "Iteration 140, loss = 0.09203075\n",
            "Iteration 141, loss = 0.09085465\n",
            "Iteration 142, loss = 0.08971020\n",
            "Iteration 143, loss = 0.08859722\n",
            "Iteration 144, loss = 0.08751655\n",
            "Iteration 145, loss = 0.08646377\n",
            "Iteration 146, loss = 0.08543924\n",
            "Iteration 147, loss = 0.08444266\n",
            "Iteration 148, loss = 0.08347115\n",
            "Iteration 149, loss = 0.08252393\n",
            "Iteration 150, loss = 0.08160144\n",
            "Iteration 151, loss = 0.08070218\n",
            "Iteration 152, loss = 0.07982445\n",
            "Iteration 153, loss = 0.07896828\n",
            "Iteration 154, loss = 0.07813330\n",
            "Iteration 155, loss = 0.07731787\n",
            "Iteration 156, loss = 0.07652125\n",
            "Iteration 157, loss = 0.07574457\n",
            "Iteration 158, loss = 0.07498650\n",
            "Iteration 159, loss = 0.07424544\n",
            "Iteration 160, loss = 0.07352179\n",
            "Iteration 161, loss = 0.07281406\n",
            "Iteration 162, loss = 0.07212126\n",
            "Iteration 163, loss = 0.07144381\n",
            "Iteration 164, loss = 0.07078113\n",
            "Iteration 165, loss = 0.07013207\n",
            "Iteration 166, loss = 0.06949642\n",
            "Iteration 167, loss = 0.06887420\n",
            "Iteration 168, loss = 0.06826454\n",
            "Iteration 169, loss = 0.06766683\n",
            "Iteration 170, loss = 0.06708113\n",
            "Iteration 171, loss = 0.06650704\n",
            "Iteration 172, loss = 0.06594387\n",
            "Iteration 173, loss = 0.06539140\n",
            "Iteration 174, loss = 0.06484955\n",
            "Iteration 175, loss = 0.06431775\n",
            "Iteration 176, loss = 0.06379561\n",
            "Iteration 177, loss = 0.06328308\n",
            "Iteration 178, loss = 0.06277981\n",
            "Iteration 179, loss = 0.06228541\n",
            "Iteration 180, loss = 0.06179969\n",
            "Iteration 181, loss = 0.06132250\n",
            "Iteration 182, loss = 0.06085346\n",
            "Iteration 183, loss = 0.06039233\n",
            "Iteration 184, loss = 0.05993899\n",
            "Iteration 185, loss = 0.05949321\n",
            "Iteration 186, loss = 0.05905470\n",
            "Iteration 187, loss = 0.05862333\n",
            "Iteration 188, loss = 0.05819893\n",
            "Iteration 189, loss = 0.05778127\n",
            "Iteration 190, loss = 0.05737026\n",
            "Iteration 191, loss = 0.05696575\n",
            "Iteration 192, loss = 0.05656747\n",
            "Iteration 193, loss = 0.05617530\n",
            "Iteration 194, loss = 0.05578906\n",
            "Iteration 195, loss = 0.05540860\n",
            "Iteration 196, loss = 0.05503379\n",
            "Iteration 197, loss = 0.05466449\n",
            "Iteration 198, loss = 0.05430056\n",
            "Iteration 199, loss = 0.05394186\n",
            "Iteration 200, loss = 0.05358830\n",
            "Iteration 201, loss = 0.05323973\n",
            "Iteration 202, loss = 0.05289603\n",
            "Iteration 203, loss = 0.05255711\n",
            "Iteration 204, loss = 0.05222285\n",
            "Iteration 205, loss = 0.05189314\n",
            "Iteration 206, loss = 0.05156789\n",
            "Iteration 207, loss = 0.05124699\n",
            "Iteration 208, loss = 0.05093035\n",
            "Iteration 209, loss = 0.05061787\n",
            "Iteration 210, loss = 0.05030957\n",
            "Iteration 211, loss = 0.05000542\n",
            "Iteration 212, loss = 0.04970519\n",
            "Iteration 213, loss = 0.04940881\n",
            "Iteration 214, loss = 0.04911616\n",
            "Iteration 215, loss = 0.04882718\n",
            "Iteration 216, loss = 0.04854181\n",
            "Iteration 217, loss = 0.04825996\n",
            "Iteration 218, loss = 0.04798155\n",
            "Iteration 219, loss = 0.04770653\n",
            "Iteration 220, loss = 0.04743482\n",
            "Iteration 221, loss = 0.04716636\n",
            "Iteration 222, loss = 0.04690108\n",
            "Iteration 223, loss = 0.04663893\n",
            "Iteration 224, loss = 0.04638341\n",
            "Iteration 225, loss = 0.04613068\n",
            "Iteration 226, loss = 0.04588028\n",
            "Iteration 227, loss = 0.04563225\n",
            "Iteration 228, loss = 0.04538660\n",
            "Iteration 229, loss = 0.04514337\n",
            "Iteration 230, loss = 0.04490259\n",
            "Iteration 231, loss = 0.04466431\n",
            "Iteration 232, loss = 0.04442849\n",
            "Iteration 233, loss = 0.04419511\n",
            "Iteration 234, loss = 0.04396638\n",
            "Iteration 235, loss = 0.04374100\n",
            "Iteration 236, loss = 0.04351788\n",
            "Iteration 237, loss = 0.04329701\n",
            "Iteration 238, loss = 0.04307835\n",
            "Iteration 239, loss = 0.04286223\n",
            "Iteration 240, loss = 0.04264826\n",
            "Iteration 241, loss = 0.04243633\n",
            "Iteration 242, loss = 0.04222643\n",
            "Iteration 243, loss = 0.04201887\n",
            "Iteration 244, loss = 0.04181514\n",
            "Iteration 245, loss = 0.04161314\n",
            "Iteration 246, loss = 0.04141295\n",
            "Iteration 247, loss = 0.04121456\n",
            "Iteration 248, loss = 0.04101795\n",
            "Iteration 249, loss = 0.04082394\n",
            "Iteration 250, loss = 0.04063236\n",
            "Iteration 251, loss = 0.04044237\n",
            "Iteration 252, loss = 0.04025396\n",
            "Iteration 253, loss = 0.04006716\n",
            "Iteration 254, loss = 0.03988357\n",
            "Iteration 255, loss = 0.03970150\n",
            "Iteration 256, loss = 0.03952093\n",
            "Iteration 257, loss = 0.03934187\n",
            "Iteration 258, loss = 0.03916431\n",
            "Iteration 259, loss = 0.03898828\n",
            "Iteration 260, loss = 0.03881548\n",
            "Iteration 261, loss = 0.03864402\n",
            "Iteration 262, loss = 0.03847380\n",
            "Iteration 263, loss = 0.03830481\n",
            "Iteration 264, loss = 0.03813707\n",
            "Iteration 265, loss = 0.03797124\n",
            "Iteration 266, loss = 0.03780777\n",
            "Iteration 267, loss = 0.03764555\n",
            "Iteration 268, loss = 0.03748459\n",
            "Iteration 269, loss = 0.03732489\n",
            "Iteration 270, loss = 0.03716658\n",
            "Iteration 271, loss = 0.03701037\n",
            "Iteration 272, loss = 0.03685515\n",
            "Iteration 273, loss = 0.03670197\n",
            "Iteration 274, loss = 0.03654998\n",
            "Iteration 275, loss = 0.03639913\n",
            "Iteration 276, loss = 0.03624942\n",
            "Iteration 277, loss = 0.03610130\n",
            "Iteration 278, loss = 0.03595468\n",
            "Iteration 279, loss = 0.03580889\n",
            "Iteration 280, loss = 0.03566509\n",
            "Iteration 281, loss = 0.03552244\n",
            "Iteration 282, loss = 0.03538080\n",
            "Iteration 283, loss = 0.03524018\n",
            "Iteration 284, loss = 0.03510060\n",
            "Iteration 285, loss = 0.03496318\n",
            "Iteration 286, loss = 0.03482668\n",
            "Iteration 287, loss = 0.03469081\n",
            "Iteration 288, loss = 0.03455564\n",
            "Iteration 289, loss = 0.03442254\n",
            "Iteration 290, loss = 0.03429036\n",
            "Iteration 291, loss = 0.03415909\n",
            "Iteration 292, loss = 0.03402970\n",
            "Iteration 293, loss = 0.03390085\n",
            "Iteration 294, loss = 0.03377255\n",
            "Iteration 295, loss = 0.03364628\n",
            "Iteration 296, loss = 0.03352098\n",
            "Iteration 297, loss = 0.03339650\n",
            "Iteration 298, loss = 0.03327284\n",
            "Iteration 299, loss = 0.03315001\n",
            "Iteration 300, loss = 0.03302819\n",
            "Iteration 301, loss = 0.03290803\n",
            "Iteration 302, loss = 0.03278893\n",
            "Iteration 303, loss = 0.03267020\n",
            "Iteration 304, loss = 0.03255227\n",
            "Iteration 305, loss = 0.03243595\n",
            "Iteration 306, loss = 0.03232038\n",
            "Iteration 307, loss = 0.03220551\n",
            "Iteration 308, loss = 0.03209140\n",
            "Iteration 309, loss = 0.03197916\n",
            "Iteration 310, loss = 0.03186723\n",
            "Iteration 311, loss = 0.03175554\n",
            "Iteration 312, loss = 0.03164530\n",
            "Iteration 313, loss = 0.03153617\n",
            "Iteration 314, loss = 0.03142768\n",
            "Iteration 315, loss = 0.03131986\n",
            "Iteration 316, loss = 0.03121270\n",
            "Iteration 317, loss = 0.03110621\n",
            "Iteration 318, loss = 0.03100089\n",
            "Iteration 319, loss = 0.03089651\n",
            "Iteration 320, loss = 0.03079224\n",
            "Iteration 321, loss = 0.03068974\n",
            "Iteration 322, loss = 0.03058782\n",
            "Iteration 323, loss = 0.03048646\n",
            "Iteration 324, loss = 0.03038570\n",
            "Iteration 325, loss = 0.03028553\n",
            "Iteration 326, loss = 0.03018596\n",
            "Iteration 327, loss = 0.03008700\n",
            "Iteration 328, loss = 0.02999005\n",
            "Iteration 329, loss = 0.02989324\n",
            "Iteration 330, loss = 0.02979641\n",
            "Iteration 331, loss = 0.02969988\n",
            "Iteration 332, loss = 0.02960510\n",
            "Iteration 333, loss = 0.02951084\n",
            "Iteration 334, loss = 0.02941709\n",
            "Iteration 335, loss = 0.02932388\n",
            "Iteration 336, loss = 0.02923120\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ],
      "source": [
        "network=MLPClassifier(verbose=1,max_iter=1000)\n",
        "#max_=[5,10,100]\n",
        "learning_rate_init=[0.1,0.01,0.2]\n",
        "hidden_layer_sizes=[(3,),(5,),(7,),(9,)]\n",
        "solver=[\"sgd\",\"adam\"]\n",
        "\n",
        "\n",
        "param_grid = dict(learning_rate_init=learning_rate_init,hidden_layer_sizes=hidden_layer_sizes,solver=solver)\n",
        "grid = GridSearchCV(estimator=network, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train,Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QpyK-k1FU9i5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4713e7f7-6420-480c-ffc6-b66c54a3e7df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(5,), learning_rate_init=0.01, max_iter=1000,\n",
              "              verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "grid_result.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_result.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsbjrLFuhiGA",
        "outputId": "b215a322-e058-42dc-fe3e-cee2523c4425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9916666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_result.cv_results_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4plOFFrD9Wg",
        "outputId": "b61565e4-3123-47b2-c28d-987607044eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.05943259, 0.1896166 , 0.14155459, 0.16711958, 0.04124745,\n",
              "        0.14867234, 0.11215655, 0.2164611 , 0.20682303, 0.21327949,\n",
              "        0.02227736, 0.10932819, 0.08691398, 0.17530163, 0.17852012,\n",
              "        0.18371336, 0.02349909, 0.16446018, 0.05406944, 0.15887928,\n",
              "        0.18743491, 0.21821149, 0.03225795, 0.12280965]),\n",
              " 'mean_score_time': array([0.00321213, 0.0032924 , 0.00820867, 0.00464598, 0.00400662,\n",
              "        0.00566824, 0.00941531, 0.00394646, 0.00323955, 0.00298246,\n",
              "        0.00335312, 0.00320093, 0.00405439, 0.00714382, 0.00601896,\n",
              "        0.00421302, 0.00310381, 0.00325783, 0.00324718, 0.00567913,\n",
              "        0.00568922, 0.00399415, 0.00579309, 0.00485635]),\n",
              " 'mean_test_score': array([0.55      , 0.98333333, 0.56666667, 0.65833333, 0.35833333,\n",
              "        0.775     , 0.91666667, 0.98333333, 0.88333333, 0.96666667,\n",
              "        0.35833333, 0.775     , 0.76666667, 0.98333333, 0.90833333,\n",
              "        1.        , 0.35      , 0.975     , 0.8       , 0.98333333,\n",
              "        0.98333333, 0.98333333, 0.33333333, 0.98333333]),\n",
              " 'param_hidden_layer_sizes': masked_array(data=[(3,), (3,), (3,), (3,), (3,), (3,), (5,), (5,), (5,),\n",
              "                    (5,), (5,), (5,), (7,), (7,), (7,), (7,), (7,), (7,),\n",
              "                    (9,), (9,), (9,), (9,), (9,), (9,)],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_learning_rate_init': masked_array(data=[0.1, 0.1, 0.01, 0.01, 0.2, 0.2, 0.1, 0.1, 0.01, 0.01,\n",
              "                    0.2, 0.2, 0.1, 0.1, 0.01, 0.01, 0.2, 0.2, 0.1, 0.1,\n",
              "                    0.01, 0.01, 0.2, 0.2],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_solver': masked_array(data=['sgd', 'adam', 'sgd', 'adam', 'sgd', 'adam', 'sgd',\n",
              "                    'adam', 'sgd', 'adam', 'sgd', 'adam', 'sgd', 'adam',\n",
              "                    'sgd', 'adam', 'sgd', 'adam', 'sgd', 'adam', 'sgd',\n",
              "                    'adam', 'sgd', 'adam'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'hidden_layer_sizes': (3,),\n",
              "   'learning_rate_init': 0.1,\n",
              "   'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (3,), 'learning_rate_init': 0.1, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (3,), 'learning_rate_init': 0.01, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (3,), 'learning_rate_init': 0.01, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (3,), 'learning_rate_init': 0.2, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (3,), 'learning_rate_init': 0.2, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (5,), 'learning_rate_init': 0.1, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (5,), 'learning_rate_init': 0.1, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (5,), 'learning_rate_init': 0.2, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (5,), 'learning_rate_init': 0.2, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (7,), 'learning_rate_init': 0.1, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (7,), 'learning_rate_init': 0.1, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (7,), 'learning_rate_init': 0.01, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (7,), 'learning_rate_init': 0.01, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (7,), 'learning_rate_init': 0.2, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (7,), 'learning_rate_init': 0.2, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (9,), 'learning_rate_init': 0.1, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (9,), 'learning_rate_init': 0.1, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (9,), 'learning_rate_init': 0.01, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (9,), 'learning_rate_init': 0.01, 'solver': 'adam'},\n",
              "  {'hidden_layer_sizes': (9,), 'learning_rate_init': 0.2, 'solver': 'sgd'},\n",
              "  {'hidden_layer_sizes': (9,), 'learning_rate_init': 0.2, 'solver': 'adam'}],\n",
              " 'rank_test_score': array([20,  2, 19, 18, 21, 15, 11,  2, 13, 10, 21, 15, 17,  2, 12,  1, 23,\n",
              "         9, 14,  2,  2,  2, 24,  2], dtype=int32),\n",
              " 'split0_test_score': array([0.3  , 0.975, 0.375, 0.975, 0.375, 0.975, 0.95 , 0.975, 0.975,\n",
              "        0.975, 0.375, 0.675, 0.975, 0.975, 0.75 , 1.   , 0.375, 1.   ,\n",
              "        0.725, 0.975, 0.975, 0.975, 0.3  , 0.975]),\n",
              " 'split1_test_score': array([1.   , 0.975, 0.975, 0.325, 0.35 , 0.35 , 0.925, 0.975, 0.675,\n",
              "        0.95 , 0.35 , 0.975, 0.325, 0.975, 1.   , 1.   , 0.325, 0.975,\n",
              "        0.675, 0.975, 0.975, 0.975, 0.35 , 0.975]),\n",
              " 'split2_test_score': array([0.35 , 1.   , 0.35 , 0.675, 0.35 , 1.   , 0.875, 1.   , 1.   ,\n",
              "        0.975, 0.35 , 0.675, 1.   , 1.   , 0.975, 1.   , 0.35 , 0.95 ,\n",
              "        1.   , 1.   , 1.   , 1.   , 0.35 , 1.   ]),\n",
              " 'std_fit_time': array([0.05722256, 0.041895  , 0.07581938, 0.09042153, 0.01715207,\n",
              "        0.07440425, 0.095503  , 0.04377286, 0.05069222, 0.02748626,\n",
              "        0.00853942, 0.08142937, 0.0419039 , 0.00791195, 0.03552666,\n",
              "        0.02625371, 0.00567481, 0.04908218, 0.02265508, 0.01906214,\n",
              "        0.0074739 , 0.00848041, 0.01389707, 0.0279531 ]),\n",
              " 'std_score_time': array([1.30606143e-04, 2.93143720e-04, 3.65136100e-03, 6.96384288e-04,\n",
              "        1.13881783e-03, 3.40459428e-03, 8.41030565e-03, 1.42059452e-03,\n",
              "        5.47743784e-04, 1.61088919e-04, 1.48995780e-04, 3.55429427e-05,\n",
              "        5.21774193e-04, 4.07908078e-03, 3.90411983e-03, 1.56424948e-03,\n",
              "        1.02902286e-04, 3.93311576e-04, 1.84003270e-05, 3.47640471e-03,\n",
              "        2.99775430e-03, 8.12474463e-04, 2.63746911e-03, 1.88672500e-03]),\n",
              " 'std_test_score': array([0.31885211, 0.01178511, 0.2889156 , 0.26562296, 0.01178511,\n",
              "        0.30069364, 0.03118048, 0.01178511, 0.14766704, 0.01178511,\n",
              "        0.01178511, 0.14142136, 0.31247222, 0.01178511, 0.11242281,\n",
              "        0.        , 0.02041241, 0.02041241, 0.1428869 , 0.01178511,\n",
              "        0.01178511, 0.01178511, 0.02357023, 0.01178511])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "steps = [('scaler', StandardScaler()), ('mlp', MLPClassifier(max_iter=1000))]\n",
        "param_grid2 = dict(mlp__learning_rate_init=learning_rate_init,\n",
        "                   mlp__hidden_layer_sizes=hidden_layer_sizes,\n",
        "                   mlp__solver=solver)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "# define the pipeline object.\n",
        "grid = GridSearchCV(pipeline, param_grid=param_grid2, cv=3)\n",
        "\n",
        "grid_result2 = grid.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "zLUpoCwdD_W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_result2.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL3xZRYddE5Y",
        "outputId": "89a1bb60-d1d4-4845-d8fb-1c63374a2342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9833333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_result2.best_estimator_"
      ],
      "metadata": {
        "id": "GzKqiBGroexl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54b08ae-d49b-4336-a138-9fa3933b8d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scaler', StandardScaler()),\n",
              "                ('mlp',\n",
              "                 MLPClassifier(hidden_layer_sizes=(3,), learning_rate_init=0.1,\n",
              "                               max_iter=1000))])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "do-rZ6rl9JJo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}